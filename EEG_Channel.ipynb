{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import elu,relu,leaky_relu\n",
    "import braindecode\n",
    "from braindecode.models import *\n",
    "from braindecode.models.modules import Expression\n",
    "from braindecode.models.functions import squeeze_final_output,square,safe_log\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.helper import predefined_split\n",
    "from config import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mne import set_log_level\n",
    "from chrononet import ChronoNet\n",
    "set_log_level(False)\n",
    "device = 'cuda' if cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads preprocessed data from mat files\n",
    "import scipy\n",
    "import numpy as np\n",
    "inputs=scipy.io.loadmat(\"E:/train_set_1.mat\")\n",
    "X=inputs[\"x\"]\n",
    "y=inputs[\"y\"].squeeze()\n",
    "\n",
    "inputs=scipy.io.loadmat(\"E:/train_set_2.mat\")\n",
    "X=np.concatenate((X,inputs[\"x\"]),axis=0)\n",
    "y=np.concatenate((y,inputs[\"y\"].squeeze()),axis=0)\n",
    "input_time_length=X.shape[-1]\n",
    "inputs=scipy.io.loadmat(\"E:/test_set.mat\")\n",
    "test_x=inputs[\"x\"]\n",
    "test_y=inputs[\"y\"].squeeze()\n",
    "del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels chosen:['FZ']\n"
     ]
    }
   ],
   "source": [
    "#We will now train the model by taking pairs or combinations of channels and passing their entire length.\n",
    "ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "picked_ch=[11]\n",
    "input_time_length=X.shape[-1]\n",
    "train_x=X[:,picked_ch]\n",
    "train_y=y\n",
    "n_chans=len(picked_ch)\n",
    "eval_x=test_x[:,picked_ch]\n",
    "eval_y=test_y\n",
    "print(f'Channels chosen:{[ch_names[ch] for ch in picked_ch]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will convert the 1d array into a 2d matrix by converting from (1,60000) to (10,6000) and then passing them through the network\n",
    "#Due to conv_time_spat layer in model, it would be better to have it so that first 10 entries are in channel 1, next 10 in channel 2 etc\n",
    "#So that channel 1 will have 10,110,210 entries and so on. This means that the convolution layer will compress the 10 entries.\n",
    "n_chans=10\n",
    "input_time_length=input_time_length//n_chans\n",
    "train_x=train_x.reshape(len(train_x),n_chans,input_time_length,order='F')\n",
    "eval_x=eval_x.reshape(len(eval_x),n_chans,input_time_length,order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing(dataset,labels):\n",
    "    abnormal=dataset[labels==1]\n",
    "    abnormal_labels=labels[labels==1]\n",
    "    dataset=np.delete(dataset,np.where(labels==1),axis=0)\n",
    "    labels=np.delete(labels,np.where(labels==1),axis=0)\n",
    "\n",
    "    factor=len(dataset)//len(abnormal)\n",
    "    abnormal=np.repeat(abnormal,factor,axis=0)\n",
    "    abnormal_labels=np.repeat(abnormal_labels,factor,axis=0)\n",
    "\n",
    "    dataset=np.concatenate((dataset,abnormal),axis=0)\n",
    "    labels=np.concatenate((labels,abnormal_labels),axis=0)\n",
    "    return (dataset,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,train_y=balancing(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='shallow_deep'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shallow_deep\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2\n",
    "criterion=torch.nn.NLLLoss\n",
    "if model_name==\"shallow\":\n",
    "    optimizer_lr = 0.000625\n",
    "    optimizer_weight_decay = 0\n",
    "    pool_time_length=150\n",
    "    pool_time_stride=50\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = ShallowFBCSPNet(n_chans,\n",
    "                                    n_classes,\n",
    "                                    n_filters_time=n_start_chans,\n",
    "                                    n_filters_spat=n_start_chans,\n",
    "                                    n_times=input_time_length,\n",
    "                                    pool_time_length=pool_time_length,\n",
    "                                    pool_time_stride=pool_time_stride,\n",
    "                                    final_conv_length='auto',)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del test,out\n",
    "elif model_name==\"deep\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                         n_filters_time=n_start_chans,\n",
    "                         n_filters_spat=n_start_chans,\n",
    "                         n_times=input_time_length,\n",
    "                         n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                         n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                         n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                         final_conv_length='auto',\n",
    "                        stride_before_pool=True)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del test,out\n",
    "elif model_name==\"hybrid\":\n",
    "    optimizer_lr = 0.000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = HybridNet(n_chans, n_classes,n_times=input_time_length,)\n",
    "    test=torch.ones(size=(2,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    model.final_layer=nn.Conv2d(100,n_classes,(out_length,1),bias=True,)\n",
    "    criterion=torch.nn.CrossEntropyLoss\n",
    "    model=nn.Sequential(model,Expression(torch.squeeze),nn.Softmax(dim=1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length\n",
    "elif model_name==\"deep_smac\" or model_name == 'deep_smac_bnorm':\n",
    "    optimizer_lr = 0.000625\n",
    "    if model_name == 'deep_smac':\n",
    "            do_batch_norm = False\n",
    "    else:\n",
    "        do_batch_norm = True\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 12\n",
    "    filter_length_3 = 24\n",
    "    filter_length_4 = 36\n",
    "    filter_time_length = 21\n",
    "    #final_conv_length = 1\n",
    "    first_nonlin = elu\n",
    "    first_pool_mode = 'mean'\n",
    "    later_nonlin = elu\n",
    "    later_pool_mode = 'mean'\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    pool_time_length = 3\n",
    "    pool_time_stride = 3\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    n_start_chans = n_filters_start\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "            n_filters_time=n_start_chans,\n",
    "            n_filters_spat=n_start_chans,\n",
    "            n_times=input_time_length,\n",
    "            n_filters_2=int(n_start_chans * n_chan_factor),\n",
    "            n_filters_3=int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "            n_filters_4=int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "            final_conv_length='auto',\n",
    "            batch_norm=do_batch_norm,\n",
    "            drop_prob=drop_prob,\n",
    "            filter_length_2=filter_length_2,\n",
    "            filter_length_3=filter_length_3,\n",
    "            filter_length_4=filter_length_4,\n",
    "            filter_time_length=filter_time_length,\n",
    "            first_conv_nonlin=first_nonlin,\n",
    "            first_pool_mode=first_pool_mode,\n",
    "            later_conv_nonlin=later_nonlin,\n",
    "            later_pool_mode=later_pool_mode,\n",
    "            #pool_time_length=pool_time_length,\n",
    "            #pool_time_stride=pool_time_stride,\n",
    "            split_first_layer=split_first_layer,\n",
    "            stride_before_pool=True)\n",
    "elif model_name==\"shallow_deep\":\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 12\n",
    "    filter_length_3 = 14\n",
    "    filter_length_4 = 32\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    n_start_chans = n_filters_start\n",
    "\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    conv_time_length=25\n",
    "    first_conv_nonlin=relu\n",
    "    first_pool_nonlin=safe_log\n",
    "    later_conv_nonlin=elu\n",
    "    later_pool_nonlin=safe_log\n",
    "    first_pool_mode = 'mean'\n",
    "    later_pool_mode = 'mean'\n",
    "    pool_time_length=15\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                            n_times=input_time_length,\n",
    "                            n_filters_time=n_start_chans,\n",
    "                            n_filters_spat=n_start_chans,\n",
    "                            n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                            n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                            n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                            final_conv_length='auto',\n",
    "                            first_pool_nonlin=first_pool_nonlin,\n",
    "                            first_conv_nonlin=first_conv_nonlin,\n",
    "                            #later_pool_nonlin=later_pool_nonlin,\n",
    "                            #later_conv_nonlin=later_conv_nonlin,\n",
    "                            filter_time_length=conv_time_length,\n",
    "                            pool_time_length=pool_time_length,\n",
    "                            first_pool_mode=first_pool_mode,\n",
    "                            later_pool_mode=later_pool_mode,\n",
    "                            split_first_layer=split_first_layer,\n",
    "                            drop_prob=drop_prob,\n",
    "                            filter_length_2=filter_length_2,\n",
    "                            filter_length_3=filter_length_3,\n",
    "                            filter_length_4=filter_length_4,\n",
    "                            )\n",
    "elif model_name==\"attention\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    model=ATCNet(n_chans,n_classes,input_time_length//sampling_freq,sampling_freq,concat=True,tcn_depth=4)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del test,out\n",
    "elif model_name==\"ChronoNet\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #I had made the ChronoNet structure as similar to tensorflow implementation as possible.\n",
    "    model=ChronoNet(input_time_length)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del test,out\n",
    "elif model_name==\"TCN\":\n",
    "    import warnings\n",
    "    #This disables the warning of the dropout2d layers receiving 3d input\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    optimizer_lr = 0.000625\n",
    "    optimizer_weight_decay = 0\n",
    "    n_blocks=7\n",
    "    n_filters=32\n",
    "    kernel_size=24\n",
    "    drop_prob = 0.3\n",
    "    add_log_softmax=False\n",
    "    #Minimum time length for TCN, found inside tcn.py\n",
    "    min_len = 1\n",
    "    for i in range(n_blocks):\n",
    "        dilation = 2 ** i\n",
    "        min_len += 2 * (kernel_size - 1) * dilation\n",
    "    print(f\"Minimum length :{min_len}\")\n",
    "    #Only setting n_classes to 1 so TCN output is (batch,1,2) so we can remove additional conv1d block.\n",
    "    #This is only possible due to input_time_length=30,n_block=3 and kernel_size=3.\n",
    "    x=TCN(n_chans,n_classes,n_blocks,n_filters,kernel_size,drop_prob,n_times=input_time_length)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=x.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    #model=nn.Sequential(x,Expression(torch.squeeze),nn.LogSoftmax(dim=1))\n",
    "    #There is no hyperparameter where output of TCN is (Batch_Size,Classes) when input is (Batch_Size,21,6000) so add new layers to meet size\n",
    "    model=nn.Sequential(x,nn.Conv1d(n_classes,n_classes,out_length,bias=True,),Expression(torch.squeeze),nn.LogSoftmax(dim=1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length,x\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=Sequential(\n",
       "    (0): TCN(\n",
       "      (ensuredims): Ensure4d()\n",
       "      (temporal_blocks): Sequential(\n",
       "        (temporal_block_0): TemporalBlock(\n",
       "          (conv1): Conv1d(10, 32, kernel_size=(24,), stride=(1,), padding=(23,))\n",
       "          (chomp1): Chomp1d(chomp_size=23)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(23,))\n",
       "          (chomp2): Chomp1d(chomp_size=23)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (downsample): Conv1d(10, 32, kernel_size=(1,), stride=(1,))\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_1): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(46,), dilation=(2,))\n",
       "          (chomp1): Chomp1d(chomp_size=46)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(46,), dilation=(2,))\n",
       "          (chomp2): Chomp1d(chomp_size=46)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_2): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(92,), dilation=(4,))\n",
       "          (chomp1): Chomp1d(chomp_size=92)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(92,), dilation=(4,))\n",
       "          (chomp2): Chomp1d(chomp_size=92)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_3): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(184,), dilation=(8,))\n",
       "          (chomp1): Chomp1d(chomp_size=184)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(184,), dilation=(8,))\n",
       "          (chomp2): Chomp1d(chomp_size=184)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_4): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(368,), dilation=(16,))\n",
       "          (chomp1): Chomp1d(chomp_size=368)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(368,), dilation=(16,))\n",
       "          (chomp2): Chomp1d(chomp_size=368)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_5): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(736,), dilation=(32,))\n",
       "          (chomp1): Chomp1d(chomp_size=736)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(736,), dilation=(32,))\n",
       "          (chomp2): Chomp1d(chomp_size=736)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_6): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(1472,), dilation=(64,))\n",
       "          (chomp1): Chomp1d(chomp_size=1472)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(24,), stride=(1,), padding=(1472,), dilation=(64,))\n",
       "          (chomp2): Chomp1d(chomp_size=1472)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (final_layer): _FinalLayer(\n",
       "        (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       "        (out_fun): Identity()\n",
       "        (squeeze): Expression(expression=squeeze_final_output) \n",
       "      )\n",
       "    )\n",
       "    (1): Conv1d(2, 2, kernel_size=(158,), stride=(1,))\n",
       "    (2): Expression(expression=squeeze) \n",
       "    (3): LogSoftmax(dim=1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'chan_{model_name}best_param.pkl',\n",
    "               f_optimizer=f'chan_{model_name}best_opt.pkl', f_history=f'chan_{model_name}best_history.json')\n",
    "classifier = braindecode.EEGClassifier(\n",
    "    model,\n",
    "    criterion=criterion,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=predefined_split(Dataset(eval_x,eval_y)),\n",
    "    optimizer__lr=optimizer_lr,\n",
    "    #optimizer__weight_decay=optimizer_weight_decay,\n",
    "    iterator_train__shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    callbacks=[\"accuracy\",\"f1\",'roc_auc',cp],#Try (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=20))\n",
    "    warm_start=True,\n",
    "    )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1D to 2D conversion of single channel of 10 minutes and using shallow-deep CNN\n",
    "#   Accuracy    F1-score       Loss              AUC\n",
    "#A1:0.7481      0.7344        0.5478           0.8070\n",
    "#A2:0.7556      0.7130        0.6907           0.7879\n",
    "#C3:0.7259      0.7040        0.6569           0.7496\n",
    "#C4:0.7111      0.6286        0.8119           0.7738\n",
    "#CZ:0.7259      0.6942        0.7168           0.7628\n",
    "#F3:0.7111      0.6549        0.8018           0.7540\n",
    "#F4:0.7111      0.6422        0.6267           0.7608\n",
    "#F7:0.7556      0.6733        0.6990           0.8015\n",
    "#F8:0.7333      0.6897        0.6086           0.7520\n",
    "#FP1:0.7259     0.7259        0.5895           0.7788\n",
    "#FP2:0.7481     0.7167        0.6900           0.7705\n",
    "#FZ:0.7852      0.7521        0.6720           0.8140   #Consistently high results\n",
    "#O1:0.7556      0.7130        0.5564           0.8158\n",
    "#O2:0.7185      0.6415        0.7401           0.7711\n",
    "#P3:0.7037      0.6154        0.5774           0.8110\n",
    "#P4:0.7407      0.7445        0.5793           0.7993\n",
    "#PZ:0.7037      0.6825        0.6528           0.7238\n",
    "#T3:0.7704      0.7395        0.4957           0.8506\n",
    "#T4:0.7185      0.7286        0.5802           0.7753\n",
    "#T5:0.7481      0.6909        0.5537           0.8239\n",
    "#T6:0.7185      0.6607        0.5840           0.7722\n",
    "\n",
    "#1D to 2D conversion of single channel of 3 seconds and using TCN\n",
    "#   Accuracy    F1-score       Loss              AUC\n",
    "#A1:0.5827      0.2397        0.8152           0.7300\n",
    "#A2:0.5903      0.2692        0.7758           0.7414\n",
    "#C3:\n",
    "#C4:\n",
    "#CZ:\n",
    "#F3:\n",
    "#F4:\n",
    "#F7:\n",
    "#F8:\n",
    "#FP1:0.6015     0.2980        0.7654           0.7980\n",
    "#FP2:0.5868     0.2593        0.7681           0.7376\n",
    "#FZ:\n",
    "#O1:\n",
    "#O2:\n",
    "#P3:\n",
    "#P4:\n",
    "#PZ:\n",
    "#T3:\n",
    "#T4:\n",
    "#T5:\n",
    "#T6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11            0.4953      0.6625        0.8183           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        108.8986\n",
      "     12            0.4953      0.6625        0.8169           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        109.6255\n",
      "     13            0.4953      0.6625        0.8183           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        108.9973\n",
      "     14            0.4953      0.6625        0.8176           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        108.5942\n",
      "     15            0.4953      0.6625        0.8180           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        108.7051\n",
      "     16            0.4953      0.6625        0.8187           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        109.1842\n",
      "     17            0.4953      0.6625        0.8169           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        108.8532\n",
      "     18            0.4953      0.6625        0.8176           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        109.0794\n",
      "     19            0.4953      0.6625        0.8169           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        108.9723\n",
      "     20            0.4953      0.6625        0.8173           0.5000       0.4741            0.4741      0.6432        0.8392           0.5000        109.4520\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=Sequential(\n",
       "    (0): HybridNet(\n",
       "      (reduced_deep_model): Sequential(\n",
       "        (ensuredims): Ensure4d()\n",
       "        (dimshuffle): Rearrange('batch C T 1 -> batch 1 T C')\n",
       "        (conv_time_spat): CombinedConv(\n",
       "          (conv_time): Conv2d(1, 20, kernel_size=(10, 1), stride=(1, 1))\n",
       "          (conv_spat): Conv2d(20, 30, kernel_size=(1, 10), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bnorm): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_nonlin): Expression(expression=elu) \n",
       "        (pool): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(1, 1), ceil_mode=False)\n",
       "        (pool_nonlin): Expression(expression=identity) \n",
       "        (drop_2): Dropout(p=0.5, inplace=False)\n",
       "        (conv_2): Conv2d(30, 40, kernel_size=(10, 1), stride=(1, 1), dilation=(3, 1), bias=False)\n",
       "        (bnorm_2): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (nonlin_2): Expression(expression=elu) \n",
       "        (pool_2): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(3, 1), ceil_mode=False)\n",
       "        (pool_nonlin_2): Expression(expression=identity) \n",
       "        (drop_3): Dropout(p=0.5, inplace=False)\n",
       "        (conv_3): Conv2d(40, 50, kernel_size=(10, 1), stride=(1, 1), dilation=(9, 1), bias=False)\n",
       "        (bnorm_3): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (nonlin_3): Expression(expression=elu) \n",
       "        (pool_3): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(9, 1), ceil_mode=False)\n",
       "        (pool_nonlin_3): Expression(expression=identity) \n",
       "        (drop_4): Dropout(p=0.5, inplace=False)\n",
       "        (conv_4): Conv2d(50, 60, kernel_size=(10, 1), stride=(1, 1), dilation=(27, 1), bias=False)\n",
       "        (bnorm_4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (nonlin_4): Expression(expression=elu) \n",
       "        (pool_4): MaxPool2d(kernel_size=(3, 1), stride=(1, 1), padding=0, dilation=(27, 1), ceil_mode=False)\n",
       "        (pool_nonlin_4): Expression(expression=identity) \n",
       "        (deep_final_conv): Conv2d(60, 60, kernel_size=(2, 1), stride=(1, 1), dilation=(81, 1))\n",
       "      )\n",
       "      (reduced_shallow_model): Sequential(\n",
       "        (ensuredims): Ensure4d()\n",
       "        (dimshuffle): Rearrange('batch C T 1 -> batch 1 T C')\n",
       "        (conv_time_spat): CombinedConv(\n",
       "          (conv_time): Conv2d(1, 30, kernel_size=(28, 1), stride=(1, 1))\n",
       "          (conv_spat): Conv2d(30, 40, kernel_size=(1, 10), stride=(1, 1), bias=False)\n",
       "        )\n",
       "        (bnorm): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv_nonlin_exp): Expression(expression=square) \n",
       "        (pool): AvgPool2d(kernel_size=(75, 1), stride=(1, 1), padding=0)\n",
       "        (pool_nonlin_exp): Expression(expression=safe_log) \n",
       "        (drop): Dropout(p=0.5, inplace=False)\n",
       "        (shallow_final_conv): Conv2d(40, 40, kernel_size=(29, 1), stride=(1, 1), dilation=(15, 1))\n",
       "      )\n",
       "      (final_layer): Conv2d(100, 2, kernel_size=(5479, 1), stride=(1, 1))\n",
       "    )\n",
       "    (1): Expression(expression=squeeze) \n",
       "    (2): Softmax(dim=1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_x,train_y,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(f_params=f'model/chan_{model_name}best_param.pkl',f_optimizer=f'model/chan_{model_name}best_opt.pkl', f_history=f'model/chan_{model_name}best_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramters Loaded\n"
     ]
    }
   ],
   "source": [
    "classifier.load_params(f_params=f'model/chan_{model_name}best_param.pkl',f_optimizer=f'model/chan_{model_name}best_opt.pkl', f_history=f'model/chan_{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TCN\n",
      "Accuracy:0.7333333333333333\n",
      "F1-Score:0.7142857142857143\n",
      "roc_auc score:0.755281690140845\n"
     ]
    }
   ],
   "source": [
    "#This block finds the accuracy, f1 score and roc auc of the valid/test set\n",
    "pred_labels=classifier.predict(eval_x)\n",
    "auc=roc_auc_score(test_y,classifier.predict_proba(eval_x)[:,1])\n",
    "accuracy=np.mean(pred_labels==test_y)\n",
    "tp=np.sum(pred_labels*test_y)\n",
    "precision=tp/np.sum(pred_labels)\n",
    "recall=tp/np.sum(test_y)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "\n",
    "print(model_name)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "print(f\"F1-Score:{f1}\")\n",
    "print(f\"roc_auc score:{auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hybrid CNN and LSTM block\n",
    "from skorch import NeuralNet\n",
    "network=NeuralNet(module=model,criterion=torch.nn.modules.loss.NLLLoss,batch_size=batch_size,device=device)\n",
    "network.initialize()\n",
    "network.load_params(\n",
    "    f_params=f'model/chan_{model_name}best_param.pkl', f_optimizer=f'model/chan_{model_name}best_opt.pkl', f_history=f'model/chan_{model_name}best_history.json')\n",
    "network.module_=torch.nn.Sequential(*(list(network.module_.children())[:-3]),nn.modules.Flatten())\n",
    "test=torch.ones(size=(2,n_chans,input_time_length))\n",
    "feat=network.predict(test).shape[1]\n",
    "print(feat)\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features=network.predict(train_x)\n",
    "eval_features=network.predict(eval_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For channel-wise classification, we used 10 minutes of a single channel instead of 1 minute of all channels, we shall consider the features to be contiguous\n",
    "#We had converted the 1D 10 minute window into 2D 1 minute window with 10 channels, we will just reshape the 1D windows into 2D again\n",
    "#t variable determines timesteps for hybrid model\n",
    "t=5\n",
    "factor=feat//t\n",
    "train_features=train_features[:,:factor*t].reshape(len(train_y),t,factor)\n",
    "eval_features=eval_features[:,:factor*t].reshape(len(eval_y),t,factor)\n",
    "print(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "  def __init__(self,input_features):\n",
    "    super().__init__()\n",
    "    self.lstm = torch.nn.LSTM(input_size=input_features, hidden_size=50, batch_first=True)\n",
    "    self.fc = torch.nn.Linear(50, 2)\n",
    "    self.tanh = torch.nn.Tanh()\n",
    "    self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    h1, _ = self.lstm(inputs)\n",
    "    h2=self.tanh(h1[:,-1,:])#Final output of LSTM will be fed to linear layer\n",
    "    h3 = self.fc(h2)\n",
    "    output = self.softmax(h3)\n",
    "    return output\n",
    "model = SimpleModel(factor)\n",
    "test=torch.ones(3,5,factor)\n",
    "out=model.forward(test)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params='chan_LSTMbest_param.pkl',f_optimizer='chan_LSTMbest_opt.pkl',f_history='chan_LSTMbest_history.json')\n",
    "classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(Dataset(eval_features,eval_y)),\n",
    "        optimizer__lr=0.0001,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",'roc_auc',cp],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_features,y=train_y,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
