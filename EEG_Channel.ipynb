{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.functional import elu,relu,leaky_relu\n",
    "import braindecode\n",
    "from braindecode.models import *\n",
    "from braindecode.models.modules import Expression\n",
    "from braindecode.models.functions import squeeze_final_output,square,safe_log\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.helper import predefined_split\n",
    "from config import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mne import set_log_level\n",
    "set_log_level(False)\n",
    "device = 'cuda' if cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads preprocessed data from mat files\n",
    "import scipy\n",
    "import numpy as np\n",
    "inputs=scipy.io.loadmat(\"E:/train_set_1.mat\")\n",
    "X=inputs[\"x\"]\n",
    "y=inputs[\"y\"].squeeze()\n",
    "\n",
    "inputs=scipy.io.loadmat(\"E:/train_set_2.mat\")\n",
    "X=np.concatenate((X,inputs[\"x\"]),axis=0)\n",
    "y=np.concatenate((y,inputs[\"y\"].squeeze()),axis=0)\n",
    "input_time_length=X.shape[-1]\n",
    "inputs=scipy.io.loadmat(\"E:/test_set.mat\")\n",
    "test_x=inputs[\"x\"]\n",
    "test_y=inputs[\"y\"].squeeze()\n",
    "del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels chosen:['FP1']\n"
     ]
    }
   ],
   "source": [
    "#We will now train the model by taking pairs or combinations of channels and passing their entire length.\n",
    "ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "picked_ch=[9]\n",
    "input_time_length=X.shape[-1]\n",
    "train_x=X[:,picked_ch]\n",
    "train_y=y\n",
    "n_chans=len(picked_ch)\n",
    "eval_x=test_x[:,picked_ch]\n",
    "eval_y=test_y\n",
    "print(f'Channels chosen:{[ch_names[ch] for ch in picked_ch]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will take windows of 3 seconds. We will reshape train_x to have (samples,1,3*sampling_frequency), we will convert it to 2D matrix later.\n",
    "#The windows will have no overlap with this method, for overlap, use braindecode create_from_X_y function\n",
    "window_length_sec=3\n",
    "no_of_windows=input_time_length//(window_length_sec*sampling_freq)\n",
    "train_x=train_x.reshape(len(train_x)*no_of_windows,n_chans,window_length_sec*sampling_freq)\n",
    "train_y=train_y.repeat(no_of_windows,axis=0)\n",
    "eval_x=eval_x.reshape(len(eval_x)*no_of_windows,n_chans,window_length_sec*sampling_freq)\n",
    "eval_y=eval_y.repeat(no_of_windows,axis=0)\n",
    "input_time_length=window_length_sec*sampling_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We will convert the 1d array into a 2d matrix by converting from (1,60000) to (10,6000) and then passing them through the network\n",
    "#Due to conv_time_spat layer in model, it would be better to have it so that first 10 entries are in channel 1, next 10 in channel 2 etc\n",
    "#So that channel 1 will have 10,110,210 entries and so on. This means that the convolution layer will compress the 10 entries.\n",
    "n_chans=3\n",
    "input_time_length=input_time_length//n_chans\n",
    "train_x=train_x.reshape(len(train_x),n_chans,input_time_length,order='F')\n",
    "eval_x=eval_x.reshape(len(eval_x),n_chans,input_time_length,order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='TCN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum length :85\n",
      "torch.Size([7, 2])\n",
      "TCN\n",
      "Sequential(\n",
      "  (0): TCN(\n",
      "    (ensuredims): Ensure4d()\n",
      "    (temporal_blocks): Sequential(\n",
      "      (temporal_block_0): TemporalBlock(\n",
      "        (conv1): Conv1d(3, 32, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "        (chomp1): Chomp1d(chomp_size=6)\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout2d(p=0.3, inplace=False)\n",
      "        (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(6,))\n",
      "        (chomp2): Chomp1d(chomp_size=6)\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout2d(p=0.3, inplace=False)\n",
      "        (downsample): Conv1d(3, 32, kernel_size=(1,), stride=(1,))\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (temporal_block_1): TemporalBlock(\n",
      "        (conv1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "        (chomp1): Chomp1d(chomp_size=12)\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout2d(p=0.3, inplace=False)\n",
      "        (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
      "        (chomp2): Chomp1d(chomp_size=12)\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout2d(p=0.3, inplace=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "      (temporal_block_2): TemporalBlock(\n",
      "        (conv1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "        (chomp1): Chomp1d(chomp_size=24)\n",
      "        (relu1): ReLU()\n",
      "        (dropout1): Dropout2d(p=0.3, inplace=False)\n",
      "        (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
      "        (chomp2): Chomp1d(chomp_size=24)\n",
      "        (relu2): ReLU()\n",
      "        (dropout2): Dropout2d(p=0.3, inplace=False)\n",
      "        (relu): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (final_layer): _FinalLayer(\n",
      "      (fc): Linear(in_features=32, out_features=2, bias=True)\n",
      "      (out_fun): Identity()\n",
      "      (squeeze): Expression(expression=squeeze_final_output) \n",
      "    )\n",
      "  )\n",
      "  (1): Conv1d(2, 2, kernel_size=(16,), stride=(1,))\n",
      "  (2): Expression(expression=squeeze) \n",
      "  (3): LogSoftmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2\n",
    "criterion=torch.nn.NLLLoss\n",
    "if model_name==\"shallow\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    pool_time_length=150\n",
    "    pool_time_stride=50\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = ShallowFBCSPNet(n_chans,\n",
    "                                    n_classes,\n",
    "                                    n_filters_time=n_start_chans,\n",
    "                                    n_filters_spat=n_start_chans,\n",
    "                                    input_window_samples=input_time_length,\n",
    "                                    pool_time_length=pool_time_length,\n",
    "                                    pool_time_stride=pool_time_stride,\n",
    "                                    final_conv_length='auto',)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del test,out\n",
    "elif model_name==\"deep\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                         n_filters_time=n_start_chans,\n",
    "                         n_filters_spat=n_start_chans,\n",
    "                         input_window_samples=input_time_length,\n",
    "                         n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                         n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                         n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                         final_conv_length='auto',\n",
    "                        stride_before_pool=True)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del test,out\n",
    "elif model_name==\"deep_smac\" or model_name == 'deep_smac_bnorm':\n",
    "    optimizer_lr = 0.0000625\n",
    "    if model_name == 'deep_smac':\n",
    "            do_batch_norm = False\n",
    "    else:\n",
    "        do_batch_norm = True\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 12\n",
    "    filter_length_3 = 24\n",
    "    filter_length_4 = 36\n",
    "    filter_time_length = 21\n",
    "    #final_conv_length = 1\n",
    "    first_nonlin = elu\n",
    "    first_pool_mode = 'mean'\n",
    "    later_nonlin = elu\n",
    "    later_pool_mode = 'mean'\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    pool_time_length = 3\n",
    "    pool_time_stride = 3\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    n_start_chans = n_filters_start\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "            n_filters_time=n_start_chans,\n",
    "            n_filters_spat=n_start_chans,\n",
    "            input_window_samples=input_time_length,\n",
    "            n_filters_2=int(n_start_chans * n_chan_factor),\n",
    "            n_filters_3=int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "            n_filters_4=int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "            final_conv_length='auto',\n",
    "            batch_norm=do_batch_norm,\n",
    "            drop_prob=drop_prob,\n",
    "            filter_length_2=filter_length_2,\n",
    "            filter_length_3=filter_length_3,\n",
    "            filter_length_4=filter_length_4,\n",
    "            filter_time_length=filter_time_length,\n",
    "            first_conv_nonlin=first_nonlin,\n",
    "            first_pool_mode=first_pool_mode,\n",
    "            later_conv_nonlin=later_nonlin,\n",
    "            later_pool_mode=later_pool_mode,\n",
    "            #pool_time_length=pool_time_length,\n",
    "            #pool_time_stride=pool_time_stride,\n",
    "            split_first_layer=split_first_layer,\n",
    "            stride_before_pool=True)\n",
    "elif model_name==\"shallow_deep\":\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 12\n",
    "    filter_length_3 = 14\n",
    "    filter_length_4 = 32\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    n_start_chans = n_filters_start\n",
    "\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    conv_time_length=25\n",
    "    first_conv_nonlin=relu\n",
    "    first_pool_nonlin=safe_log\n",
    "    later_conv_nonlin=elu\n",
    "    later_pool_nonlin=safe_log\n",
    "    first_pool_mode = 'mean'\n",
    "    later_pool_mode = 'mean'\n",
    "    pool_time_length=15\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                            input_time_length,\n",
    "                            n_filters_time=n_start_chans,\n",
    "                            n_filters_spat=n_start_chans,\n",
    "                            n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                            n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                            n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                            final_conv_length='auto',\n",
    "                            first_pool_nonlin=first_pool_nonlin,\n",
    "                            first_conv_nonlin=first_conv_nonlin,\n",
    "                            #later_pool_nonlin=later_pool_nonlin,\n",
    "                            #later_conv_nonlin=later_conv_nonlin,\n",
    "                            filter_time_length=conv_time_length,\n",
    "                            pool_time_length=pool_time_length,\n",
    "                            first_pool_mode=first_pool_mode,\n",
    "                            later_pool_mode=later_pool_mode,\n",
    "                            split_first_layer=split_first_layer,\n",
    "                            drop_prob=drop_prob,\n",
    "                            filter_length_2=filter_length_2,\n",
    "                            filter_length_3=filter_length_3,\n",
    "                            filter_length_4=filter_length_4,\n",
    "                            )\n",
    "elif model_name==\"attention\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    model=ATCNet(n_chans,n_classes,input_time_length//sampling_freq,sampling_freq,concat=True,tcn_depth=4)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del test,out\n",
    "\n",
    "elif model_name==\"TCN\":\n",
    "    import warnings\n",
    "    #This disables the warning of the dropout2d layers receiving 3d input\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    optimizer_lr = 0.000625\n",
    "    optimizer_weight_decay = 0\n",
    "    n_blocks=3\n",
    "    n_filters=32\n",
    "    kernel_size=7\n",
    "    drop_prob = 0.3\n",
    "    add_log_softmax=False\n",
    "    #Minimum time length for TCN, found inside tcn.py\n",
    "    min_len = 1\n",
    "    for i in range(n_blocks):\n",
    "        dilation = 2 ** i\n",
    "        min_len += 2 * (kernel_size - 1) * dilation\n",
    "    print(f\"Minimum length :{min_len}\")\n",
    "    #Only setting n_classes to 1 so TCN output is (batch,1,2) so we can remove additional conv1d block.\n",
    "    #This is only possible due to input_time_length=30,n_block=3 and kernel_size=3.\n",
    "    x=TCN(n_chans,n_classes,n_blocks,n_filters,kernel_size,drop_prob,n_times=input_time_length)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=x.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    #model=nn.Sequential(x,Expression(torch.squeeze),nn.LogSoftmax(dim=1))\n",
    "    #There is no hyperparameter where output of TCN is (Batch_Size,Classes) when input is (Batch_Size,21,6000) so add new layers to meet size\n",
    "    model=nn.Sequential(x,nn.Conv1d(n_classes,n_classes,out_length,bias=True,),Expression(torch.squeeze),nn.LogSoftmax(dim=1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length,x\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "\n",
    "print(model_name)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=Sequential(\n",
       "    (0): TCN(\n",
       "      (ensuredims): Ensure4d()\n",
       "      (temporal_blocks): Sequential(\n",
       "        (temporal_block_0): TemporalBlock(\n",
       "          (conv1): Conv1d(3, 32, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "          (chomp1): Chomp1d(chomp_size=6)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "          (chomp2): Chomp1d(chomp_size=6)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (downsample): Conv1d(3, 32, kernel_size=(1,), stride=(1,))\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_1): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "          (chomp1): Chomp1d(chomp_size=12)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "          (chomp2): Chomp1d(chomp_size=12)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_2): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "          (chomp1): Chomp1d(chomp_size=24)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "          (chomp2): Chomp1d(chomp_size=24)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (final_layer): _FinalLayer(\n",
       "        (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       "        (out_fun): Identity()\n",
       "        (squeeze): Expression(expression=squeeze_final_output) \n",
       "      )\n",
       "    )\n",
       "    (1): Conv1d(2, 2, kernel_size=(16,), stride=(1,))\n",
       "    (2): Expression(expression=squeeze) \n",
       "    (3): LogSoftmax(dim=1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'chanbest_param.pkl',\n",
    "               f_optimizer=f'chanbest_opt.pkl', f_history=f'chanbest_history.json')\n",
    "classifier = braindecode.EEGClassifier(\n",
    "    model,\n",
    "    criterion=criterion,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    train_split=predefined_split(Dataset(eval_x,eval_y)),\n",
    "    optimizer__lr=optimizer_lr,\n",
    "    #optimizer__weight_decay=optimizer_weight_decay,\n",
    "    iterator_train__shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    callbacks=[\"accuracy\",\"f1\",'roc_auc',cp],#Try (\"lr_scheduler\", LRScheduler(\"CosineAnnealingLR\", T_max=20))\n",
    "    warm_start=True,\n",
    "    )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Accuracy    F1-score       Loss              AUC\n",
    "#A1:0.7481      0.7344        0.5478           0.8070\n",
    "#A2:0.7556      0.7130        0.6907           0.7879\n",
    "#C3:0.7259      0.7040        0.6569           0.7496\n",
    "#C4:0.7111      0.6286        0.8119           0.7738\n",
    "#CZ:0.7259      0.6942        0.7168           0.7628\n",
    "#F3:0.7111      0.6549        0.8018           0.7540\n",
    "#F4:0.7111      0.6422        0.6267           0.7608\n",
    "#F7:0.7556      0.6733        0.6990           0.8015\n",
    "#F8:0.7333      0.6897        0.6086           0.7520\n",
    "#FP1:0.7259     0.7259        0.5895           0.7788\n",
    "#FP2:0.7481     0.7167        0.6900           0.7705\n",
    "#FZ:0.7852      0.7521        0.6720           0.8140   #Consistently high results\n",
    "#O1:0.7556      0.7130        0.5564           0.8158\n",
    "#O2:0.7185      0.6415        0.7401           0.7711\n",
    "#P3:0.7037      0.6154        0.5774           0.8110\n",
    "#P4:0.7407      0.7445        0.5793           0.7993\n",
    "#PZ:0.7037      0.6825        0.6528           0.7238\n",
    "#T3:0.7704      0.7395        0.4957           0.8506\n",
    "#T4:0.7185      0.7286        0.5802           0.7753\n",
    "#T5:0.7481      0.6909        0.5537           0.8239\n",
    "#T6:0.7185      0.6607        0.5840           0.7722"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    196            0.8508      0.2257        0.3719           0.7909       0.5820            0.5820      0.2299        0.7681           0.7773        44.9001\n",
      "    197            0.8490      0.1800        0.3724           0.7937       0.5729            0.5729      0.1910        0.8061           0.7911        44.1373\n",
      "    198            0.8499      0.2157        0.3718           0.7867       0.5820            0.5820      0.2292        0.8047           0.7536        41.0943\n",
      "    199            0.8477      0.1561        0.3722           0.7934       0.5609            0.5609      0.1466        0.8012           0.7867        43.5997\n",
      "    200            0.8511      0.2118        0.3722           \u001b[31m0.7961\u001b[0m       0.5861            0.5861      0.2405        0.7845           0.7967        42.7554\n",
      "    201            0.8498      0.1983        0.3722           0.7933       0.5785            0.5785      0.2129        0.7785           0.7881        40.3967\n",
      "    202            0.8505      0.1993        0.3723           \u001b[31m0.7972\u001b[0m       0.5753            0.5753      0.2002        0.7853           0.7921        41.6942\n",
      "    203            \u001b[36m0.8528\u001b[0m      0.2663        0.3724           0.7958       0.6015            0.6015      0.2980        0.7654           0.7980        42.8893\n",
      "    204            0.8499      0.2060        0.3723           0.7955       0.5760            0.5760      0.2074        0.7634           0.7856        40.8187\n",
      "    205            0.8509      0.2062        0.3725           0.7969       0.5807            0.5807      0.2236        0.7477           0.8087        38.6187\n",
      "    206            0.8490      0.1821        0.3721           0.7937       0.5677            0.5677      0.1741        0.7838           0.7818        41.4682\n",
      "    207            0.8505      0.2032        0.3722           0.7957       0.5764            0.5764      0.2053        0.7619           0.7971        32.4766\n",
      "    208            0.8482      0.1720        0.3722           0.7878       0.5653            0.5653      0.1637        0.8039           0.7640        28.7820\n",
      "    209            0.8513      0.2125        0.3718           0.7969       0.5808            0.5808      0.2233        0.7840           0.7959        29.2921\n",
      "    210            0.8498      0.1928        0.3719           0.7934       0.5739            0.5739      0.1956        0.7765           0.7894        29.7133\n",
      "    211            0.8513      0.2220        0.3719           0.7958       0.5851            0.5851      0.2382        0.7662           0.7967        29.1279\n",
      "    212            0.8488      0.1762        0.3719           0.7933       0.5673            0.5673      0.1717        0.7935           0.7962        32.0406\n",
      "    213            0.8515      0.2335        0.3717           0.7915       0.5839            0.5839      0.2359        0.8195           0.7722        30.1274\n",
      "    214            0.8521      0.2505        0.3719           0.7936       0.5899            0.5899      0.2612        0.7843           0.7909        29.8580\n",
      "    215            0.8510      0.2117        0.3720           0.7971       0.5793            0.5793      0.2157        0.7590           0.7906        29.9758\n",
      "    216            0.8524      0.2346        0.3723           \u001b[31m0.7979\u001b[0m       0.5885            0.5885      0.2503        0.7466           0.8000        29.8526\n",
      "    217            0.8506      0.2072        0.3722           0.7950       0.5831            0.5831      0.2306        0.7463           0.8006        29.9904\n",
      "    218            0.8476      0.1532        0.3720           0.7962       0.5601            0.5601      0.1425        0.8234           0.7948        30.2333\n",
      "    219            0.8498      0.2033        0.3720           0.7924       0.5776            0.5776      0.2129        0.7372           0.7806        29.9421\n",
      "    220            0.8491      0.1802        0.3721           0.7940       0.5694            0.5694      0.1770        0.7618           0.7880        29.9826\n",
      "    221            0.8504      0.2111        0.3720           0.7946       0.5784            0.5784      0.2144        0.7525           0.7900        29.9244\n",
      "    222            0.8520      0.2322        0.3722           0.7970       0.5845            0.5845      0.2384        0.7634           0.7997        29.9982\n",
      "    223            0.8517      0.2321        0.3720           0.7941       0.5810            0.5810      0.2273        0.7551           0.7780        31.6936\n",
      "    224            0.8503      0.2132        0.3721           0.7898       0.5806            0.5806      0.2232        0.7672           0.7826        33.4613\n",
      "    225            0.8484      0.1723        0.3718           0.7928       0.5685            0.5685      0.1744        0.7911           0.7906        34.1847\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=Sequential(\n",
       "    (0): TCN(\n",
       "      (ensuredims): Ensure4d()\n",
       "      (temporal_blocks): Sequential(\n",
       "        (temporal_block_0): TemporalBlock(\n",
       "          (conv1): Conv1d(3, 32, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "          (chomp1): Chomp1d(chomp_size=6)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(6,))\n",
       "          (chomp2): Chomp1d(chomp_size=6)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (downsample): Conv1d(3, 32, kernel_size=(1,), stride=(1,))\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_1): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "          (chomp1): Chomp1d(chomp_size=12)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(12,), dilation=(2,))\n",
       "          (chomp2): Chomp1d(chomp_size=12)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "        (temporal_block_2): TemporalBlock(\n",
       "          (conv1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "          (chomp1): Chomp1d(chomp_size=24)\n",
       "          (relu1): ReLU()\n",
       "          (dropout1): Dropout2d(p=0.3, inplace=False)\n",
       "          (conv2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(24,), dilation=(4,))\n",
       "          (chomp2): Chomp1d(chomp_size=24)\n",
       "          (relu2): ReLU()\n",
       "          (dropout2): Dropout2d(p=0.3, inplace=False)\n",
       "          (relu): ReLU()\n",
       "        )\n",
       "      )\n",
       "      (final_layer): _FinalLayer(\n",
       "        (fc): Linear(in_features=32, out_features=2, bias=True)\n",
       "        (out_fun): Identity()\n",
       "        (squeeze): Expression(expression=squeeze_final_output) \n",
       "      )\n",
       "    )\n",
       "    (1): Conv1d(2, 2, kernel_size=(16,), stride=(1,))\n",
       "    (2): Expression(expression=squeeze) \n",
       "    (3): LogSoftmax(dim=1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_x,train_y,epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(f_params=f'model/chanbest_param.pkl',f_optimizer=f'model/chanbest_opt.pkl', f_history=f'model/chanbest_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_params(f_params=f'model/chanbest_param.pkl',f_optimizer=f'model/chanbest_opt.pkl', f_history=f'model/chanbest_history.json')\n",
    "print(\"Paramters Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block finds the accuracy, f1 score and roc auc of the valid/test set\n",
    "pred_labels=classifier.predict(eval_x)\n",
    "auc=roc_auc_score(test_y,classifier.predict_proba(eval_x)[:,1])\n",
    "accuracy=np.mean(pred_labels==test_y)\n",
    "tp=np.sum(pred_labels*test_y)\n",
    "precision=tp/np.sum(pred_labels)\n",
    "recall=tp/np.sum(test_y)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "\n",
    "print(model_name)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "print(f\"F1-Score:{f1}\")\n",
    "print(f\"roc_auc score:{auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
