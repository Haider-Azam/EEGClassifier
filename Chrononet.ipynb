{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#/********************************************\n",
    "#* Author: Rahat Ul Ain \n",
    "#* Based on chrononet implementation by Kunal Patel\n",
    "#* location: https://github.com/kunalpatel1793/Neural-Nets-Final-Project\n",
    "#********************************************/\n",
    "from pdb import set_trace\n",
    "import mne\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import braindecode \n",
    "import torch\n",
    "from torch import nn\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.dataset import ValidSplit,Dataset\n",
    "from skorch.helper import predefined_split\n",
    "import math\n",
    "import os\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN,LSTM, Dense, Activation, Bidirectional, Flatten, Dropout, Convolution2D, BatchNormalization, MaxPooling2D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#tf.config.set_logical_device_configuration(\n",
    "#        tf.config.list_physical_devices('GPU')[0],\n",
    "#        [tf.config.LogicalDeviceConfiguration(memory_limit=int(1024*3.9))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDatafromPath(path,windows=1):\n",
    "\tmatrix= np.empty((15000, 22), dtype='f')\n",
    "\tfor file in os.listdir(path):\n",
    "\t\tif '.edf' in file:\n",
    "\t\t\tf=os.path.join(path, file)\n",
    "\t\t\ttry:\n",
    "\t\t\t\tedf_file = mne.io.read_raw_edf(f,  eog = ['FP1', 'FP2', 'F3', 'F4',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'C3', 'C4',  'P3', 'P4','O1', 'O2','F7', 'F8',\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t'T3', 'T4', 'T5', 'T6','PZ','FZ', 'CZ','A1', 'A2'], verbose='error')\n",
    "\t\t\t\tprint(f)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tedf_file_down_sampled = edf_file.resample(250, npad = \"auto\")# set sampling frequency to 250 Hz\n",
    "\t\t\ted = edf_file_down_sampled.to_data_frame(picks = None, index = None, time_format = None, scalings = None,\n",
    "\t\t\tcopy = True, start = None, stop = None)# converting into dataframe\n",
    "\t\t\tFp1_Fp7 = (ed.loc[: , 'FP1']) - (ed.loc[: , 'F7'])\n",
    "\t\t\tFP2_F8 = (ed.loc[: , 'FP2']) - (ed.loc[: , 'F8'])\n",
    "\t\t\tF7_T3 = (ed.loc[: , 'F7']) - (ed.loc[: , 'T3'])\n",
    "\t\t\tF8_T4 = (ed.loc[: , 'F8']) - (ed.loc[: , 'T4'])\n",
    "\t\t\tT3_T5 = (ed.loc[: , 'T3']) - (ed.loc[: , 'T5'])\n",
    "\t\t\tT4_T6 = (ed.loc[: , 'T4']) - (ed.loc[: , 'T6'])\n",
    "\t\t\tT5_O1 = (ed.loc[: , 'T5']) - (ed.loc[: , 'O1'])\n",
    "\t\t\tT6_O2 = (ed.loc[: , 'T6']) - (ed.loc[: , 'O2'])\n",
    "\t\t\tA1_T3 = (ed.loc[: , 'A1']) - (ed.loc[: , 'T3'])\n",
    "\t\t\tT4_A2 = (ed.loc[: , 'T4']) - (ed.loc[: , 'A2'])\n",
    "\t\t\tT3_C3 = (ed.loc[: , 'T3']) - (ed.loc[: , 'C3'])\n",
    "\t\t\tC4_T4 = (ed.loc[: , 'C4']) - (ed.loc[: , 'T4'])\n",
    "\t\t\tC3_CZ = (ed.loc[: , 'C3']) - (ed.loc[: , 'CZ'])\n",
    "\t\t\tCZ_C4 = (ed.loc[: , 'CZ']) - (ed.loc[: , 'C4'])\n",
    "\t\t\tFP1_F3 = (ed.loc[: , 'FP1']) - (ed.loc[: , 'F3'])\n",
    "\t\t\tFP2_F4 = (ed.loc[: , 'FP2']) - (ed.loc[: , 'F4'])\n",
    "\t\t\tF3_C3 = (ed.loc[: , 'F3']) - (ed.loc[: , 'C3'])\n",
    "\t\t\tF4_C4 = (ed.loc[: , 'F4']) - (ed.loc[: , 'C4'])\n",
    "\t\t\tC3_P3 = (ed.loc[: , 'C3']) - (ed.loc[: , 'P3'])\n",
    "\t\t\tC4_P4 = (ed.loc[: , 'C4']) - (ed.loc[: , 'P4'])\n",
    "\t\t\tP3_O1 = (ed.loc[: , 'P3']) - (ed.loc[: , 'O1'])\n",
    "\t\t\tP4_O2 = (ed.loc[: , 'P4']) - (ed.loc[: , 'O2'])\n",
    "\t\t\tdata = {\n",
    "\t\t\t'Fp1_Fp7': Fp1_Fp7,\n",
    "\t\t\t'FP2_F8': FP2_F8,\n",
    "\t\t\t'F7_T3': F7_T3,\n",
    "\t\t\t'F8_T4': F8_T4,\n",
    "\t\t\t'T3_T5': T3_T5,\n",
    "\t\t\t'T4_T6': T4_T6,\n",
    "\t\t\t'T5_O1': T5_O1,\n",
    "\t\t\t'T6_O2': T6_O2,\n",
    "\t\t\t'A1_T3': A1_T3,\n",
    "\t\t\t'T4_A2': T4_A2,\n",
    "\t\t\t'T3_C3': T3_C3,\n",
    "\t\t\t'C4_T4': C4_T4,\n",
    "\t\t\t'C3_CZ': C3_CZ,\n",
    "\t\t\t'CZ_C4': CZ_C4,\n",
    "\t\t\t'FP1_F3': FP1_F3,\n",
    "\t\t\t'FP2_F4': FP2_F4,\n",
    "\t\t\t'F3_C3': F3_C3,\n",
    "\t\t\t'F4_C4': F4_C4,\n",
    "\t\t\t'C3_P3': C3_P3,\n",
    "\t\t\t'C4_P4': C4_P4,\n",
    "\t\t\t'P3_O1': P3_O1,\n",
    "\t\t\t'P4_O2': P4_O2\n",
    "\t\t\t}\n",
    "\t\t\tnew_data_frame = pd.DataFrame(data, columns = ['Fp1_Fp7', 'FP2_F8', 'F7_T3', 'F8_T4', 'T3_T5', 'T4_T6', 'T5_O1', 'T6_O2', 'A1_T3', 'T4_A2', 'T3_C3', 'C4_T4', 'C3_CZ',\n",
    "\t\t\t'CZ_C4', 'FP1_F3', 'FP2_F4', 'F3_C3', 'F4_C4', 'C3_P3', 'C4_P4', 'P3_O1', 'P4_O2'\n",
    "\t\t\t])\n",
    "\t\t\tfs = edf_file_down_sampled.info['sfreq']\n",
    "\t\t\t[row, col] = new_data_frame.shape\n",
    "\t\t\tn = math.ceil(row / (15000 - (fs * 5)))\n",
    "\t\t\ti = 0\n",
    "\t\t\tj = 15000\n",
    "\t\t\t#print(f\"row:{row},n:{n}\")\n",
    "\n",
    "\t\t\tfor y in range(n - 1):\n",
    "\n",
    "\t\t\t\t#print(f\"i:{i},j:{j},y:{y}\")\n",
    "\t\t\t\tif y>windows:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\t\telif y == 0 and j < row:\n",
    "\t\t\t\t\texample_1 = new_data_frame[0: 15000]\n",
    "\t\t\t\t\tmatrix=np.dstack((matrix,example_1.to_numpy()))\n",
    "\t\t\t\telif j < row:\n",
    "\t\t\t\t\texample = new_data_frame[i: j]\n",
    "\t\t\t\t\tmatrix = np.dstack((matrix, example.to_numpy()))\n",
    "\t\t\t\telse :\n",
    "\t\t\t\t\ttry:\n",
    "\t\t\t\t\t\texample = new_data_frame[-15000: ]\n",
    "\t\t\t\t\t\tmatrix = np.dstack((matrix, example.to_numpy()))\n",
    "\t\t\t\t\texcept:\n",
    "\t\t\t\t\t\tpass\n",
    "\t\t\t\t\tfinally:\n",
    "\t\t\t\t\t\tbreak\n",
    "\t\t\t\t#Goes to next window with 5 seconds of overlap\n",
    "\t\t\t\ti = int(j - (fs * 5))\n",
    "\t\t\t\tj = int(j + 15000 - (fs * 5))\n",
    "\t\t\t#print(f\"shape:{matrix.shape}\")\n",
    "\tmatrix=matrix[:,:,1:]\n",
    "\treturn matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Chrono_config\n",
    "print('starting')\n",
    "normal_train = readDatafromPath(path = Chrono_config.normaldir)\n",
    "normal_train_dim = normal_train.shape[-1]\n",
    "# print(\"normal original dim\")\n",
    "# print(normal_train_dim)\n",
    "normal_train_zeros = np.zeros(normal_train_dim)\n",
    "# print(\"zeros array dim\")\n",
    "# print(normal_train_zeros)\n",
    "abnormal_train = readDatafromPath(path = Chrono_config.abnormaldir)\n",
    "abnormal_train_dim = abnormal_train.shape[-1]\n",
    "#print(abnormal_train_dim)\n",
    "abnormal_train_ones = np.ones(abnormal_train_dim)\n",
    "#print(abnormal_train_dim)\n",
    "\n",
    "train_data = np.dstack((normal_train, abnormal_train))\n",
    "train_label = np.append(normal_train_zeros, abnormal_train_ones)\n",
    "\n",
    "train_data = np.swapaxes(train_data,0,2)\n",
    "\n",
    "bs,t,f = train_data.shape\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(train_data.dtype)\n",
    "print(train_label.dtype)\n",
    "#enc_labels = to_categorical(train_label, num_classes=2)              \n",
    "#train_label= enc_labels\n",
    "print(train_data.shape)\n",
    "print(train_label.shape)\n",
    "print(train_data.dtype)\n",
    "print(train_label.dtype)\n",
    "print('training labels have been loaded')\n",
    "del normal_train,normal_train_dim,normal_train_zeros,abnormal_train,abnormal_train_dim,abnormal_train_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------CHRONONET Testing-----------------------\n",
    "from tensorflow.keras.layers import Dense,concatenate,Flatten,GRU,Conv1D\n",
    "from tensorflow.keras import Model,Input\n",
    "inputsin= Input(shape=(t,f))\n",
    "# ------------------First Inception\n",
    "tower1 = Conv1D(32, 2, strides=2,activation='relu',padding=\"causal\")(inputsin)\n",
    "tower1 = BatchNormalization()(tower1)\n",
    "tower2 = Conv1D(32, 4, strides=2,activation='relu',padding=\"causal\")(inputsin)\n",
    "tower2 = BatchNormalization()(tower2)\n",
    "tower3 = Conv1D(32, 8, strides=2,activation='relu',padding=\"causal\")(inputsin)\n",
    "tower3 = BatchNormalization()(tower3)\n",
    "x = concatenate([tower1,tower2,tower3],axis=2)\n",
    "x = Dropout(0.45)(x)\n",
    "\n",
    "# ----------------------Second Inception\n",
    "tower1 = Conv1D(32, 2, strides=2,activation='relu',padding=\"causal\")(x)\n",
    "tower1 = BatchNormalization()(tower1)\n",
    "tower2 = Conv1D(32, 4, strides=2,activation='relu',padding=\"causal\")(x)\n",
    "tower2 = BatchNormalization()(tower2)\n",
    "tower3 = Conv1D(32, 8, strides=2,activation='relu',padding=\"causal\")(x)\n",
    "tower3 = BatchNormalization()(tower3)\n",
    "x = concatenate([tower1,tower2,tower3],axis=2)\n",
    "x = Dropout(0.45)(x)\n",
    "\n",
    "# ----------------------------------Third Inception\n",
    "tower1 = Conv1D(32, 2, strides=2,activation='relu',padding=\"causal\")(x)\n",
    "tower1 = BatchNormalization()(tower1)\n",
    "tower2 = Conv1D(32, 4, strides=2,activation='relu',padding=\"causal\")(x)\n",
    "tower2 = BatchNormalization()(tower2)\n",
    "tower3 = Conv1D(32, 8, strides=2,activation='relu',padding=\"causal\")(x)\n",
    "tower3 = BatchNormalization()(tower3)\n",
    "x = concatenate([tower1,tower2,tower3],axis=2)\n",
    "x = Dropout(0.55)(x)\n",
    "\n",
    "res1 = GRU(32,activation='tanh',return_sequences=True)(x)\n",
    "res2 = GRU(32,activation='tanh',return_sequences=True)(res1)\n",
    "res1_2 = concatenate([res1,res2],axis=2)\n",
    "res3 = GRU(32,activation='tanh',return_sequences=True)(res1_2)\n",
    "x = concatenate([res1,res2,res3])\n",
    "x = GRU(32,activation='tanh')(x)\n",
    "\n",
    "predictions = Dense(2,activation='softmax')(x)\n",
    "model = Model(inputs=inputsin, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "print(model.metrics_names)\n",
    "print(model.summary())\n",
    "\n",
    "# early stopping\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.01, mode='min', verbose=1, patience=25)                          #patience\n",
    "mc = ModelCheckpoint('modelbest_acc.hdf5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)        #filepath (save model as)\n",
    "mces = ModelCheckpoint('modelbest_loss.hdf5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)      #filepath (save model as)\n",
    "del inputsin,tower1,tower2,tower3,x,res1,res2,res1_2,res3,predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "hist=model.fit(train_data,train_label,validation_split=0.2,epochs=500,batch_size=1,verbose=1,callbacks=[es, mc,mces],shuffle=True) #epochs #split #\n",
    "print('The End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "E:/NMT_dataset/normal/eval\\0000024.edf\n",
      "E:/NMT_dataset/normal/eval\\0000044.edf\n",
      "E:/NMT_dataset/normal/eval\\0000074.edf\n",
      "E:/NMT_dataset/normal/eval\\0000077.edf\n",
      "E:/NMT_dataset/normal/eval\\0000117.edf\n",
      "E:/NMT_dataset/normal/eval\\0000124.edf\n",
      "E:/NMT_dataset/normal/eval\\0000156.edf\n",
      "E:/NMT_dataset/normal/eval\\0000194.edf\n",
      "E:/NMT_dataset/normal/eval\\0000204.edf\n",
      "E:/NMT_dataset/normal/eval\\0000207.edf\n",
      "E:/NMT_dataset/normal/eval\\0000214.edf\n",
      "E:/NMT_dataset/normal/eval\\0000254.edf\n",
      "E:/NMT_dataset/normal/eval\\0000274.edf\n",
      "E:/NMT_dataset/normal/eval\\0000276.edf\n",
      "E:/NMT_dataset/normal/eval\\0000336.edf\n",
      "E:/NMT_dataset/normal/eval\\0000396.edf\n",
      "E:/NMT_dataset/normal/eval\\0000436.edf\n",
      "E:/NMT_dataset/normal/eval\\0000454.edf\n",
      "E:/NMT_dataset/normal/eval\\0000504.edf\n",
      "E:/NMT_dataset/normal/eval\\0000526.edf\n",
      "E:/NMT_dataset/normal/eval\\0000537.edf\n",
      "E:/NMT_dataset/normal/eval\\0000544.edf\n",
      "E:/NMT_dataset/normal/eval\\0000566.edf\n",
      "E:/NMT_dataset/normal/eval\\0000584.edf\n",
      "E:/NMT_dataset/normal/eval\\0000596.edf\n",
      "E:/NMT_dataset/normal/eval\\0000614.edf\n",
      "E:/NMT_dataset/normal/eval\\0000616.edf\n",
      "E:/NMT_dataset/normal/eval\\0000617.edf\n",
      "E:/NMT_dataset/normal/eval\\0000624.edf\n",
      "E:/NMT_dataset/normal/eval\\0000666.edf\n",
      "E:/NMT_dataset/normal/eval\\0000676.edf\n",
      "E:/NMT_dataset/normal/eval\\0000714.edf\n",
      "E:/NMT_dataset/normal/eval\\0000744.edf\n",
      "E:/NMT_dataset/normal/eval\\0000784.edf\n",
      "E:/NMT_dataset/normal/eval\\0000824.edf\n",
      "E:/NMT_dataset/normal/eval\\0000844.edf\n",
      "E:/NMT_dataset/normal/eval\\0000894.edf\n",
      "E:/NMT_dataset/normal/eval\\0000914.edf\n",
      "E:/NMT_dataset/normal/eval\\0000934.edf\n",
      "E:/NMT_dataset/normal/eval\\0000947.edf\n",
      "E:/NMT_dataset/normal/eval\\0000986.edf\n",
      "E:/NMT_dataset/normal/eval\\0001056.edf\n",
      "E:/NMT_dataset/normal/eval\\0001057.edf\n",
      "E:/NMT_dataset/normal/eval\\0001096.edf\n",
      "E:/NMT_dataset/normal/eval\\0001114.edf\n",
      "E:/NMT_dataset/normal/eval\\0001146.edf\n",
      "E:/NMT_dataset/normal/eval\\0001164.edf\n",
      "E:/NMT_dataset/normal/eval\\0001196.edf\n",
      "E:/NMT_dataset/normal/eval\\0001204.edf\n",
      "E:/NMT_dataset/normal/eval\\0001234.edf\n",
      "E:/NMT_dataset/normal/eval\\0001264.edf\n",
      "E:/NMT_dataset/normal/eval\\0001267.edf\n",
      "E:/NMT_dataset/normal/eval\\0001347.edf\n",
      "E:/NMT_dataset/normal/eval\\0001356.edf\n",
      "E:/NMT_dataset/normal/eval\\0001376.edf\n",
      "E:/NMT_dataset/normal/eval\\0001416.edf\n",
      "E:/NMT_dataset/normal/eval\\0001447.edf\n",
      "E:/NMT_dataset/normal/eval\\0001454.edf\n",
      "E:/NMT_dataset/normal/eval\\0001456.edf\n",
      "E:/NMT_dataset/normal/eval\\0001496.edf\n",
      "E:/NMT_dataset/normal/eval\\0001534.edf\n",
      "E:/NMT_dataset/normal/eval\\0001536.edf\n",
      "E:/NMT_dataset/normal/eval\\0001566.edf\n",
      "E:/NMT_dataset/normal/eval\\0001574.edf\n",
      "E:/NMT_dataset/normal/eval\\0001594.edf\n",
      "E:/NMT_dataset/normal/eval\\0001684.edf\n",
      "E:/NMT_dataset/normal/eval\\0001706.edf\n",
      "E:/NMT_dataset/normal/eval\\0001724.edf\n",
      "E:/NMT_dataset/normal/eval\\0001776.edf\n",
      "E:/NMT_dataset/normal/eval\\0001794.edf\n",
      "E:/NMT_dataset/normal/eval\\0001826.edf\n",
      "E:/NMT_dataset/normal/eval\\0001854.edf\n",
      "E:/NMT_dataset/normal/eval\\0001856.edf\n",
      "E:/NMT_dataset/normal/eval\\0001894.edf\n",
      "E:/NMT_dataset/normal/eval\\0001896.edf\n",
      "E:/NMT_dataset/normal/eval\\0001966.edf\n",
      "E:/NMT_dataset/normal/eval\\0001967.edf\n",
      "E:/NMT_dataset/normal/eval\\0002016.edf\n",
      "E:/NMT_dataset/normal/eval\\0002047.edf\n",
      "E:/NMT_dataset/normal/eval\\0002064.edf\n",
      "E:/NMT_dataset/normal/eval\\0002106.edf\n",
      "E:/NMT_dataset/normal/eval\\0002114.edf\n",
      "E:/NMT_dataset/normal/eval\\0002164.edf\n",
      "E:/NMT_dataset/normal/eval\\0002166.edf\n",
      "E:/NMT_dataset/normal/eval\\0002196.edf\n",
      "E:/NMT_dataset/normal/eval\\0002244.edf\n",
      "E:/NMT_dataset/normal/eval\\0002276.edf\n",
      "E:/NMT_dataset/normal/eval\\0002304.edf\n",
      "E:/NMT_dataset/normal/eval\\0002316.edf\n",
      "E:/NMT_dataset/normal/eval\\0002344.edf\n",
      "E:/NMT_dataset/normal/eval\\0002366.edf\n",
      "E:/NMT_dataset/normal/eval\\0002375.edf\n",
      "E:/NMT_dataset/normal/eval\\0002384.edf\n",
      "E:/NMT_dataset/normal/eval\\0002406.edf\n",
      "E:/NMT_dataset/normal/eval\\0002407.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000036.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000046.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000076.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000080.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000093.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000116.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000123.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000203.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000206.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000246.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000256.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000283.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000286.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000316.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000346.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000363.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000406.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000426.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000476.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000496.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000506.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000513.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000546.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000573.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000583.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000643.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000746.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000783.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000786.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000816.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000823.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000826.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000838.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000866.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000868.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000886.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000896.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000906.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000916.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000926.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0000993.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001016.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001033.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001153.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001156.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001161.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001173.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001183.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001216.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001266.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001293.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001302.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001306.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001331.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001333.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001353.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001383.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001406.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001433.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001436.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001466.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001486.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001513.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001573.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001583.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001586.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001593.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001616.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001653.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001683.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001703.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001713.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001833.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001843.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001887.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001893.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0001953.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002053.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002063.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002091.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002143.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002153.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002181.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002203.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002249.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002266.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002303.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002339.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002373.edf\n",
      "E:/NMT_dataset/abnormal/eval\\0002380.edf\n"
     ]
    }
   ],
   "source": [
    "import Chrono_config\n",
    "print('starting')\n",
    "normal_eval = readDatafromPath(path = Chrono_config.eval_normaldir,windows=8)\n",
    "normal_eval_dim = normal_eval.shape[-1]\n",
    "# print(\"normal original dim\")\n",
    "# print(normal_eval_dim)\n",
    "normal_eval_zeros = np.zeros(normal_eval_dim)\n",
    "# print(\"zeros array dim\")\n",
    "# print(normal_eval_zeros)\n",
    "\n",
    "abnormal_eval = readDatafromPath(path = Chrono_config.eval_abnormaldir,windows=8)\n",
    "abnormal_eval_dim = abnormal_eval.shape[-1]\n",
    "#print(abnormal_eval_dim)\n",
    "abnormal_eval_ones = np.ones(abnormal_eval_dim)\n",
    "#print(abnormal_eval_dim)\n",
    "\n",
    "eval_data = np.dstack((normal_eval, abnormal_eval))\n",
    "eval_label = np.append(normal_eval_zeros, abnormal_eval_ones)\n",
    "\n",
    "eval_data = np.swapaxes(eval_data,0,2)\n",
    "\n",
    "bs,t,f = eval_data.shape\n",
    "\n",
    "#enc_labels = to_categorical(eval_label, num_classes=2)\n",
    "#eval_label= enc_labels\n",
    "del normal_eval,normal_eval_dim,normal_eval_zeros,abnormal_eval,abnormal_eval_dim,abnormal_eval_ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.io.savemat(\"D:/chrono_eval.mat\",{\"x\":eval_data,\"y\":eval_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "mid=len(train_label)//2\n",
    "scipy.io.savemat(\"D:/chrono_train_1.mat\",{\"x\":train_data[:mid,:,:],\"y\":train_label[:mid,:]})\n",
    "scipy.io.savemat(\"D:/chrono_train_2.mat\",{\"x\":train_data[mid:,:,:],\"y\":train_label[mid:,:]})\n",
    "scipy.io.savemat(\"D:/chrono_eval.mat\",{\"x\":eval_data,\"y\":eval_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "inputs=scipy.io.loadmat(\"D:/chrono_train_1.mat\")\n",
    "train_data=inputs[\"x\"]\n",
    "train_label=inputs[\"y\"]\n",
    "inputs=scipy.io.loadmat(\"D:/chrono_train_2.mat\")\n",
    "train_data=np.concatenate((train_data,inputs[\"x\"]),axis=0)\n",
    "train_label=np.concatenate((train_label,inputs[\"y\"]),axis=0)\n",
    "train_label=train_label.argmax(axis=-1)\n",
    "bs,t,f = train_data.shape\n",
    "inputs=scipy.io.loadmat(\"D:/chrono_eval.mat\")\n",
    "eval_data=inputs[\"x\"]\n",
    "eval_label=inputs[\"y\"].argmax(axis=-1)\n",
    "del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception_Block(torch.nn.Module):\n",
    "  def __init__(self,t=32,out_channels=32,pad=0):\n",
    "    super().__init__()\n",
    "    self.tower1= nn.Conv1d(in_channels=t,out_channels=32,kernel_size=2,stride=2,padding=pad)\n",
    "    self.tower2= nn.Conv1d(in_channels=t,out_channels=32,kernel_size=4,stride=2,padding=1+pad)\n",
    "    self.tower3= nn.Conv1d(in_channels=t,out_channels=32,kernel_size=8,stride=2,padding=3+pad)\n",
    "    self.bnorm=nn.BatchNorm1d(32)\n",
    "    self.relu=nn.ReLU()\n",
    "    self.dropout = nn.Dropout1d(0.45)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    t1=self.tower1(inputs)\n",
    "    t1=self.relu(t1)\n",
    "    t1=self.bnorm(t1)\n",
    "\n",
    "    t2=self.tower2(inputs)\n",
    "    t2=self.relu(t2)\n",
    "    t2=self.bnorm(t2)\n",
    "\n",
    "    t3=self.tower3(inputs)\n",
    "    t3=self.relu(t3)\n",
    "    t3=self.bnorm(t3)\n",
    "\n",
    "    output=torch.cat([t1,t2,t3],dim=1)\n",
    "    output=self.dropout(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChronoNet(torch.nn.Module):\n",
    "  def __init__(self,t=32,out_channels=32):\n",
    "    super().__init__()\n",
    "    self.block1= Inception_Block(t,out_channels)\n",
    "    self.block2= Inception_Block(96,out_channels,pad=1)\n",
    "    self.block3= Inception_Block(96,out_channels)\n",
    "    self.gru1=nn.GRU(input_size=96,hidden_size=32,batch_first=True)\n",
    "    self.gru2=nn.GRU(input_size=32,hidden_size=32,batch_first=True)\n",
    "    self.gru3=nn.GRU(input_size=64,hidden_size=32,batch_first=True)\n",
    "    self.gru4=nn.GRU(input_size=96,hidden_size=32,batch_first=True)\n",
    "    self.tanh=nn.Tanh()\n",
    "    self.classifier = nn.Linear(32,2)\n",
    "    self.softmax=nn.Softmax(dim=1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "\n",
    "    b1=self.block1(inputs.mT)\n",
    "\n",
    "    b2=self.block2(b1)\n",
    "\n",
    "    b3=self.block3(b2)\n",
    "\n",
    "    res1,_=self.gru1(b3.mT)\n",
    "    res1=self.tanh(res1)\n",
    "    res2,_=self.gru2(res1)\n",
    "    res2=self.tanh(res2)\n",
    "    res1_2=torch.cat([res1,res2],dim=2)\n",
    "    res3,_=self.gru3(res1_2)\n",
    "    res3=self.tanh(res3)\n",
    "    x=torch.cat([res1,res2,res3],dim=2)\n",
    "    x,_=self.gru4(x)\n",
    "    x=self.tanh(x)\n",
    "    if x.dim()==2:\n",
    "      x=x[-1,:]\n",
    "    elif x.dim()==3:\n",
    "      x=x[:,-1,:]\n",
    "    output=self.classifier(x)\n",
    "    output=self.softmax(output)\n",
    "    return output\n",
    "#Causal conv1d difficult, causal padding is left padding of dilation_rate*(kernel_size-1) where rate is 1\n",
    "#Will have to define a class in order to make chrononet\n",
    "model=ChronoNet(t=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "a=torch.rand(size=(3,22,15000))\n",
    "out=model.forward(a)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=ChronoNet(\n",
       "    (block1): Inception_Block(\n",
       "      (tower1): Conv1d(15000, 32, kernel_size=(2,), stride=(2,))\n",
       "      (tower2): Conv1d(15000, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (tower3): Conv1d(15000, 32, kernel_size=(8,), stride=(2,), padding=(3,))\n",
       "      (bnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout1d(p=0.45, inplace=False)\n",
       "    )\n",
       "    (block2): Inception_Block(\n",
       "      (tower1): Conv1d(96, 32, kernel_size=(2,), stride=(2,), padding=(1,))\n",
       "      (tower2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(2,))\n",
       "      (tower3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(4,))\n",
       "      (bnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout1d(p=0.45, inplace=False)\n",
       "    )\n",
       "    (block3): Inception_Block(\n",
       "      (tower1): Conv1d(96, 32, kernel_size=(2,), stride=(2,))\n",
       "      (tower2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (tower3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(3,))\n",
       "      (bnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout1d(p=0.45, inplace=False)\n",
       "    )\n",
       "    (gru1): GRU(96, 32, batch_first=True)\n",
       "    (gru2): GRU(32, 32, batch_first=True)\n",
       "    (gru3): GRU(64, 32, batch_first=True)\n",
       "    (gru4): GRU(96, 32, batch_first=True)\n",
       "    (tanh): Tanh()\n",
       "    (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params='ChronoNetbest_param.pkl',f_optimizer='ChronoNetbest_opt.pkl',\n",
    "              f_history='ChronoNetbest_history.json')\n",
    "classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=nn.CrossEntropyLoss(),\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(Dataset(eval_data,eval_label)),\n",
    "        optimizer__lr=0.00003,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=16,\n",
    "        device=\"cuda\",\n",
    "        callbacks=[\"accuracy\",\"f1\",'roc_auc',cp],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramters Loaded\n"
     ]
    }
   ],
   "source": [
    "classifier.load_params(\n",
    "        f_params=f'model/ChronoNetbest_param.pkl', f_optimizer=f'model/ChronoNetbest_opt.pkl', f_history=f'model/ChronoNetbest_history.json')\n",
    "print(\"Paramters Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     10            \u001b[36m0.9519\u001b[0m      \u001b[32m0.8229\u001b[0m        \u001b[35m0.4194\u001b[0m           \u001b[31m0.8942\u001b[0m            0.5565      0.2293        0.7476           0.6483        56.9080\n",
      "     11            \u001b[36m0.9562\u001b[0m      \u001b[32m0.8402\u001b[0m        \u001b[35m0.4054\u001b[0m           \u001b[31m0.9042\u001b[0m            0.5521      0.2104        0.7544           0.6396        56.9490\n",
      "     12            0.9445      0.7868        \u001b[35m0.3973\u001b[0m           \u001b[31m0.9126\u001b[0m            0.5295      0.1008        0.7791           0.6273        56.7230\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=ChronoNet(\n",
       "    (block1): Inception_Block(\n",
       "      (tower1): Conv1d(15000, 32, kernel_size=(2,), stride=(2,))\n",
       "      (tower2): Conv1d(15000, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (tower3): Conv1d(15000, 32, kernel_size=(8,), stride=(2,), padding=(3,))\n",
       "      (bnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout1d(p=0.45, inplace=False)\n",
       "    )\n",
       "    (block2): Inception_Block(\n",
       "      (tower1): Conv1d(96, 32, kernel_size=(2,), stride=(2,), padding=(1,))\n",
       "      (tower2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(2,))\n",
       "      (tower3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(4,))\n",
       "      (bnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout1d(p=0.45, inplace=False)\n",
       "    )\n",
       "    (block3): Inception_Block(\n",
       "      (tower1): Conv1d(96, 32, kernel_size=(2,), stride=(2,))\n",
       "      (tower2): Conv1d(96, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "      (tower3): Conv1d(96, 32, kernel_size=(8,), stride=(2,), padding=(3,))\n",
       "      (bnorm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU()\n",
       "      (dropout): Dropout1d(p=0.45, inplace=False)\n",
       "    )\n",
       "    (gru1): GRU(96, 32, batch_first=True)\n",
       "    (gru2): GRU(32, 32, batch_first=True)\n",
       "    (gru3): GRU(64, 32, batch_first=True)\n",
       "    (gru4): GRU(96, 32, batch_first=True)\n",
       "    (tanh): Tanh()\n",
       "    (classifier): Linear(in_features=32, out_features=2, bias=True)\n",
       "    (softmax): Softmax(dim=1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_data,train_label,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.5367847411444142\n",
      "F1-Score:0.17475728155339806\n"
     ]
    }
   ],
   "source": [
    "predicted_labels=classifier.predict(eval_data)\n",
    "accuracy=np.mean(predicted_labels==eval_label)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "tp=np.sum(predicted_labels*eval_label)\n",
    "precision=tp/np.sum(predicted_labels)\n",
    "recall=tp/np.sum(eval_label)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(f\"F1-Score:{f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('testing')\n",
    "# load the saved accuracy model\n",
    "savedmodel = load_model('model3flipped_loss.hdf5')                      # model name\n",
    "print('model loaded')\n",
    "test_score = savedmodel.evaluate(test_data, test_labels, batch_size=32)\n",
    "print (\"Evaluation loss and Evaluation accuracy for best accuracy model is: \", test_score)\n",
    "print ('The End')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
