{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "from torch import nn\n",
    "from torch.nn.functional import elu,relu,leaky_relu\n",
    "import torchvision.transforms as transform\n",
    "import braindecode \n",
    "from braindecode.models import *\n",
    "from braindecode.models.modules import Expression\n",
    "from braindecode.models.functions import squeeze_final_output\n",
    "from braindecode.datasets import BaseDataset, BaseConcatDataset,create_from_X_y\n",
    "from braindecode.models.util import to_dense_prediction_model, get_output_shape\n",
    "import pandas as pd\n",
    "import resampy\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import Checkpoint,ProgressBar\n",
    "from skorch.helper import predefined_split\n",
    "from config import *\n",
    "from dataset import *\n",
    "from braindecode.preprocessing import create_fixed_length_windows\n",
    "from mne import set_log_level\n",
    "set_log_level(False)\n",
    "device = 'cuda' if cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_functions = []\n",
    "preproc_functions.append( lambda data, fs: (data[:, int(sec_to_cut * fs):-int(sec_to_cut * fs)], fs))\n",
    "preproc_functions.append(lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))\n",
    "if max_abs_val is not None:\n",
    "    preproc_functions.append(lambda data, fs:(np.clip(data, -max_abs_val, max_abs_val), fs))\n",
    "preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,sampling_freq,axis=1,filter='kaiser_fast'),sampling_freq))\n",
    "if divisor is not None:\n",
    "    preproc_functions.append(lambda data, fs: (data / divisor, fs))\n",
    "dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='train',\n",
    "                           sensor_types=sensor_types)\n",
    "if test_on_eval:\n",
    "    test_dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='eval',\n",
    "                           sensor_types=sensor_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get timesteps, we can use numpy.reshape\n",
    "def create_windows(X,y,stride=sampling_freq):\n",
    "    trials=[]\n",
    "    labels=[]\n",
    "    for i in range(len(X)):\n",
    "        no_of_windows=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "        for j in range(no_of_windows):\n",
    "            trials.append(X[i][:,j*stride:j*stride+input_time_length])\n",
    "            labels.append(y[i])\n",
    "    trials=np.array(trials)\n",
    "    labels=np.array(labels)\n",
    "    return trials,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset.load()\n",
    "if test_on_eval:\n",
    "    test_x,test_y=test_dataset.load()\n",
    "#    test_trials,test_labels=create_windows(test_x,test_y,sampling_freq*10)\n",
    "#    test_set=Dataset(test_trials,test_labels)\n",
    "#    del test_x,test_y,test_trials,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block will be used to separate the abnormal and normal training trials\n",
    "abnormal_indexes=np.nonzero(y)[0][::-1]\n",
    "abnormal=[]\n",
    "for i in abnormal_indexes:\n",
    "    abnormal.append(X.pop(i))\n",
    "abnormal_labels=y[i:]\n",
    "y=y[:i]\n",
    "del abnormal_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stride=sampling_freq*10\n",
    "trials=[]\n",
    "labels=[]\n",
    "for i in range(len(X)):\n",
    "    no_of_windows=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "    for j in range(no_of_windows):\n",
    "        trials.append(X[i][:,j*stride:j*stride+input_time_length])\n",
    "        labels.append(y[i])\n",
    "stride=sampling_freq*5\n",
    "for i in range(len(abnormal)):\n",
    "    no_of_windows=((abnormal[i].shape[1]-input_time_length)//stride)-1\n",
    "    for j in range(no_of_windows):\n",
    "        trials.append(abnormal[i][:,j*stride:j*stride+input_time_length])\n",
    "        labels.append(abnormal_labels[i])\n",
    "del abnormal,X,y,stride,i,j,no_of_windows,abnormal_labels\n",
    "trials=np.array(trials)\n",
    "labels=np.array(labels)\n",
    "train_set=Dataset(trials,labels)\n",
    "del trials,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del divisor,max_abs_val,sec_to_cut,duration_recording_mins,preproc_functions,data_folders\n",
    "def create_set(X, y, inds):\n",
    "    \"\"\"\n",
    "    X list and y nparray\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    for i in inds:\n",
    "        new_X.append(X[i])\n",
    "    new_y = y[inds]\n",
    "    return (new_X, new_y)\n",
    "#Use of TrainValidTestSplitter is not necessary in newer versions of braindecode\n",
    "class TrainValidSplitter(object):\n",
    "    def __init__(self, n_folds, i_valid_fold, shuffle):\n",
    "        self.n_folds = n_folds\n",
    "        self.i_valid_fold = i_valid_fold\n",
    "        self.rng = np.random.RandomState(39483948)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def split(self, X, y):\n",
    "        if len(X) < self.n_folds:\n",
    "            raise ValueError(\"Less Trials: {:d} than folds: {:d}\".format(\n",
    "                len(X), self.n_folds\n",
    "            ))\n",
    "        indices=np.arange(len(y))\n",
    "        #Compared to paper, the valid set will be unbalanced\n",
    "        batch_size=len(X)//self.n_folds\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(indices)\n",
    "        valid_inds=indices[self.i_valid_fold*batch_size:(self.i_valid_fold+1)*batch_size]\n",
    "        train_inds = np.setdiff1d(indices,valid_inds)\n",
    "        train_set = create_set(X, y, train_inds)\n",
    "        valid_set = create_set(X, y, valid_inds)\n",
    "        return train_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval==False:\n",
    "    splitter=TrainValidSplitter(n_folds,i_test_fold,True)\n",
    "    train_set,valid_set=splitter.split(X,y)\n",
    "    del X,y\n",
    "    X,y=train_set\n",
    "    valid_X,valid_y=valid_set\n",
    "    del n_folds,i_test_fold,train_set,valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "#we take a 20 second stride as 1 sample and 1 second stride takes too long\n",
    "stride=sampling_freq*10\n",
    "train_set=create_from_X_y(X,y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                       window_stride_samples=stride)\n",
    "if test_on_eval==False:\n",
    "    valid_set=create_from_X_y(valid_X,valid_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=sampling_freq)\n",
    "    del valid_X,valid_y\n",
    "elif test_on_eval:\n",
    "    test_set=create_from_X_y(test_x,test_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=sampling_freq)\n",
    "    del test_x,test_y\n",
    "del stride,ch_names,X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 2])\n"
     ]
    }
   ],
   "source": [
    "n_classes = 2\n",
    "if model_name==\"shallow\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = ShallowFBCSPNet(n_chans,\n",
    "                                    n_classes,\n",
    "                                    n_filters_time=n_start_chans,\n",
    "                                    n_filters_spat=n_start_chans,\n",
    "                                    input_window_samples=input_time_length,\n",
    "                                    final_conv_length='auto',)\n",
    "    test=torch.ones(size=(7,21,6000))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"deep\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                         n_filters_time=n_start_chans,\n",
    "                         n_filters_spat=n_start_chans,\n",
    "                         input_window_samples=input_time_length,\n",
    "                         n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                         n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                         n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                         final_conv_length='auto',\n",
    "                        stride_before_pool=True)\n",
    "    test=torch.ones(size=(7,21,6000,1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"deep_smac\":\n",
    "    optimizer_lr = init_lr\n",
    "    if model_name == 'deep_smac':\n",
    "            do_batch_norm = False\n",
    "    else:\n",
    "        assert model_name == 'deep_smac_bnorm'\n",
    "        do_batch_norm = True\n",
    "    double_time_convs = False\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 12\n",
    "    filter_length_3 = 14\n",
    "    filter_length_4 = 30\n",
    "    filter_time_length = 21\n",
    "    #final_conv_length = 1\n",
    "    first_nonlin = leaky_relu\n",
    "    first_pool_mode = 'mean'\n",
    "    later_nonlin = leaky_relu\n",
    "    later_pool_mode = 'mean'\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    pool_time_length = 1\n",
    "    pool_time_stride = 2\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    n_start_chans = n_filters_start\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "            n_filters_time=n_start_chans,\n",
    "            n_filters_spat=n_start_chans,\n",
    "            input_window_samples=input_time_length,\n",
    "            n_filters_2=int(n_start_chans * n_chan_factor),\n",
    "            n_filters_3=int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "            n_filters_4=int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "            final_conv_length='auto',\n",
    "            batch_norm=True,\n",
    "            drop_prob=drop_prob,\n",
    "            filter_length_2=filter_length_2,\n",
    "            filter_length_3=filter_length_3,\n",
    "            filter_length_4=filter_length_4,\n",
    "            filter_time_length=filter_time_length,\n",
    "            first_conv_nonlin=first_nonlin,\n",
    "            first_pool_mode=first_pool_mode,\n",
    "            later_conv_nonlin=later_nonlin,\n",
    "            later_pool_mode=later_pool_mode,\n",
    "            pool_time_length=pool_time_length,\n",
    "            pool_time_stride=pool_time_stride,\n",
    "            split_first_layer=split_first_layer,\n",
    "            stride_before_pool=True)\n",
    "    test=torch.ones(size=(6,21,6000,1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del do_batch_norm,double_time_convs,drop_prob,filter_length_2,filter_length_3,filter_length_4,filter_time_length,first_nonlin,n_chan_factor,n_start_chans,first_pool_mode,later_nonlin,later_pool_mode,n_filters_factor,n_filters_start,pool_time_length,pool_time_stride,split_first_layer\n",
    "#Works properly, fit the hybrid cnn\n",
    "elif model_name==\"hybrid\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = HybridNet(n_chans, n_classes,input_window_samples=input_time_length,)\n",
    "    test=torch.ones(size=(2,21,6000))\n",
    "    out=model.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    model.final_conv=nn.Conv2d(100,n_classes,(out_length,1),bias=True,)\n",
    "    model=nn.Sequential(model,Expression(torch.squeeze))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length\n",
    "elif model_name==\"TCN\":\n",
    "    import warnings\n",
    "    #This disables the warning of the dropout2d layers receiving 3d input\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    n_blocks=7\n",
    "    n_filters=32\n",
    "    kernel_size=24\n",
    "    drop_prob = 0.3\n",
    "    add_log_softmax=False\n",
    "    x=TCN(n_chans,n_classes,n_blocks,n_filters,kernel_size,drop_prob,add_log_softmax)\n",
    "    test=torch.ones(size=(7,21,6000,1))\n",
    "    out=x.forward(test)\n",
    "    print(out.shape)\n",
    "    out_length=out.shape[2]\n",
    "    #There is no hyperparameter where output of TCN is (Batch_Size,Classes) when input is (Batch_Size,21,6000) so add new layers to meet size\n",
    "    model=nn.Sequential(x,nn.Conv1d(n_classes,n_classes,out_length,bias=True,),Expression(torch.squeeze),nn.LogSoftmax(dim=1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length,x\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "del test,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep4Net(\n",
       "  (ensuredims): Ensure4d()\n",
       "  (dimshuffle): Expression(expression=transpose_time_to_spat) \n",
       "  (conv_time): Conv2d(1, 32, kernel_size=(21, 1), stride=(1, 1))\n",
       "  (conv_spat): Conv2d(32, 32, kernel_size=(1, 21), stride=(2, 1), bias=False)\n",
       "  (bnorm): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_nonlin): Expression(expression=leaky_relu) \n",
       "  (pool): AvgPool2dWithConv()\n",
       "  (pool_nonlin): Expression(expression=identity) \n",
       "  (drop_2): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_2): Conv2d(32, 53, kernel_size=(12, 1), stride=(2, 1), bias=False)\n",
       "  (bnorm_2): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_2): Expression(expression=leaky_relu) \n",
       "  (pool_2): AvgPool2dWithConv()\n",
       "  (pool_nonlin_2): Expression(expression=identity) \n",
       "  (drop_3): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_3): Conv2d(53, 90, kernel_size=(14, 1), stride=(2, 1), bias=False)\n",
       "  (bnorm_3): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_3): Expression(expression=leaky_relu) \n",
       "  (pool_3): AvgPool2dWithConv()\n",
       "  (pool_nonlin_3): Expression(expression=identity) \n",
       "  (drop_4): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_4): Conv2d(90, 151, kernel_size=(30, 1), stride=(2, 1), bias=False)\n",
       "  (bnorm_4): BatchNorm2d(151, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_4): Expression(expression=leaky_relu) \n",
       "  (pool_4): AvgPool2dWithConv()\n",
       "  (pool_nonlin_4): Expression(expression=identity) \n",
       "  (conv_classifier): Conv2d(151, 2, kernel_size=(355, 1), stride=(1, 1))\n",
       "  (softmax): LogSoftmax(dim=1)\n",
       "  (squeeze): Expression(expression=squeeze_final_output) \n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials,labels=create_windows(X,y,sampling_freq*10)\n",
    "del X,y\n",
    "train_set=Dataset(trials,labels)\n",
    "del trials,labels\n",
    "if test_on_eval==False:\n",
    "    trials,labels=create_windows(valid_X,valid_y,sampling_freq*10)\n",
    "    del valid_X,valid_y\n",
    "    valid_set=Dataset(trials,labels)\n",
    "    del trials,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "if test_on_eval==False:\n",
    "    classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(valid_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",cp],#Try ‘roc_auc’\n",
    "        warm_start=True,\n",
    "        )\n",
    "elif test_on_eval:\n",
    "    classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",cp],#Try ‘roc_auc’\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=f'{model_name}II'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads Phase 1 parameters and fit them further in phase 2\n",
    "path=f'{model_name}'\n",
    "if test_on_eval:\n",
    "    classifier.load_params(\n",
    "        f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "    print(\"Paramters Loaded\")\n",
    "    path=f'{model_name}II'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval:\n",
    "    path=f'{model_name}II'\n",
    "elif test_on_eval==False:\n",
    "    path=f'{model_name}'\n",
    "try:\n",
    "    classifier.load_params(\n",
    "        f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "    print(\"Paramters Loaded\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the history of training the neural network\n",
    "classifier.history_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_set,y=None,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(\n",
    "    f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "#torch.save({\"model\":classifier.module_.state_dict(),\"optimizer\":classifier.optimizer_.state_dict()}, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval==False:\n",
    "    pred_labels=classifier.predict(valid_set)\n",
    "    actual_labels=[label[1] for label in valid_set]\n",
    "elif test_on_eval:\n",
    "    pred_labels=classifier.predict(test_set)\n",
    "    actual_labels=[label[1] for label in test_set]\n",
    "actual_labels=np.array(actual_labels)\n",
    "accuracy=np.mean(pred_labels==actual_labels)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "tp=np.sum(pred_labels*actual_labels)\n",
    "precision=tp/np.sum(pred_labels)\n",
    "recall=tp/np.sum(actual_labels)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(f\"F1-Score:{f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model on proper test set according to paper\n",
    "if test_on_eval:\n",
    "    try:\n",
    "        del train_set,test_set\n",
    "    except:\n",
    "        pass\n",
    "    test_x,test_y=test_dataset.load()\n",
    "    ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "    #Stride between windows is set to sampling frequency as written in paper\n",
    "    test_set=create_from_X_y(test_x,test_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=sampling_freq)\n",
    "    del test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval:    \n",
    "    pred_labels=classifier.predict(test_set)\n",
    "    actual_labels=[label[1] for label in test_set]\n",
    "    actual_labels=np.array(actual_labels)\n",
    "    accuracy=np.mean(pred_labels==actual_labels)\n",
    "    print(f\"Accuracy:{accuracy}\")\n",
    "    tp=np.sum(pred_labels*actual_labels)\n",
    "    precision=tp/np.sum(pred_labels)\n",
    "    recall=tp/np.sum(actual_labels)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    print(f\"F1-Score:{f1}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramters Loaded\n"
     ]
    }
   ],
   "source": [
    "#This will load the model and parameters and then replace it with one whose classification layer is removed\n",
    "from skorch import NeuralNet\n",
    "network=NeuralNet(module=model,criterion=torch.nn.modules.loss.NLLLoss,batch_size=batch_size,device=device)\n",
    "network.initialize()\n",
    "network.load_params(\n",
    "    f_params=f'model/{model_name}best_param.pkl', f_optimizer=f'model/{model_name}best_opt.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")\n",
    "network.module_=torch.nn.Sequential(*(list(network.module_.children())[:-3]),nn.modules.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Ensure4d()\n",
       "  (1): Expression(expression=transpose_time_to_spat) \n",
       "  (2): Conv2d(1, 32, kernel_size=(21, 1), stride=(1, 1))\n",
       "  (3): Conv2d(32, 32, kernel_size=(1, 21), stride=(2, 1), bias=False)\n",
       "  (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (5): Expression(expression=leaky_relu) \n",
       "  (6): AvgPool2dWithConv()\n",
       "  (7): Expression(expression=identity) \n",
       "  (8): Dropout(p=0.244445, inplace=False)\n",
       "  (9): Conv2d(32, 53, kernel_size=(12, 1), stride=(2, 1), bias=False)\n",
       "  (10): BatchNorm2d(53, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): Expression(expression=leaky_relu) \n",
       "  (12): AvgPool2dWithConv()\n",
       "  (13): Expression(expression=identity) \n",
       "  (14): Dropout(p=0.244445, inplace=False)\n",
       "  (15): Conv2d(53, 90, kernel_size=(14, 1), stride=(2, 1), bias=False)\n",
       "  (16): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (17): Expression(expression=leaky_relu) \n",
       "  (18): AvgPool2dWithConv()\n",
       "  (19): Expression(expression=identity) \n",
       "  (20): Dropout(p=0.244445, inplace=False)\n",
       "  (21): Conv2d(90, 151, kernel_size=(30, 1), stride=(2, 1), bias=False)\n",
       "  (22): BatchNorm2d(151, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (23): Expression(expression=leaky_relu) \n",
       "  (24): AvgPool2dWithConv()\n",
       "  (25): Expression(expression=identity) \n",
       "  (26): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.module_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53605\n"
     ]
    }
   ],
   "source": [
    "test=torch.ones(size=(2,21,6000))\n",
    "feat=network.predict(test).shape[1]\n",
    "print(features)\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads dataset, finds smallest trial, with this, we find number of windows using stride and convert it to array of windows of trials\n",
    "#shape is (no_of_trials,no_of_windows,channels,input_time_length) in the end\n",
    "X,y=dataset.load()\n",
    "trials,labels=create_windows(X,y,sampling_freq*10)\n",
    "del X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 504000 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#This will calculate the features before classification layer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m features\u001b[39m=\u001b[39mnetwork\u001b[39m.\u001b[39;49mpredict(trials)\n\u001b[0;32m      3\u001b[0m \u001b[39mdel\u001b[39;00m trials\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skorch\\net.py:1615\u001b[0m, in \u001b[0;36mNeuralNet.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1585\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m   1586\u001b[0m     \u001b[39m\"\"\"Where applicable, return class labels for samples in X.\u001b[39;00m\n\u001b[0;32m   1587\u001b[0m \n\u001b[0;32m   1588\u001b[0m \u001b[39m    If the module's forward method returns multiple outputs as a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1613\u001b[0m \n\u001b[0;32m   1614\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1615\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skorch\\net.py:1578\u001b[0m, in \u001b[0;36mNeuralNet.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1576\u001b[0m nonlin \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_predict_nonlinearity()\n\u001b[0;32m   1577\u001b[0m y_probas \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1578\u001b[0m \u001b[39mfor\u001b[39;00m yp \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_iter(X, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m   1579\u001b[0m     yp \u001b[39m=\u001b[39m yp[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(yp, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m yp\n\u001b[0;32m   1580\u001b[0m     yp \u001b[39m=\u001b[39m nonlin(yp)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\skorch\\net.py:1423\u001b[0m, in \u001b[0;36mNeuralNet.forward_iter\u001b[1;34m(self, X, training, device)\u001b[0m\n\u001b[0;32m   1421\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_dataset(X)\n\u001b[0;32m   1422\u001b[0m iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(dataset, training\u001b[39m=\u001b[39mtraining)\n\u001b[1;32m-> 1423\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m iterator:\n\u001b[0;32m   1424\u001b[0m     yp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mevaluation_step(batch, training\u001b[39m=\u001b[39mtraining)\n\u001b[0;32m   1425\u001b[0m     \u001b[39myield\u001b[39;00m to_device(yp, device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:145\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type([collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed])\n\u001b[0;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m         \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    148\u001b[0m         \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:145\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m         \u001b[39mreturn\u001b[39;00m elem_type([collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed])\n\u001b[0;32m    146\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m         \u001b[39m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    148\u001b[0m         \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mif\u001b[39;00m np_str_obj_array_pattern\u001b[39m.\u001b[39msearch(elem\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mstr) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[39m.\u001b[39mformat(elem\u001b[39m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m collate([torch\u001b[39m.\u001b[39;49mas_tensor(b) \u001b[39mfor\u001b[39;49;00m b \u001b[39min\u001b[39;49;00m batch], collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 504000 bytes."
     ]
    }
   ],
   "source": [
    "#This will calculate the features before classification layer\n",
    "features=network.predict(trials)\n",
    "del trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset.load()\n",
    "no_of_trials=0\n",
    "stride=sampling_freq*10\n",
    "for i in range(len(X)):\n",
    "    no_of_trials+=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "features=np.zeros(shape=(no_of_trials,feat),dtype=np.float32)\n",
    "labels=[]\n",
    "position=0\n",
    "for i in range(len(X)):\n",
    "    windows=[]\n",
    "    no_of_windows=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "    for j in range(no_of_windows):\n",
    "        windows.append(X[i][:,j*stride:j*stride+input_time_length])\n",
    "        labels.append(y[i])\n",
    "    windows=np.array(windows)\n",
    "    features[position:position+no_of_windows]=network.predict(windows)\n",
    "    position+=no_of_windows\n",
    "del i,j,no_of_windows,X,y,windows\n",
    "labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "OverflowError",
     "evalue": "Python int too large to convert to C long",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOverflowError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#This saves the features along with labels of each trial in a .mat file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m scipy\u001b[39m.\u001b[39;49mio\u001b[39m.\u001b[39;49msavemat(\u001b[39m\"\u001b[39;49m\u001b[39mE:/train_features.mat\u001b[39;49m\u001b[39m\"\u001b[39;49m,{\u001b[39m\"\u001b[39;49m\u001b[39mx\u001b[39;49m\u001b[39m\"\u001b[39;49m:features,\u001b[39m\"\u001b[39;49m\u001b[39my\u001b[39;49m\u001b[39m\"\u001b[39;49m:labels})\n\u001b[0;32m      3\u001b[0m \u001b[39mdel\u001b[39;00m features,labels\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio.py:300\u001b[0m, in \u001b[0;36msavemat\u001b[1;34m(file_name, mdict, appendmat, format, long_field_names, do_compression, oned_as)\u001b[0m\n\u001b[0;32m    298\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    299\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mFormat should be \u001b[39m\u001b[39m'\u001b[39m\u001b[39m4\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39m5\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 300\u001b[0m MW\u001b[39m.\u001b[39;49mput_variables(mdict)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:892\u001b[0m, in \u001b[0;36mMatFile5Writer.put_variables\u001b[1;34m(self, mdict, write_header)\u001b[0m\n\u001b[0;32m    890\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream\u001b[39m.\u001b[39mwrite(out_str)\n\u001b[0;32m    891\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# not compressing\u001b[39;00m\n\u001b[1;32m--> 892\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_matrix_writer\u001b[39m.\u001b[39;49mwrite_top(var, name\u001b[39m.\u001b[39;49mencode(\u001b[39m'\u001b[39;49m\u001b[39mlatin1\u001b[39;49m\u001b[39m'\u001b[39;49m), is_global)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:633\u001b[0m, in \u001b[0;36mVarWriter5.write_top\u001b[1;34m(self, arr, name, is_global)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_var_name \u001b[39m=\u001b[39m name\n\u001b[0;32m    632\u001b[0m \u001b[39m# write the header and data\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite(arr)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:672\u001b[0m, in \u001b[0;36mVarWriter5.write\u001b[1;34m(self, arr)\u001b[0m\n\u001b[0;32m    670\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_char(narr, codec)\n\u001b[0;32m    671\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 672\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_numeric(narr)\n\u001b[0;32m    673\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_matrix_tag(mat_tag_pos)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:698\u001b[0m, in \u001b[0;36mVarWriter5.write_numeric\u001b[1;34m(self, arr)\u001b[0m\n\u001b[0;32m    696\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_element(arr\u001b[39m.\u001b[39mimag)\n\u001b[0;32m    697\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 698\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_element(arr)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:539\u001b[0m, in \u001b[0;36mVarWriter5.write_element\u001b[1;34m(self, arr, mdtype)\u001b[0m\n\u001b[0;32m    537\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_smalldata_element(arr, mdtype, byte_count)\n\u001b[0;32m    538\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 539\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwrite_regular_element(arr, mdtype, byte_count)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\scipy\\io\\matlab\\_mio5.py:553\u001b[0m, in \u001b[0;36mVarWriter5.write_regular_element\u001b[1;34m(self, arr, mdtype, byte_count)\u001b[0m\n\u001b[0;32m    551\u001b[0m tag \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((), NDT_TAG_FULL)\n\u001b[0;32m    552\u001b[0m tag[\u001b[39m'\u001b[39m\u001b[39mmdtype\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m mdtype\n\u001b[1;32m--> 553\u001b[0m tag[\u001b[39m'\u001b[39;49m\u001b[39mbyte_count\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m byte_count\n\u001b[0;32m    554\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_bytes(tag)\n\u001b[0;32m    555\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrite_bytes(arr)\n",
      "\u001b[1;31mOverflowError\u001b[0m: Python int too large to convert to C long"
     ]
    }
   ],
   "source": [
    "#This saves the features along with labels of each trial in a .mat file\n",
    "scipy.io.savemat(\"E:/train_features.mat\",{\"x\":features,\"y\":labels})\n",
    "del features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y=test_dataset.load()\n",
    "test_trials,test_labels=create_windows(test_x,test_y,sampling_freq*10)\n",
    "del test_x,test_y\n",
    "test_features=[]\n",
    "for i in range(len(test_trials)):\n",
    "    out=network.predict(test_trials[i])\n",
    "    test_features.append(out)\n",
    "test_features=np.asarray(test_features)\n",
    "del test_trials,out\n",
    "scipy.io.savemat(\"E:/test_features.mat\",{\"x\":test_features,\"y\":test_labels})\n",
    "del test_features,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "inputs=scipy.io.loadmat(\"E:/train_features.mat\")\n",
    "X=inputs[\"x\"]\n",
    "y=inputs[\"y\"].squeeze()\n",
    "_,t,f=X.shape\n",
    "del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msoftmax(h3)\n\u001b[0;32m     14\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m---> 15\u001b[0m model \u001b[39m=\u001b[39m SimpleModel(f)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'f' is not defined"
     ]
    }
   ],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "  def __init__(self,input_features):\n",
    "    super().__init__()\n",
    "    self.lstm = torch.nn.LSTM(input_size=input_features, hidden_size=50, batch_first=True)\n",
    "    self.fc = torch.nn.Linear(50, 2)\n",
    "    self.tanh = torch.nn.Tanh()\n",
    "    self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    _, (h1_T,_) = self.lstm(inputs)\n",
    "    h2=self.tanh(h1_T.squeeze())\n",
    "    h3 = self.fc(h2)       # inplace of h2[-1,:,:] we can use h2_T. Both are identical\n",
    "    output = self.softmax(h3)\n",
    "    return output\n",
    "model = SimpleModel(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs=scipy.io.loadmat(\"E:/test_features.mat\")\n",
    "test_X=inputs[\"x\"]\n",
    "test_y=inputs[\"y\"].squeeze()\n",
    "test_set=Dataset(test_X,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params='LSTMbest_param.pkl',f_optimizer='LSTMbest_opt.pkl',f_history='LSTMbest_history.json')\n",
    "classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=init_lr,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",cp],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try deep smac by itself and as feature extractor and determine effectiveness\n",
    "classifier.fit(X,y=y,epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=classifier.predict(test_X)\n",
    "accuracy=np.mean(out==test_y)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "tp=np.sum(out*test_y)\n",
    "precision=tp/np.sum(out)\n",
    "recall=tp/np.sum(test_y)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(f\"F1-Score:{f1}\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
