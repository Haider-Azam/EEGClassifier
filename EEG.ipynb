{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "from torch import nn\n",
    "import torchvision.transforms as transform\n",
    "import braindecode \n",
    "from braindecode.models import *\n",
    "from braindecode.models.modules import Expression\n",
    "from braindecode.models.functions import squeeze_final_output\n",
    "from braindecode.datasets import BaseDataset, BaseConcatDataset,create_from_X_y\n",
    "from braindecode.models.util import to_dense_prediction_model, get_output_shape\n",
    "import pandas as pd\n",
    "import resampy\n",
    "from skorch.callbacks import Checkpoint,ProgressBar\n",
    "from skorch.helper import predefined_split\n",
    "from config import *\n",
    "from dataset import *\n",
    "from braindecode.preprocessing import create_fixed_length_windows\n",
    "from mne import set_log_level\n",
    "set_log_level(False)\n",
    "device = 'cuda' if cuda else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_functions = []\n",
    "preproc_functions.append( lambda data, fs: (data[:, int(sec_to_cut * fs):-int(sec_to_cut * fs)], fs))\n",
    "preproc_functions.append(lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))\n",
    "if max_abs_val is not None:\n",
    "    preproc_functions.append(lambda data, fs:(np.clip(data, -max_abs_val, max_abs_val), fs))\n",
    "preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,sampling_freq,axis=1,filter='kaiser_fast'),sampling_freq))\n",
    "if divisor is not None:\n",
    "    preproc_functions.append(lambda data, fs: (data / divisor, fs))\n",
    "dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='train',\n",
    "                           sensor_types=sensor_types)\n",
    "if test_on_eval:\n",
    "    test_dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='eval',\n",
    "                           sensor_types=sensor_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=dataset.load()\n",
    "if test_on_eval:\n",
    "    test_x,test_y=test_dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del divisor,max_abs_val,sec_to_cut,duration_recording_mins,preproc_functions\n",
    "def create_set(X, y, inds):\n",
    "    \"\"\"\n",
    "    X list and y nparray\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    for i in inds:\n",
    "        new_X.append(X[i])\n",
    "    new_y = y[inds]\n",
    "    return (new_X, new_y)\n",
    "#Use of TrainValidTestSplitter is not necessary in newer versions of braindecode\n",
    "class TrainValidSplitter(object):\n",
    "    def __init__(self, n_folds, i_valid_fold, shuffle):\n",
    "        self.n_folds = n_folds\n",
    "        self.i_valid_fold = i_valid_fold\n",
    "        self.rng = np.random.RandomState(39483948)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def split(self, X, y):\n",
    "        if len(X) < self.n_folds:\n",
    "            raise ValueError(\"Less Trials: {:d} than folds: {:d}\".format(\n",
    "                len(X), self.n_folds\n",
    "            ))\n",
    "        indices=np.arange(len(y))\n",
    "        #Compared to paper, the valid set will be unbalanced\n",
    "        batch_size=len(X)//self.n_folds\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(indices)\n",
    "        valid_inds=indices[self.i_valid_fold*batch_size:(self.i_valid_fold+1)*batch_size]\n",
    "        train_inds = np.setdiff1d(indices,valid_inds)\n",
    "        train_set = create_set(X, y, train_inds)\n",
    "        valid_set = create_set(X, y, valid_inds)\n",
    "        return train_set, valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval==False:\n",
    "    splitter=TrainValidSplitter(n_folds,i_test_fold,True)\n",
    "    train_set,valid_set=splitter.split(X,y)\n",
    "    del X,y\n",
    "    X,y=train_set\n",
    "    valid_X,valid_y=valid_set\n",
    "    del n_folds,i_test_fold,train_set,valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "#we take a 20 second stride as 1 sample and 1 second stride takes too long\n",
    "stride=sampling_freq*30\n",
    "train_set=create_from_X_y(X,y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                       window_stride_samples=stride)\n",
    "if test_on_eval==False:\n",
    "    valid_set=create_from_X_y(valid_X,valid_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=stride)\n",
    "    del valid_X,valid_y\n",
    "elif test_on_eval:\n",
    "    test_set=create_from_X_y(test_x,test_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=stride)\n",
    "    del test_x,test_y\n",
    "del stride,ch_names,X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 2\n",
    "if model_name==\"shallow\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = ShallowFBCSPNet(n_chans,\n",
    "                                    n_classes,\n",
    "                                    n_filters_time=n_start_chans,\n",
    "                                    n_filters_spat=n_start_chans,\n",
    "                                    input_window_samples=input_time_length,\n",
    "                                    final_conv_length='auto',)\n",
    "    test=torch.ones(size=(7,21,6000))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "if model_name==\"deep\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                         n_filters_time=n_start_chans,\n",
    "                         n_filters_spat=n_start_chans,\n",
    "                         input_window_samples=input_time_length,\n",
    "                         n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                         n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                         n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                         final_conv_length='auto',\n",
    "                        stride_before_pool=True)\n",
    "    test=torch.ones(size=(7,21,6000,1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "#Works properly, fit the hybrid cnn\n",
    "if model_name==\"hybrid\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = HybridNet(n_chans, n_classes,input_window_samples=input_time_length,)\n",
    "    test=torch.ones(size=(2,21,6000))\n",
    "    out=model.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    model.final_conv=nn.Conv2d(100,n_classes,(out_length,1),bias=True,)\n",
    "    model=nn.Sequential(model,Expression(torch.squeeze))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length\n",
    "if model_name==\"TCN\":\n",
    "    import warnings\n",
    "    #This disables the warning of the dropout2d layers receiving 3d input\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    n_blocks=7\n",
    "    n_filters=32\n",
    "    kernel_size=24\n",
    "    drop_prob = 0.3\n",
    "    add_log_softmax=False\n",
    "    x=TCN(n_chans,n_classes,n_blocks,n_filters,kernel_size,drop_prob,add_log_softmax)\n",
    "    test=torch.ones(size=(7,21,6000,1))\n",
    "    out=x.forward(test)\n",
    "    print(out.shape)\n",
    "    out_length=out.shape[2]\n",
    "    #There is no hyperparameter where output of TCN is (Batch_Size,Classes) when input is (Batch_Size,21,6000) so add new layers to meet size\n",
    "    model=nn.Sequential(x,nn.Conv1d(n_classes,n_classes,out_length,bias=True,),Expression(torch.squeeze),nn.LogSoftmax(dim=1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length,x\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "del test,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "if test_on_eval==False:\n",
    "    classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(valid_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",cp],\n",
    "        warm_start=True,\n",
    "        )\n",
    "elif test_on_eval:\n",
    "    classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",cp],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads Phase 1 parameters and fit them further in phase 2\n",
    "path=f'{model_name}'\n",
    "if test_on_eval:\n",
    "    classifier.load_params(\n",
    "        f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "    print(\"Paramters Loaded\")\n",
    "    path=f'{model_name}II'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval:\n",
    "    path=f'{model_name}II'\n",
    "elif test_on_eval==False:\n",
    "    path=f'{model_name}'\n",
    "try:\n",
    "    classifier.load_params(\n",
    "        f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "    print(\"Paramters Loaded\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shows the history of training the neural network\n",
    "classifier.history_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(train_set,y=None,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(\n",
    "    f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "#torch.save({\"model\":classifier.module_.state_dict(),\"optimizer\":classifier.optimizer_.state_dict()}, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval==False:\n",
    "    pred_labels=classifier.predict(valid_set)\n",
    "    actual_labels=[label[1] for label in valid_set]\n",
    "elif test_on_eval:\n",
    "    pred_labels=classifier.predict(test_set)\n",
    "    actual_labels=[label[1] for label in test_set]\n",
    "actual_labels=np.array(actual_labels)\n",
    "accuracy=np.mean(pred_labels==actual_labels)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "tp=np.sum(pred_labels*actual_labels)\n",
    "precision=tp/np.sum(pred_labels)\n",
    "recall=tp/np.sum(actual_labels)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(f\"F1-Score:{f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test the model on proper test set according to paper\n",
    "if test_on_eval:\n",
    "    try:\n",
    "        del train_set,test_set\n",
    "    except:\n",
    "        pass\n",
    "    test_x,test_y=test_dataset.load()\n",
    "    ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "    #Stride between windows is set to sampling frequency as written in paper\n",
    "    test_set=create_from_X_y(test_x,test_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=sampling_freq)\n",
    "    del test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval:    \n",
    "    pred_labels=classifier.predict(test_set)\n",
    "    actual_labels=[label[1] for label in test_set]\n",
    "    actual_labels=np.array(actual_labels)\n",
    "    accuracy=np.mean(pred_labels==actual_labels)\n",
    "    print(f\"Accuracy:{accuracy}\")\n",
    "    tp=np.sum(pred_labels*actual_labels)\n",
    "    precision=tp/np.sum(pred_labels)\n",
    "    recall=tp/np.sum(actual_labels)\n",
    "    f1=2*precision*recall/(precision+recall)\n",
    "    print(f\"F1-Score:{f1}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will load the model and parameters and then replace it with one whose classification layer is removed\n",
    "from skorch import NeuralNet\n",
    "network=NeuralNet(module=model,criterion=torch.nn.modules.loss.NLLLoss,batch_size=batch_size,device=device)\n",
    "network.initialize()\n",
    "network.load_params(\n",
    "    f_params=f'model/{model_name}best_param.pkl', f_optimizer=f'model/{model_name}best_opt.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")\n",
    "network.module_=torch.nn.Sequential(*(list(network.module_.children())[:-3]),nn.modules.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads dataset, finds smallest trial, with this, we find number of windows using stride and convert it to array of windows of trials\n",
    "#shape is (no_of_trials,no_of_windows,channels,input_time_length) in the end\n",
    "X,y=dataset.load()\n",
    "min_shape=X[0].shape[1]\n",
    "for arr in X:\n",
    "    if min_shape>arr.shape[1]:\n",
    "        min_shape=arr.shape[1]\n",
    "print(min_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#30 second stride between windows\n",
    "stride=sampling_freq*20\n",
    "no_of_windows=((min_shape-input_time_length)//stride)\n",
    "#To make the features for the LSTM, we will make all the trials of same length as smallest to allow batch training\n",
    "for i in range(len(X)):\n",
    "    windows=[]\n",
    "    for j in range(no_of_windows):\n",
    "        windows.append(X[i][:,j*stride:j*stride+input_time_length])\n",
    "    X[i]=np.asarray(windows)\n",
    "trials=np.array(X)\n",
    "del windows,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will calculate the features before classification layer\n",
    "features=[]\n",
    "for i in range(len(trials)):\n",
    "    out=network.predict(trials[i])\n",
    "    features.append(out)\n",
    "features=np.asarray(features)\n",
    "del trials,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This saves the features along with labels of each trial in a .mat file\n",
    "scipy.io.savemat(\"E:/train_features.mat\",{\"x\":features,\"y\":y})\n",
    "del features,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "inputs=scipy.io.loadmat(\"E:/train_features.mat\")\n",
    "X=inputs[\"x\"][:,:16,:]\n",
    "y=inputs[\"y\"].squeeze()\n",
    "_,t,f=X.shape\n",
    "del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import LSTM, Dense,Input\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "inputsin= Input(shape=(t,f))\n",
    "\n",
    "x=LSTM(50,activation='tanh')(inputsin)\n",
    "predictions = Dense(2,activation='softmax')(x)\n",
    "model = Model(inputs=inputsin, outputs=predictions)\n",
    "del inputsin,predictions,x\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.1, amsgrad=False)\n",
    "\n",
    "model.compile(optimizer = opt, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "es = EarlyStopping(monitor='val_loss', min_delta=0.01, mode='min', verbose=1, patience=15)\n",
    "mc = ModelCheckpoint('model/LSTM_acc.hdf5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n",
    "mces = ModelCheckpoint('model/LSTM_loss.hdf5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "166/167 [============================>.] - ETA: 0s - loss: 0.0052 - accuracy: 0.9962\n",
      "Epoch 1: val_accuracy improved from -inf to 0.23952, saving model to model\\LSTM_acc.hdf5\n",
      "\n",
      "Epoch 1: val_loss improved from inf to 5.83784, saving model to model\\LSTM_loss.hdf5\n",
      "167/167 [==============================] - 10s 38ms/step - loss: 0.0051 - accuracy: 0.9963 - val_loss: 5.8378 - val_accuracy: 0.2395\n",
      "Epoch 2/500\n",
      "165/167 [============================>.] - ETA: 0s - loss: 1.5754e-04 - accuracy: 1.0000\n",
      "Epoch 2: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 2: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 1.5796e-04 - accuracy: 1.0000 - val_loss: 6.0832 - val_accuracy: 0.2395\n",
      "Epoch 3/500\n",
      "166/167 [============================>.] - ETA: 0s - loss: 1.2066e-04 - accuracy: 1.0000\n",
      "Epoch 3: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 3: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 1.2073e-04 - accuracy: 1.0000 - val_loss: 6.2358 - val_accuracy: 0.2395\n",
      "Epoch 4/500\n",
      "165/167 [============================>.] - ETA: 0s - loss: 1.0240e-04 - accuracy: 1.0000\n",
      "Epoch 4: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 4: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 1.0212e-04 - accuracy: 1.0000 - val_loss: 6.3470 - val_accuracy: 0.2395\n",
      "Epoch 5/500\n",
      "166/167 [============================>.] - ETA: 0s - loss: 8.9684e-05 - accuracy: 1.0000\n",
      "Epoch 5: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 5: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 8.9584e-05 - accuracy: 1.0000 - val_loss: 6.4369 - val_accuracy: 0.2395\n",
      "Epoch 6/500\n",
      "167/167 [==============================] - ETA: 0s - loss: 8.0108e-05 - accuracy: 1.0000\n",
      "Epoch 6: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 6: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 8.0108e-05 - accuracy: 1.0000 - val_loss: 6.5153 - val_accuracy: 0.2395\n",
      "Epoch 7/500\n",
      "165/167 [============================>.] - ETA: 0s - loss: 7.2187e-05 - accuracy: 1.0000\n",
      "Epoch 7: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 7: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 21ms/step - loss: 7.2532e-05 - accuracy: 1.0000 - val_loss: 6.5859 - val_accuracy: 0.2395\n",
      "Epoch 8/500\n",
      "165/167 [============================>.] - ETA: 0s - loss: 6.6666e-05 - accuracy: 1.0000\n",
      "Epoch 8: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 8: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 6.6222e-05 - accuracy: 1.0000 - val_loss: 6.6515 - val_accuracy: 0.2395\n",
      "Epoch 9/500\n",
      "164/167 [============================>.] - ETA: 0s - loss: 5.9469e-05 - accuracy: 1.0000\n",
      "Epoch 9: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 9: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 5.9029e-05 - accuracy: 1.0000 - val_loss: 6.7092 - val_accuracy: 0.2395\n",
      "Epoch 10/500\n",
      "166/167 [============================>.] - ETA: 0s - loss: 5.4878e-05 - accuracy: 1.0000\n",
      "Epoch 10: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 10: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 5.4749e-05 - accuracy: 1.0000 - val_loss: 6.7635 - val_accuracy: 0.2395\n",
      "Epoch 11/500\n",
      "167/167 [==============================] - ETA: 0s - loss: 5.0959e-05 - accuracy: 1.0000\n",
      "Epoch 11: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 11: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 19ms/step - loss: 5.0959e-05 - accuracy: 1.0000 - val_loss: 6.8147 - val_accuracy: 0.2395\n",
      "Epoch 12/500\n",
      "166/167 [============================>.] - ETA: 0s - loss: 4.7719e-05 - accuracy: 1.0000\n",
      "Epoch 12: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 12: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 21ms/step - loss: 4.7603e-05 - accuracy: 1.0000 - val_loss: 6.8640 - val_accuracy: 0.2395\n",
      "Epoch 13/500\n",
      "165/167 [============================>.] - ETA: 0s - loss: 4.4879e-05 - accuracy: 1.0000\n",
      "Epoch 13: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 13: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 21ms/step - loss: 4.4569e-05 - accuracy: 1.0000 - val_loss: 6.9098 - val_accuracy: 0.2395\n",
      "Epoch 14/500\n",
      "164/167 [============================>.] - ETA: 0s - loss: 4.0368e-05 - accuracy: 1.0000\n",
      "Epoch 14: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 14: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 4.1736e-05 - accuracy: 1.0000 - val_loss: 6.9540 - val_accuracy: 0.2395\n",
      "Epoch 15/500\n",
      "167/167 [==============================] - ETA: 0s - loss: 3.9250e-05 - accuracy: 1.0000\n",
      "Epoch 15: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 15: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 3s 20ms/step - loss: 3.9250e-05 - accuracy: 1.0000 - val_loss: 6.9976 - val_accuracy: 0.2395\n",
      "Epoch 16/500\n",
      "166/167 [============================>.] - ETA: 0s - loss: 3.6692e-05 - accuracy: 1.0000\n",
      "Epoch 16: val_accuracy did not improve from 0.23952\n",
      "\n",
      "Epoch 16: val_loss did not improve from 5.83784\n",
      "167/167 [==============================] - 4s 23ms/step - loss: 3.6965e-05 - accuracy: 1.0000 - val_loss: 7.0384 - val_accuracy: 0.2365\n",
      "Epoch 16: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x209f6ab6920>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,validation_split=0.2,epochs=500,batch_size=8,verbose=1,callbacks=[es, mc,mces],shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
