{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import scipy\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import elu,relu,leaky_relu\n",
    "import braindecode \n",
    "from braindecode.models import *\n",
    "from braindecode.models.modules import Expression\n",
    "from braindecode.models.functions import squeeze_final_output,square,safe_log\n",
    "from braindecode.datasets import BaseConcatDataset,create_from_X_y\n",
    "from skorch.dataset import Dataset\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.helper import predefined_split\n",
    "from config import *\n",
    "from dataset import *\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mne import set_log_level\n",
    "import resampy\n",
    "import h5py\n",
    "from skorch.callbacks import LRScheduler\n",
    "set_log_level(False)\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "preproc_functions = []\n",
    "preproc_functions.append( lambda data, fs: (data[:, int(sec_to_cut * fs):-int(sec_to_cut * fs)], fs))\n",
    "preproc_functions.append(lambda data, fs: (data[:, :int(duration_recording_mins * 60 * fs)], fs))\n",
    "if max_abs_val is not None:\n",
    "    preproc_functions.append(lambda data, fs:(np.clip(data, -max_abs_val, max_abs_val), fs))\n",
    "preproc_functions.append(lambda data, fs: (resampy.resample(data, fs,sampling_freq,axis=1,filter='kaiser_fast'),sampling_freq))\n",
    "if divisor is not None:\n",
    "    preproc_functions.append(lambda data, fs: (data / divisor, fs))\n",
    "dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='train',\n",
    "                           sensor_types=sensor_types)\n",
    "if test_on_eval:\n",
    "    test_dataset = DiagnosisSet(n_recordings=n_recordings,\n",
    "                           max_recording_mins=max_recording_mins,\n",
    "                           preproc_functions=preproc_functions,\n",
    "                           data_folders=data_folders,\n",
    "                           train_or_eval='eval',\n",
    "                           sensor_types=sensor_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy array doesn't work as they take too much space, use BaseConcatDataset instead\n",
    "#BaseConcatDataset does work recursively with itself.\n",
    "X,y=dataset.load()\n",
    "if test_on_eval:\n",
    "    test_x,test_y=test_dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import pandas as pd\n",
    "#HDF5 implementation\n",
    "file_names=[]\n",
    "no_of_windows=[]\n",
    "split='train'\n",
    "path=os.path.join(processed_folder,split)\n",
    "for i in range(len(y)):\n",
    "    file_path=f'{path}/{i}.hdf5'\n",
    "    file_names.append(file_path)\n",
    "    no_of_trials=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "    no_of_windows.append(no_of_trials)\n",
    "    with h5py.File(file_path, 'a') as f:\n",
    "        f['x']=X[i]\n",
    "        f['y']=y[i]\n",
    "    #np.savez_compressed(file_path,x=X[i],y=y[i])\n",
    "file_names=pd.Series(file_names)\n",
    "lbs=pd.Series(y)\n",
    "train_dataframe=pd.DataFrame({'name':file_names,'label':lbs,'no_of_windows':no_of_windows})\n",
    "train_dataframe.to_excel(f\"{processed_folder}/{split}.xlsx\",index=False)\n",
    "\n",
    "\n",
    "file_names=[]\n",
    "no_of_windows=[]\n",
    "split='eval'\n",
    "path=os.path.join(processed_folder,split)\n",
    "for i in range(len(test_y)):\n",
    "    file_path=f'{path}/{i}.hdf5'\n",
    "    file_names.append(file_path)\n",
    "    no_of_trials=((test_x[i].shape[1]-input_time_length)//stride)-1\n",
    "    no_of_windows.append(no_of_trials)\n",
    "    with h5py.File(file_path, 'a') as f:\n",
    "        f['x']=test_x[i]\n",
    "        f['y']=test_y[i]\n",
    "    #np.savez_compressed(f'{path}/{i}',x=test_x[i],y=test_y[i])\n",
    "file_names=pd.Series(file_names)\n",
    "lbs=pd.Series(test_y)\n",
    "eval_dataframe=pd.DataFrame({'name':file_names,'label':lbs,'no_of_windows':no_of_windows})\n",
    "eval_dataframe.to_excel(f\"{processed_folder}/{split}.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del divisor,max_abs_val,sec_to_cut,duration_recording_mins,preproc_functions,data_folders\n",
    "def create_set(X, y, inds):\n",
    "    \"\"\"\n",
    "    X list and y nparray\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    new_X = []\n",
    "    for i in inds:\n",
    "        new_X.append(X[i])\n",
    "    new_y = y[inds]\n",
    "    return (new_X, new_y)\n",
    "\n",
    "#Use of TrainValidTestSplitter is not necessary in newer versions of braindecode\n",
    "class TrainValidSplitter(object):\n",
    "    def __init__(self, n_folds, i_valid_fold, shuffle):\n",
    "        self.n_folds = n_folds\n",
    "        self.i_valid_fold = i_valid_fold\n",
    "        self.rng = np.random.RandomState(39483948)\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def split(self, X, y):\n",
    "        if len(X) < self.n_folds:\n",
    "            raise ValueError(\"Less Trials: {:d} than folds: {:d}\".format(\n",
    "                len(X), self.n_folds\n",
    "            ))\n",
    "        indices=np.arange(len(y))\n",
    "        #Compared to paper, the valid set will be unbalanced\n",
    "        batch_size=len(X)//self.n_folds\n",
    "        if self.shuffle:\n",
    "            self.rng.shuffle(indices)\n",
    "        valid_inds=indices[self.i_valid_fold*batch_size:(self.i_valid_fold+1)*batch_size]\n",
    "        train_inds = np.setdiff1d(indices,valid_inds)\n",
    "        train_set = create_set(X, y, train_inds)\n",
    "        valid_set = create_set(X, y, valid_inds)\n",
    "        return train_set, valid_set\n",
    "    \n",
    "#To get timesteps, we can use numpy.reshape\n",
    "def create_windows(X,y,stride=sampling_freq):\n",
    "    no_of_trials=0\n",
    "    trials=[]\n",
    "    labels=[]\n",
    "    for i in range(len(X)):\n",
    "        no_of_trials+=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "\n",
    "    trials=np.zeros(shape=(no_of_trials,21,6000),dtype=np.float32)\n",
    "    position=0\n",
    "    for i in range(len(X)):\n",
    "        windows=[]\n",
    "        no_of_windows=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "        for j in range(no_of_windows):\n",
    "            windows.append(X[i][:,j*stride:j*stride+input_time_length])\n",
    "            labels.append(y[i])\n",
    "        trials[position:position+no_of_windows]=np.array(windows)\n",
    "        position+=no_of_windows\n",
    "    labels=np.array(labels)\n",
    "    return trials,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_on_eval==False:\n",
    "    splitter=TrainValidSplitter(n_folds,i_test_fold,True)\n",
    "    train_set,valid_set=splitter.split(X,y)\n",
    "    del X,y\n",
    "    X,y=train_set\n",
    "    valid_X,valid_y=valid_set\n",
    "    del n_folds,i_test_fold,train_set,valid_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Methods of data augmentation:-\n",
    "Time Warping: This involves stretching or compressing the time axis. In time-series analysis, it can lead to a better understanding of variations in time.\n",
    "Mathematical Explanation:\n",
    "x′(t)=x(a⋅t)\n",
    "where a is the warping factor.\n",
    "Window Slicing: Similar to cropping, but with fixed-size windows. Overlapping windows can also be used to increase the amount of data.\n",
    "Time Masking: Certain time steps are masked (set to zero or mean value), which can help the model become more robust to missing data.\n",
    "Noise Injection: Random noise can be added to the sequence, aiding the model in learning to ignore irrelevant variations.\n",
    "Mathematical Explanation:\n",
    "x′(t)=x(t)+N(0,σ2)\n",
    "where N(0,σ2) is Gaussian noise with mean 0 and variance σ2.\n",
    "Data Mixing: By mixing two or more sequences, you can create a new sequence. For instance, in audio processing, overlaying two sound tracks.\n",
    "Temporal Jittering: It involves adding small random shifts to the temporal alignment of the sequence. It's often used in speech and audio processing.\n",
    "Sequence-to-sequence Transformation: This involves applying complex transformations like Fourier transform followed by an inverse transformation after modifications in the frequency domain.\n",
    "Mathematical Explanation:\n",
    "X′=F−1(F(X)+N)\n",
    "where F and F−1 are the Fourier and inverse Fourier transforms, and N is a noise term.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block will be used to separate the abnormal and normal training trials\n",
    "abnormal_indexes=np.nonzero(y)[0][::-1]\n",
    "abnormal=[]\n",
    "for i in abnormal_indexes:\n",
    "    abnormal.append(X.pop(i))\n",
    "abnormal_labels=y[i:]\n",
    "y=y[:i]\n",
    "del abnormal_indexes\n",
    "factor=len(y)//len(abnormal_labels)\n",
    "print(f\"normal cases:{len(y)}\")\n",
    "print(f\"abnormal cases:{len(abnormal_labels)}\")\n",
    "print(factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "#we take a 10 second stride as 1 second stride takes too long\n",
    "stride=sampling_freq*10\n",
    "abnormal_stride=sampling_freq*10\n",
    "train_set=create_from_X_y(X,y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                       window_stride_samples=stride)\n",
    "del X,y\n",
    "if test_on_eval==False:\n",
    "    valid_set=create_from_X_y(valid_X,valid_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=sampling_freq)\n",
    "    del valid_X,valid_y\n",
    "elif test_on_eval:\n",
    "    test_set=create_from_X_y(test_x,test_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=sampling_freq)\n",
    "    del test_x,test_y\n",
    "abnormal_train_set=create_from_X_y(abnormal,abnormal_labels,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names\n",
    "                            ,window_size_samples=input_time_length,window_stride_samples=abnormal_stride)\n",
    "del abnormal,abnormal_labels\n",
    "\n",
    "print(f\"normal windows:{len(train_set)}\")\n",
    "print(f\"abnormal windows:{len(abnormal_train_set)}\")\n",
    "factor=len(train_set)//len(abnormal_train_set)\n",
    "abnormal_train_set=BaseConcatDataset([abnormal_train_set for _ in range(factor)])#Duplicates the data to match size of normal samples\n",
    "train_set=BaseConcatDataset([abnormal_train_set,train_set])\n",
    "print(f\"Total windows:{len(train_set)}\")\n",
    "del abnormal_train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block is used to save processed windows as a hdf5 file\n",
    "#This is not recommended due to the large amount of space needed to store every window\n",
    "import h5py\n",
    "import numpy\n",
    "train_labels=[]\n",
    "for sample in train_set:\n",
    "    train_labels.append(sample[1])\n",
    "f=h5py.File('E:/exp1_1/train.hdf5', 'a')\n",
    "f.create_dataset(\"Y_train\", data = train_labels,compression=\"gzip\", chunks=True)\n",
    "f.close()\n",
    "with h5py.File('E:/exp1_1/train.hdf5', 'a') as f:\n",
    "    f[\"X_train\"].resize(len(train_set),axis=0)\n",
    "    for i in range(len(train_set)):\n",
    "        f[\"X_train\"][i]=train_set[i][0]\n",
    "\n",
    "test_labels=[]\n",
    "for sample in test_set:\n",
    "    test_labels.append(sample[1])\n",
    "f=h5py.File('E:/exp1_1/test.hdf5', 'a')\n",
    "f.create_dataset(\"Y_test\", data = test_labels,compression=\"gzip\", chunks=True)\n",
    "f.close()\n",
    "with h5py.File('E:/exp1_1/test.hdf5', 'a') as f:\n",
    "    f.create_dataset(\"X_test\", data = test_set[0][0][None,:,:],compression=\"gzip\", chunks=True,maxshape=(len(test_set),21,6000))\n",
    "    f[\"X_test\"].resize(len(test_set),axis=0)\n",
    "    for i in range(len(test_set)):\n",
    "        f[\"X_test\"][i]=test_set[i][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This dataset won't work when num_workers>0 on windows\n",
    "class H5_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hd5_file,split):\n",
    "        self.path = hd5_file\n",
    "        super().__init__()\n",
    "        self.split=split\n",
    "\n",
    "    def open_hdf5(self):\n",
    "        self.h5_file = h5py.File(self.path, 'r')\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if not hasattr(self, 'h5_file'):\n",
    "            self.open_hdf5()\n",
    "\n",
    "        window=self.h5_file[f'X_{self.split}'][index]\n",
    "        label=self.h5_file[f'Y_{self.split}'][index]\n",
    "        return window,label\n",
    " \n",
    "    def __len__(self):\n",
    "        if not hasattr(self, 'h5_file'):\n",
    "            self.open_hdf5()\n",
    "        return len(self.h5_file[f'Y_{self.split}'])\n",
    "test_set=H5_Dataset('E:/exp1_1/test.hdf5','test')\n",
    "train_set=H5_Dataset('E:/exp1_1/train.hdf5','train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WindowDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, excel_path,input_time_length,stride):\n",
    "        self.input_time_length=input_time_length\n",
    "        self.stride=stride\n",
    "        super().__init__()\n",
    "        excel_file=pd.read_excel(excel_path)\n",
    "        self.file_names=excel_file['name'].to_numpy(dtype=str)\n",
    "        self.label=excel_file['label'].to_numpy()\n",
    "        self.windows=excel_file['no_of_windows'].to_numpy()\n",
    "        self.windows=np.cumsum(self.windows)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        position=np.argmax(self.windows > index)\n",
    "        #Calculates position relative to a recording\n",
    "        if position>0:\n",
    "            window_position=(index-self.windows[position-1])\n",
    "        else:\n",
    "            window_position=index\n",
    "        window_position*=self.stride\n",
    "        #print(self.file_names[position])\n",
    "        with h5py.File(self.file_names[position], 'r') as h5_file:\n",
    "            window=h5_file['x'][:,window_position : window_position + self.input_time_length]\n",
    "\n",
    "        label=self.label[position]\n",
    "        return window,label\n",
    " \n",
    "    def __len__(self):\n",
    "        return self.windows[-1]\n",
    "test_set=WindowDataset(f'{processed_folder}/eval.xlsx',input_time_length,stride)\n",
    "train_set=WindowDataset(f'{processed_folder}/train.xlsx',input_time_length,stride)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.06785323, -0.07633081, -0.06958825, ...,  0.01386732,\n",
       "          0.00623307,  0.00142142],\n",
       "        [-0.06785323, -0.07633081, -0.06958825, ...,  0.01386732,\n",
       "          0.00623307,  0.00142142],\n",
       "        [ 0.06539513,  0.06591074,  0.04241068, ..., -0.00847166,\n",
       "         -0.00454815, -0.00354735],\n",
       "        ...,\n",
       "        [-0.04003958, -0.03828413, -0.02247119, ...,  0.01964251,\n",
       "          0.01304058,  0.00952763],\n",
       "        [-0.07677436, -0.09000701, -0.09551565, ...,  0.02451516,\n",
       "          0.01189578,  0.00276193],\n",
       "        [-0.07714031, -0.08671535, -0.0830165 , ...,  0.03405689,\n",
       "          0.02825564,  0.01848716]], dtype=float32),\n",
       " 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"shallow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 2])\n",
      "shallow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\braindecode\\models\\base.py:180: UserWarning: LogSoftmax final layer will be removed! Please adjust your loss function accordingly (e.g. CrossEntropyLoss)!\n",
      "  warnings.warn(\"LogSoftmax final layer will be removed! \" +\n"
     ]
    }
   ],
   "source": [
    "criterion=torch.nn.NLLLoss\n",
    "n_classes = 2\n",
    "if model_name==\"shallow\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = ShallowFBCSPNet(n_chans,\n",
    "                                    n_classes,\n",
    "                                    n_filters_time=n_start_chans,\n",
    "                                    n_filters_spat=n_start_chans,\n",
    "                                    n_times=input_time_length,\n",
    "                                    final_conv_length='auto',)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name == 'shallow_smac':\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #conv_nonlin = identity\n",
    "    do_batch_norm = True\n",
    "    drop_prob = 0.328794\n",
    "    filter_time_length = 56\n",
    "    n_filters_spat = 73\n",
    "    n_filters_time = 24\n",
    "    pool_mode = 'max'\n",
    "    #pool_nonlin = identity\n",
    "    pool_time_length = 84\n",
    "    pool_time_stride = 3\n",
    "    split_first_layer = True\n",
    "    model = ShallowFBCSPNet(in_chans=n_chans, n_classes=n_classes,\n",
    "                            n_filters_time=n_filters_time,\n",
    "                            n_filters_spat=n_filters_spat,\n",
    "                            n_times=input_time_length,\n",
    "                            final_conv_length='auto',\n",
    "                            #conv_nonlin=conv_nonlin,\n",
    "                            batch_norm=do_batch_norm,\n",
    "                            drop_prob=drop_prob,\n",
    "                            filter_time_length=filter_time_length,\n",
    "                            pool_mode=pool_mode,\n",
    "                            #pool_nonlin=pool_nonlin,\n",
    "                            pool_time_length=pool_time_length,\n",
    "                            pool_time_stride=pool_time_stride,\n",
    "                            split_first_layer=split_first_layer,\n",
    "                            )\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"deep\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                         n_filters_time=n_start_chans,\n",
    "                         n_filters_spat=n_start_chans,\n",
    "                         n_times=input_time_length,\n",
    "                         n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                         n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                         n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                         final_conv_length='auto',\n",
    "                        stride_before_pool=True)\n",
    "    test=torch.ones(size=(6,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"deep_smac\" or model_name == 'deep_smac_bnorm':\n",
    "    optimizer_lr = 0.0000625\n",
    "    if model_name == 'deep_smac':\n",
    "            do_batch_norm = False\n",
    "    else:\n",
    "        do_batch_norm = True\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 12\n",
    "    filter_length_3 = 14\n",
    "    filter_length_4 = 32\n",
    "    filter_time_length = 21\n",
    "    #final_conv_length = 1\n",
    "    first_nonlin = elu\n",
    "    first_pool_mode = 'mean'\n",
    "    later_nonlin = elu\n",
    "    later_pool_mode = 'mean'\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    pool_time_length = 1\n",
    "    pool_time_stride = 2\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    n_start_chans = n_filters_start\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "            n_filters_time=n_start_chans,\n",
    "            n_filters_spat=n_start_chans,\n",
    "            n_times=input_time_length,\n",
    "            n_filters_2=int(n_start_chans * n_chan_factor),\n",
    "            n_filters_3=int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "            n_filters_4=int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "            final_conv_length='auto',\n",
    "            stride_before_pool=True,\n",
    "            drop_prob=drop_prob,\n",
    "            filter_length_2=filter_length_2,\n",
    "            filter_length_3=filter_length_3,\n",
    "            filter_length_4=filter_length_4,\n",
    "            filter_time_length=filter_time_length,\n",
    "            first_conv_nonlin=first_nonlin,\n",
    "            first_pool_mode=first_pool_mode,\n",
    "            later_conv_nonlin=later_nonlin,\n",
    "            later_pool_mode=later_pool_mode,\n",
    "            pool_time_length=pool_time_length,\n",
    "            pool_time_stride=pool_time_stride,\n",
    "            split_first_layer=split_first_layer\n",
    "            )\n",
    "    test=torch.ones(size=(6,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del do_batch_norm,drop_prob,filter_length_2,filter_length_3,filter_length_4,filter_time_length,first_nonlin,n_chan_factor,n_start_chans,first_pool_mode,later_nonlin,later_pool_mode,n_filters_factor,n_filters_start,pool_time_length,pool_time_stride,split_first_layer\n",
    "#Works properly, fit the hybrid cnn\n",
    "elif model_name==\"hybrid\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = HybridNet(n_chans, n_classes,n_times=input_time_length,)\n",
    "    test=torch.ones(size=(2,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    model.final_layer=nn.Conv2d(100,n_classes,(out_length,1),bias=True,)\n",
    "    model=nn.Sequential(model,nn.Flatten(),nn.LogSoftmax(dim=1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length\n",
    "elif model_name==\"TCN\":\n",
    "    import warnings\n",
    "    #This disables the warning of the dropout2d layers receiving 3d input\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    n_blocks=7\n",
    "    n_filters=32\n",
    "    kernel_size=24\n",
    "    drop_prob = 0.3\n",
    "    x=TCN(n_chans,n_classes,n_blocks,n_filters,kernel_size,drop_prob)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=x.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    #There is no hyperparameter where output of TCN is (Batch_Size,Classes) when input is (Batch_Size,21,6000) so add new layers to meet size\n",
    "    model=nn.Sequential(x,nn.Conv1d(n_classes,n_classes,out_length,bias=True,),nn.LogSoftmax(dim=1),nn.Flatten())\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length,x\n",
    "elif model_name==\"shallow_deep\":\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 12\n",
    "    filter_length_3 = 14\n",
    "    filter_length_4 = 32\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    #n_start_chans = n_filters_start\n",
    "\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    conv_time_length=25\n",
    "    first_conv_nonlin=relu\n",
    "    first_pool_nonlin=safe_log\n",
    "    later_conv_nonlin=elu\n",
    "    later_pool_nonlin=safe_log\n",
    "    first_pool_mode = \"mean\"\n",
    "    later_pool_mode = \"mean\"\n",
    "    pool_time_length=15\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                            n_filters_time=n_start_chans,\n",
    "                            n_filters_spat=n_start_chans,\n",
    "                            n_times=input_time_length,\n",
    "                            n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                            n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                            n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                            final_conv_length='auto',\n",
    "                            first_pool_nonlin=first_pool_nonlin,\n",
    "                            first_conv_nonlin=first_conv_nonlin,\n",
    "                            #later_pool_nonlin=later_pool_nonlin,\n",
    "                            #later_conv_nonlin=later_conv_nonlin,\n",
    "                            filter_time_length=conv_time_length,\n",
    "                            pool_time_length=pool_time_length,\n",
    "                            first_pool_mode=first_pool_mode,\n",
    "                            later_pool_mode=later_pool_mode,\n",
    "                            split_first_layer=split_first_layer,\n",
    "                            drop_prob=drop_prob,\n",
    "                            filter_length_2=filter_length_2,\n",
    "                            filter_length_3=filter_length_3,\n",
    "                            filter_length_4=filter_length_4,\n",
    "                            )\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "#    out=model.forward(test)\n",
    "#    print(out.shape)\n",
    "\n",
    "elif model_name==\"attention\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    model=ATCNet(n_chans,n_classes,input_time_length//sampling_freq,sampling_freq,concat=True)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"transformer\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    criterion=torch.nn.CrossEntropyLoss\n",
    "    n_filters_time=20\n",
    "    att_depth=8\n",
    "    filter_time_length=35\n",
    "    att_heads=5\n",
    "    model=EEGConformer(n_outputs=n_classes,n_chans=n_chans,n_times=input_time_length,input_window_seconds=input_time_length//sampling_freq,\n",
    "                       sfreq=sampling_freq,final_fc_length=7860,n_filters_time=n_filters_time,att_depth=att_depth,\n",
    "                       filter_time_length=filter_time_length,att_heads=att_heads)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "del test\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep4Net(\n",
       "  (ensuredims): Ensure4d()\n",
       "  (dimshuffle): Rearrange('batch C T 1 -> batch 1 T C')\n",
       "  (conv_time_spat): CombinedConv(\n",
       "    (conv_time): Conv2d(1, 25, kernel_size=(25, 1), stride=(1, 1))\n",
       "    (conv_spat): Conv2d(25, 25, kernel_size=(1, 21), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (bnorm): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_nonlin): Expression(expression=relu) \n",
       "  (pool): AvgPool2dWithConv()\n",
       "  (pool_nonlin): Expression(expression=safe_log) \n",
       "  (drop_2): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_2): Conv2d(25, 41, kernel_size=(12, 1), stride=(1, 1), bias=False)\n",
       "  (bnorm_2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_2): Expression(expression=elu) \n",
       "  (pool_2): AvgPool2dWithConv()\n",
       "  (pool_nonlin_2): Expression(expression=identity) \n",
       "  (drop_3): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_3): Conv2d(41, 70, kernel_size=(14, 1), stride=(1, 1), bias=False)\n",
       "  (bnorm_3): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_3): Expression(expression=elu) \n",
       "  (pool_3): AvgPool2dWithConv()\n",
       "  (pool_nonlin_3): Expression(expression=identity) \n",
       "  (drop_4): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_4): Conv2d(70, 118, kernel_size=(32, 1), stride=(1, 1), bias=False)\n",
       "  (bnorm_4): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_4): Expression(expression=elu) \n",
       "  (pool_4): AvgPool2dWithConv()\n",
       "  (pool_nonlin_4): Expression(expression=identity) \n",
       "  (final_layer): Sequential(\n",
       "    (conv_classifier): Conv2d(118, 2, kernel_size=(55, 1), stride=(1, 1))\n",
       "    (logsoftmax): LogSoftmax(dim=1)\n",
       "    (squeeze): Expression(expression=squeeze_final_output) \n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.06785323, -0.07633081, -0.06958825, ...,  0.01386732,\n",
       "          0.00623307,  0.00142142],\n",
       "        [-0.06785323, -0.07633081, -0.06958825, ...,  0.01386732,\n",
       "          0.00623307,  0.00142142],\n",
       "        [ 0.06539513,  0.06591074,  0.04241068, ..., -0.00847166,\n",
       "         -0.00454815, -0.00354735],\n",
       "        ...,\n",
       "        [-0.04003958, -0.03828413, -0.02247119, ...,  0.01964251,\n",
       "          0.01304058,  0.00952763],\n",
       "        [-0.07677436, -0.09000701, -0.09551565, ...,  0.02451516,\n",
       "          0.01189578,  0.00276193],\n",
       "        [-0.07714031, -0.08671535, -0.0830165 , ...,  0.03405689,\n",
       "          0.02825564,  0.01848716]]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.__getitem__(2)[0][None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 21, 6000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8547, -0.0593]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=torch.from_numpy(test_set.__getitem__(2)[0][None,:,:]).cuda()\n",
    "#test=torch.ones(size=(2,n_chans,input_time_length)).cuda()\n",
    "print(test.shape)\n",
    "model.forward(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=============================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ============================================================================================================================================\n",
       "  ShallowFBCSPNet (ShallowFBCSPNet)        [1, 21, 6000]             [1, 2]                    --                        --\n",
       "  ├─Ensure4d (ensuredims): 1-1             [1, 21, 6000]             [1, 21, 6000, 1]          --                        --\n",
       "  ├─Rearrange (dimshuffle): 1-2            [1, 21, 6000, 1]          [1, 1, 6000, 21]          --                        --\n",
       "  ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 6000, 21]          [1, 25, 5976, 1]          13,775                    --\n",
       "  ├─BatchNorm2d (bnorm): 1-4               [1, 25, 5976, 1]          [1, 25, 5976, 1]          50                        --\n",
       "  ├─Expression (conv_nonlin_exp): 1-5      [1, 25, 5976, 1]          [1, 25, 5976, 1]          --                        --\n",
       "  ├─AvgPool2d (pool): 1-6                  [1, 25, 5976, 1]          [1, 25, 394, 1]           --                        [75, 1]\n",
       "  ├─Expression (pool_nonlin_exp): 1-7      [1, 25, 394, 1]           [1, 25, 394, 1]           --                        --\n",
       "  ├─Dropout (drop): 1-8                    [1, 25, 394, 1]           [1, 25, 394, 1]           --                        --\n",
       "  ├─Sequential (final_layer): 1-9          [1, 25, 394, 1]           [1, 2]                    --                        --\n",
       "  │    └─Conv2d (conv_classifier): 2-1     [1, 25, 394, 1]           [1, 2, 1, 1]              19,702                    [394, 1]\n",
       "  │    └─LogSoftmax (logsoftmax): 2-2      [1, 2, 1, 1]              [1, 2, 1, 1]              --                        --\n",
       "  │    └─Expression (squeeze): 2-3         [1, 2, 1, 1]              [1, 2]                    --                        --\n",
       "  ============================================================================================================================================\n",
       "  Total params: 33,527\n",
       "  Trainable params: 33,527\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (M): 0.02\n",
       "  ============================================================================================================================================\n",
       "  Input size (MB): 0.50\n",
       "  Forward/backward pass size (MB): 1.20\n",
       "  Params size (MB): 0.08\n",
       "  Estimated Total Size (MB): 1.78\n",
       "  ============================================================================================================================================,\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "if test_on_eval==False:\n",
    "    path=f'{model_name}'\n",
    "    classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=criterion,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(valid_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",cp],#Try ‘roc_auc’\n",
    "        warm_start=True,\n",
    "        )\n",
    "elif test_on_eval:\n",
    "    path=f'{model_name}II'\n",
    "    classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=criterion,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train=DataLoader,\n",
    "        iterator_valid=DataLoader,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        #iterator_train__num_workers=1,\n",
    "        #iterator_valid__num_workers=1,\n",
    "        #iterator_train__persistent_workers=True,\n",
    "        #iterator_valid__persistent_workers=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",cp,],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1]\n"
     ]
    }
   ],
   "source": [
    "test=np.random.rand(3,n_chans,input_time_length)\n",
    "out=classifier.predict(test)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads Phase 1 parameters and fit them further in phase 2\n",
    "path=f'{model_name}'\n",
    "if test_on_eval:\n",
    "    classifier.load_params(\n",
    "        f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "    print(\"Paramters Loaded\")\n",
    "    path=f'{model_name}II'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to load parameters for ongoing training\n",
    "try:\n",
    "    classifier.load_params(\n",
    "        f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')\n",
    "    print(\"Paramters Loaded\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=============================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ============================================================================================================================================\n",
       "  Deep4Net (Deep4Net)                      [1, 21, 6000]             [1, 2]                    --                        --\n",
       "  ├─Ensure4d (ensuredims): 1-1             [1, 21, 6000]             [1, 21, 6000, 1]          --                        --\n",
       "  ├─Rearrange (dimshuffle): 1-2            [1, 21, 6000, 1]          [1, 1, 6000, 21]          --                        --\n",
       "  ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 6000, 21]          [1, 25, 5991, 1]          13,400                    --\n",
       "  ├─BatchNorm2d (bnorm): 1-4               [1, 25, 5991, 1]          [1, 25, 5991, 1]          50                        --\n",
       "  ├─Expression (conv_nonlin): 1-5          [1, 25, 5991, 1]          [1, 25, 5991, 1]          --                        --\n",
       "  ├─MaxPool2d (pool): 1-6                  [1, 25, 5991, 1]          [1, 25, 5989, 1]          --                        [3, 1]\n",
       "  ├─Expression (pool_nonlin): 1-7          [1, 25, 5989, 1]          [1, 25, 5989, 1]          --                        --\n",
       "  ├─Dropout (drop_2): 1-8                  [1, 25, 5989, 1]          [1, 25, 5989, 1]          --                        --\n",
       "  ├─Conv2d (conv_2): 1-9                   [1, 25, 5989, 1]          [1, 50, 1994, 1]          12,500                    [10, 1]\n",
       "  ├─BatchNorm2d (bnorm_2): 1-10            [1, 50, 1994, 1]          [1, 50, 1994, 1]          100                       --\n",
       "  ├─Expression (nonlin_2): 1-11            [1, 50, 1994, 1]          [1, 50, 1994, 1]          --                        --\n",
       "  ├─MaxPool2d (pool_2): 1-12               [1, 50, 1994, 1]          [1, 50, 1992, 1]          --                        [3, 1]\n",
       "  ├─Expression (pool_nonlin_2): 1-13       [1, 50, 1992, 1]          [1, 50, 1992, 1]          --                        --\n",
       "  ├─Dropout (drop_3): 1-14                 [1, 50, 1992, 1]          [1, 50, 1992, 1]          --                        --\n",
       "  ├─Conv2d (conv_3): 1-15                  [1, 50, 1992, 1]          [1, 100, 661, 1]          50,000                    [10, 1]\n",
       "  ├─BatchNorm2d (bnorm_3): 1-16            [1, 100, 661, 1]          [1, 100, 661, 1]          200                       --\n",
       "  ├─Expression (nonlin_3): 1-17            [1, 100, 661, 1]          [1, 100, 661, 1]          --                        --\n",
       "  ├─MaxPool2d (pool_3): 1-18               [1, 100, 661, 1]          [1, 100, 659, 1]          --                        [3, 1]\n",
       "  ├─Expression (pool_nonlin_3): 1-19       [1, 100, 659, 1]          [1, 100, 659, 1]          --                        --\n",
       "  ├─Dropout (drop_4): 1-20                 [1, 100, 659, 1]          [1, 100, 659, 1]          --                        --\n",
       "  ├─Conv2d (conv_4): 1-21                  [1, 100, 659, 1]          [1, 200, 217, 1]          200,000                   [10, 1]\n",
       "  ├─BatchNorm2d (bnorm_4): 1-22            [1, 200, 217, 1]          [1, 200, 217, 1]          400                       --\n",
       "  ├─Expression (nonlin_4): 1-23            [1, 200, 217, 1]          [1, 200, 217, 1]          --                        --\n",
       "  ├─MaxPool2d (pool_4): 1-24               [1, 200, 217, 1]          [1, 200, 215, 1]          --                        [3, 1]\n",
       "  ├─Expression (pool_nonlin_4): 1-25       [1, 200, 215, 1]          [1, 200, 215, 1]          --                        --\n",
       "  ├─Sequential (final_layer): 1-26         [1, 200, 215, 1]          [1, 2]                    --                        --\n",
       "  │    └─Conv2d (conv_classifier): 2-1     [1, 200, 215, 1]          [1, 2, 1, 1]              86,002                    [215, 1]\n",
       "  │    └─LogSoftmax (logsoftmax): 2-2      [1, 2, 1, 1]              [1, 2, 1, 1]              --                        --\n",
       "  │    └─Expression (squeeze): 2-3         [1, 2, 1, 1]              [1, 2]                    --                        --\n",
       "  ============================================================================================================================================\n",
       "  Total params: 362,652\n",
       "  Trainable params: 362,652\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (M): 101.46\n",
       "  ============================================================================================================================================\n",
       "  Input size (MB): 0.50\n",
       "  Forward/backward pass size (MB): 4.55\n",
       "  Params size (MB): 1.40\n",
       "  Estimated Total Size (MB): 6.45\n",
       "  ============================================================================================================================================,\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_set,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(\n",
    "    f_params=f'model/{path}_param.pkl', f_optimizer=f'model/{path}_opt.pkl', f_history=f'model/{path}_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y=test_dataset.load()\n",
    "ch_names=['A1', 'A2', 'C3', 'C4', 'CZ', 'F3', 'F4', 'F7', 'F8', 'FP1','FP2', 'FZ', 'O1', 'O2','P3', 'P4', 'PZ', 'T3', 'T4', 'T5', 'T6']\n",
    "test_set=create_from_X_y(test_x,test_y,sfreq=sampling_freq,drop_last_window=True,ch_names=ch_names,window_size_samples=input_time_length,\n",
    "                        window_stride_samples=sampling_freq)\n",
    "del test_x,test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This block loads the best parameters and finds the accuracy, f1 score and roc auc of the valid/test set\n",
    "classifier = braindecode.EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer__lr=optimizer_lr,\n",
    "    #optimizer__weight_decay=optimizer_weight_decay,\n",
    "    iterator_train__shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    device=device,\n",
    "    callbacks=[\"accuracy\",\"f1\",cp],#Try ‘roc_auc’\n",
    "    warm_start=True,\n",
    "        )\n",
    "classifier.initialize()\n",
    "classifier.load_params(\n",
    "        f_params=f'model/{model_name}best_param.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")\n",
    "if test_on_eval==False:\n",
    "    pred_labels=classifier.predict(valid_set)\n",
    "    actual_labels=[label[1] for label in valid_set]\n",
    "    auc=roc_auc_score(actual_labels,classifier.predict_proba(valid_set)[:,1])\n",
    "elif test_on_eval:\n",
    "    pred_labels=classifier.predict(test_set)\n",
    "    actual_labels=[label[1] for label in test_set]\n",
    "    auc=roc_auc_score(actual_labels,classifier.predict_proba(test_set)[:,1])\n",
    "actual_labels=np.array(actual_labels)\n",
    "accuracy=np.mean(pred_labels==actual_labels)\n",
    "tp=np.sum(pred_labels*actual_labels)\n",
    "precision=tp/np.sum(pred_labels)\n",
    "recall=tp/np.sum(actual_labels)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "\n",
    "print(model_name)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "print(f\"F1-Score:{f1}\")\n",
    "print(f\"roc_auc score:{auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramters Loaded\n"
     ]
    }
   ],
   "source": [
    "#This will load the model and parameters and then replace it with one whose classification layer is removed\n",
    "#This is used for feature extraction and the features will be used to train LSTM.\n",
    "from skorch import NeuralNet\n",
    "network=NeuralNet(module=model,criterion=torch.nn.modules.loss.NLLLoss,batch_size=batch_size,device=device)\n",
    "network.initialize()\n",
    "network.load_params(\n",
    "    f_params=f'model/{model_name}best_param.pkl', f_optimizer=f'model/{model_name}best_opt.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")\n",
    "network.module_=torch.nn.Sequential(*(list(network.module_.children())[:-1]),nn.modules.Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Ensure4d()\n",
       "  (1): Rearrange('batch C T 1 -> batch 1 T C')\n",
       "  (2): CombinedConv(\n",
       "    (conv_time): Conv2d(1, 25, kernel_size=(25, 1), stride=(1, 1))\n",
       "    (conv_spat): Conv2d(25, 25, kernel_size=(1, 21), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (3): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (4): Expression(expression=square) \n",
       "  (5): AvgPool2d(kernel_size=(75, 1), stride=(15, 1), padding=0)\n",
       "  (6): Expression(expression=safe_log) \n",
       "  (7): Dropout(p=0.5, inplace=False)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network.module_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9850\n"
     ]
    }
   ],
   "source": [
    "test=torch.ones(size=(2,21,6000))\n",
    "feat=network.predict(test).shape[1]\n",
    "print(feat)\n",
    "del test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads dataset, finds smallest trial, with this, we find number of windows using stride and convert it to array of windows of trials\n",
    "#shape is (no_of_trials,no_of_windows,channels,input_time_length) in the end\n",
    "X,y=dataset.load()\n",
    "#Separates normal and abnormal recordings\n",
    "abnormal_indexes=np.nonzero(y)[0][::-1]\n",
    "abnormal=[]\n",
    "for i in abnormal_indexes:\n",
    "    abnormal.append(X.pop(i))\n",
    "abnormal_labels=y[i:]\n",
    "y=y[:i]\n",
    "del abnormal_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_train_trials=train_set.__len__()\n",
    "train_features=np.zeros(shape=(no_of_train_trials,feat),dtype=np.float32)\n",
    "train_labels=np.zeros(shape=(no_of_train_trials),dtype=np.float32)\n",
    "for i in range(no_of_train_trials):\n",
    "    window,train_labels[i]=train_set.__getitem__(i)\n",
    "    train_features[i]=network.predict(window[None,:,:])\n",
    "mid=len(train_features)//2\n",
    "scipy.io.savemat(\"E:/train_features_1.mat\",{\"x\":train_features[:mid,:],\"y\":train_labels[:mid]})\n",
    "scipy.io.savemat(\"E:/train_features_2.mat\",{\"x\":train_features[mid:,:],\"y\":train_labels[mid:]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_test_trials=test_set.__len__()\n",
    "test_features=np.zeros(shape=(no_of_test_trials,feat),dtype=np.float32)\n",
    "test_labels=np.zeros(shape=(no_of_test_trials),dtype=np.float32)\n",
    "for i in range(no_of_test_trials):\n",
    "    window,test_labels[i]=test_set.__getitem__(i)\n",
    "    test_features[i]=network.predict(window[None,:,:])\n",
    "scipy.io.savemat(\"E:/test_features.mat\",{\"x\":test_features,\"y\":test_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This method is wrong, make t windows of a single sample and pass them as batch, that will be first input for LSTM\n",
    "#However, as long as t is kept small, there will only be as many incorrect windows as there were samples in the original dataset\n",
    "#Thus, the number of incorrect windows are miniscule in comparision to the final dataset.\n",
    "\n",
    "#Counting normal trials windows\n",
    "no_of_trials=0\n",
    "stride=sampling_freq*10\n",
    "for i in range(len(X)):\n",
    "    no_of_trials+=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "#Counting abnormal trials windows\n",
    "abstride=sampling_freq\n",
    "for i in range(len(abnormal)):\n",
    "    no_of_trials+=((abnormal[i].shape[1]-input_time_length)//abstride)-1\n",
    "features=np.zeros(shape=(no_of_trials,feat),dtype=np.float32)\n",
    "labels=[]\n",
    "\n",
    "#Normal features\n",
    "position=0\n",
    "for i in range(len(X)):\n",
    "    windows=[]\n",
    "    no_of_windows=((X[i].shape[1]-input_time_length)//stride)-1\n",
    "    for j in range(no_of_windows):\n",
    "        windows.append(X[i][:,j*stride:j*stride+input_time_length])\n",
    "        labels.append(y[i])\n",
    "    windows=np.array(windows)\n",
    "    features[position:position+no_of_windows]=network.predict(windows)\n",
    "    position+=no_of_windows\n",
    "del i,j,no_of_windows,X,y,windows\n",
    "#Abnormal features\n",
    "for i in range(len(abnormal)):\n",
    "    windows=[]\n",
    "    no_of_windows=((abnormal[i].shape[1]-input_time_length)//abstride)-1\n",
    "    for j in range(no_of_windows):\n",
    "        windows.append(abnormal[i][:,j*abstride:j*abstride+input_time_length])\n",
    "        labels.append(abnormal_labels[i])\n",
    "    windows=np.array(windows)\n",
    "    features[position:position+no_of_windows]=network.predict(windows)\n",
    "    position+=no_of_windows\n",
    "del i,j,no_of_windows,abnormal,abnormal_labels,windows\n",
    "labels=np.array(labels)\n",
    "\n",
    "#This saves the features along with labels of each trial in a .mat file\n",
    "#scipy.io.savemat(\"E:/train_features.mat\",{\"x\":features,\"y\":labels})\n",
    "mid=len(features)//2\n",
    "scipy.io.savemat(\"E:/train_features_1.mat\",{\"x\":features[:mid,:],\"y\":labels[:mid]})\n",
    "scipy.io.savemat(\"E:/train_features_2.mat\",{\"x\":features[mid:,:],\"y\":labels[mid:]})\n",
    "#del features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x,test_y=test_dataset.load()\n",
    "no_of_trials=0\n",
    "#Test set must match test set from paper as much as possible\n",
    "stride=sampling_freq\n",
    "for i in range(len(test_x)):\n",
    "    no_of_trials+=((test_x[i].shape[1]-input_time_length)//stride)-1\n",
    "test_features=np.zeros(shape=(no_of_trials,feat),dtype=np.float32)\n",
    "test_labels=[]\n",
    "position=0\n",
    "for i in range(len(test_x)):\n",
    "    windows=[]\n",
    "    no_of_windows=((test_x[i].shape[1]-input_time_length)//stride)-1\n",
    "    for j in range(no_of_windows):\n",
    "        windows.append(test_x[i][:,j*stride:j*stride+input_time_length])\n",
    "        test_labels.append(test_y[i])\n",
    "    windows=np.array(windows)\n",
    "    test_features[position:position+no_of_windows]=network.predict(windows)\n",
    "    position+=no_of_windows\n",
    "del i,j,no_of_windows,test_x,test_y,windows\n",
    "test_labels=np.array(test_labels)\n",
    "\n",
    "mid=len(test_features)//2\n",
    "scipy.io.savemat(\"E:/test_features.mat\",{\"x\":test_features,\"y\":test_labels})\n",
    "#scipy.io.savemat(\"E:/test_features_1.mat\",{\"x\":test_features[:mid,:],\"y\":test_labels[:mid]})\n",
    "#scipy.io.savemat(\"E:/test_features_2.mat\",{\"x\":test_features[mid:,:],\"y\":test_labels[mid:]})\n",
    "#del test_features,test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import numpy as np\n",
    "inputs=scipy.io.loadmat(\"E:/train_features_1.mat\")\n",
    "train_features=inputs[\"x\"]\n",
    "train_labels=inputs[\"y\"].squeeze()\n",
    "\n",
    "inputs=scipy.io.loadmat(\"E:/train_features_2.mat\")\n",
    "train_features=np.concatenate((train_features,inputs[\"x\"]),axis=0)\n",
    "train_labels=np.concatenate((train_labels,inputs[\"y\"].squeeze()),axis=0)\n",
    "inputs=scipy.io.loadmat(\"E:/test_features.mat\")\n",
    "test_features=inputs[\"x\"]\n",
    "test_labels=inputs[\"y\"].squeeze()\n",
    "del inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t variable determines timesteps for hybrid model\n",
    "t=7\n",
    "f=train_features.shape[-1]\n",
    "seq_features=train_features[:(len(train_labels)//t)*t].reshape((len(train_labels)//t,t,f))\n",
    "seq_labels=train_labels[:(len(train_labels)//t)*t].reshape((len(train_labels)//t,t))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_test_features=test_features[:(len(test_labels)//t)*t].reshape((len(test_labels)//t,t,f))\n",
    "seq_test_labels=test_labels[:(len(test_labels)//t)*t].reshape((test_labels.shape[0]//t,t))[:,0]\n",
    "test_set=Dataset(seq_test_features,seq_test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(torch.nn.Module):\n",
    "  def __init__(self,input_features):\n",
    "    super().__init__()\n",
    "    self.lstm = torch.nn.LSTM(input_size=input_features, hidden_size=50, batch_first=True)\n",
    "    self.fc = torch.nn.Linear(50, 2)\n",
    "    self.tanh = torch.nn.Tanh()\n",
    "    self.softmax = torch.nn.LogSoftmax(dim=1)\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    h1_T, _ = self.lstm(inputs)\n",
    "    h2=self.tanh(h1_T[:,-1].squeeze())\n",
    "    h3 = self.fc(h2)       # inplace of h2[-1,:,:] we can use h2_T. Both are identical\n",
    "    output = self.softmax(h3)\n",
    "    return output\n",
    "model = SimpleModel(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='shallow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=SimpleModel(\n",
       "    (lstm): LSTM(9850, 50, batch_first=True)\n",
       "    (fc): Linear(in_features=50, out_features=2, bias=True)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'LSTM{model_name}best_param.pkl',f_optimizer=f'LSTM{model_name}best_opt.pkl',\n",
    "              f_history=f'LSTM{model_name}best_history.json')\n",
    "classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=torch.nn.NLLLoss,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=0.0001,\n",
    "        iterator_train__shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\",\"f1\",'roc_auc',cp],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "test=torch.randn(size=(2,t,f))\n",
    "shape=classifier.predict(test).shape\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_f1    train_loss    train_roc_auc    valid_acc    valid_accuracy    valid_f1    valid_loss    valid_roc_auc    cp      dur\n",
      "-------  ----------------  ----------  ------------  ---------------  -----------  ----------------  ----------  ------------  ---------------  ----  -------\n",
      "      1            \u001b[36m0.7590\u001b[0m      \u001b[32m0.7408\u001b[0m        \u001b[35m0.5777\u001b[0m           \u001b[31m0.8530\u001b[0m       \u001b[94m0.7189\u001b[0m            \u001b[36m0.7189\u001b[0m      \u001b[32m0.6811\u001b[0m        \u001b[35m0.5414\u001b[0m           \u001b[31m0.8177\u001b[0m     +  34.1340\n",
      "      2            \u001b[36m0.8208\u001b[0m      \u001b[32m0.8259\u001b[0m        \u001b[35m0.4767\u001b[0m           \u001b[31m0.9008\u001b[0m       \u001b[94m0.7566\u001b[0m            \u001b[36m0.7566\u001b[0m      \u001b[32m0.7447\u001b[0m        \u001b[35m0.5098\u001b[0m           \u001b[31m0.8360\u001b[0m     +  29.8180\n",
      "      3            \u001b[36m0.8496\u001b[0m      \u001b[32m0.8456\u001b[0m        \u001b[35m0.4036\u001b[0m           \u001b[31m0.9263\u001b[0m       0.7322            0.7322      0.6920        0.5336           0.8273        27.7570\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=SimpleModel(\n",
       "    (lstm): LSTM(9850, 50, batch_first=True)\n",
       "    (fc): Linear(in_features=50, out_features=2, bias=True)\n",
       "    (tanh): Tanh()\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Try deep smac by itself and as feature extractor and determine effectiveness\n",
    "classifier.fit(seq_features,y=seq_labels,epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_params(\n",
    "    f_params=f'model/LSTM{model_name}best_param.pkl', f_optimizer=f'model/LSTM{model_name}best_opt.pkl',\n",
    "    f_history=f'model/LSTM{model_name}best_history.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_params(\n",
    "    f_params=f'model/LSTM{model_name}best_param.pkl', f_optimizer=f'model/LSTM{model_name}best_opt.pkl', \n",
    "    f_history=f'model/LSTM{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=classifier.predict(seq_test_features)\n",
    "accuracy=np.mean(out==seq_test_labels)\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "tp=np.sum(out*seq_test_labels)\n",
    "precision=tp/np.sum(out)\n",
    "recall=tp/np.sum(seq_test_labels)\n",
    "f1=2*precision*recall/(precision+recall)\n",
    "print(f\"F1-Score:{f1}\") \n",
    "roc_auc_score(seq_test_labels,classifier.predict_proba(seq_test_features)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length=min([trial.shape[1] for trial in X])\n",
    "trials=len(X)\n",
    "X_new=np.zeros(shape=(trials,n_chans,min_length),dtype=np.float32)\n",
    "for i in range(trials):\n",
    "    X_new[i]=X[i][:,:min_length]\n",
    "trials=len(test_x)\n",
    "test_x_new=np.zeros(shape=(trials,n_chans,min_length),dtype=np.float32)\n",
    "for i in range(trials):\n",
    "    test_x_new[i]=test_x[i][:,:min_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_length=min([trial.shape[1] for trial in X])\n",
    "trials=len(X)\n",
    "X_new=np.zeros(shape=(trials,n_chans,min_length),dtype=np.float32)\n",
    "for i in range(trials):\n",
    "    X_new[i]=X[i][:,:min_length]\n",
    "trials=len(test_x)\n",
    "test_x_new=np.zeros(shape=(trials,n_chans,min_length),dtype=np.float32)\n",
    "for i in range(trials):\n",
    "    test_x_new[i]=test_x[i][:,:min_length]\n",
    "mid=len(X_new)//2\n",
    "scipy.io.savemat(\"E:/train_set_1.mat\",{\"x\":X_new[:mid,:,:],\"y\":y[:mid]})\n",
    "scipy.io.savemat(\"E:/train_set_2.mat\",{\"x\":X_new[mid:,:,:],\"y\":y[mid:]})\n",
    "scipy.io.savemat(\"E:/test_set.mat\",{\"x\":test_x_new,\"y\":test_y})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
