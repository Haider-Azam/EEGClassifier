{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow not install, you could not use those pipelines\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import skorch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import elu,relu,leaky_relu\n",
    "from torchvision.transforms import Resize\n",
    "import braindecode \n",
    "from braindecode.models import ShallowFBCSPNet,Deep4Net,TCN,ATCNet,HybridNet,EEGConformer\n",
    "from braindecode.models.modules import Expression\n",
    "from braindecode.models.functions import squeeze_final_output,safe_log\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.helper import predefined_split\n",
    "from config_exp3 import *\n",
    "from dataset import *\n",
    "from sklearn.metrics import roc_auc_score,f1_score\n",
    "from mne import set_log_level\n",
    "from skorch.callbacks import LRScheduler\n",
    "import os\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "ab_label_dict = ['Normal','Sharp Wave','Delta Slow Wave', 'Spike and Wave Discharge', 'Beta Wave', 'Theta Wave', 'Triphasic Wave', 'Low Voltage','Burst Suppression','Unknown']\n",
    "#n_chans=2\n",
    "#input_time_length=input_time_length//n_chans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders=['Abnormality_Crops','Normal_Crops']\n",
    "file_names=[]\n",
    "labels=[]\n",
    "ab_labels=[]\n",
    "for fold in folders:\n",
    "    folder_path=os.path.join(processed_folder,fold)\n",
    "    files=os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        file_path=os.path.join(folder_path,file)\n",
    "        file_names.append(file_path)\n",
    "        data=np.load(file_path)\n",
    "        labels.append(int(data['label']))\n",
    "        ab_labels.append(int(data['ab_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "windows=np.array(list(zip(file_names,labels,ab_labels)))\n",
    "train_data,test_data=train_test_split(windows,test_size=0.2)\n",
    "train_dataframe=pd.DataFrame({'file_path':train_data[:,0],'label':train_data[:,1],'ab_label':train_data[:,2]})\n",
    "train_dataframe.to_excel(f\"{processed_folder}/train.xlsx\",index=False)\n",
    "test_dataframe=pd.DataFrame({'file_path':test_data[:,0],'label':test_data[:,1],'ab_label':test_data[:,2]})\n",
    "test_dataframe.to_excel(f\"{processed_folder}/eval.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe=pd.read_excel(f'{processed_folder}/train.xlsx')\n",
    "eval_dataframe=pd.read_excel(f'{processed_folder}/eval.xlsx')\n",
    "\n",
    "train_data=[]\n",
    "train_label=[]\n",
    "for i in range(len(train_dataframe)):\n",
    "    file=np.load(train_dataframe['file_path'][i])\n",
    "    train_data.append(file['data'])\n",
    "    train_label.append(train_dataframe['ab_label'][i])\n",
    "train_data=np.array(train_data)\n",
    "train_label=np.array(train_label)\n",
    "\n",
    "\n",
    "eval_data=[]\n",
    "eval_label=[]\n",
    "for i in range(len(eval_dataframe)):\n",
    "    file=np.load(eval_dataframe['file_path'][i])\n",
    "    eval_data.append(file['data'])\n",
    "    eval_label.append(eval_dataframe['ab_label'][i])\n",
    "eval_data=np.array(eval_data)\n",
    "eval_label=np.array(eval_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.reshape(len(train_data),n_chans,input_time_length,order='F')\n",
    "eval_data=eval_data.reshape(len(eval_data),n_chans,input_time_length,order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=skorch.dataset.Dataset(train_data,train_label)\n",
    "test_set=skorch.dataset.Dataset(eval_data,eval_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make it reshape it as well\n",
    "#train_x=train_x.reshape(len(train_x),n_chans,input_time_length,order='F')\n",
    "#This is not giving results, just load all the windows as a single numpy array and train on that.\n",
    "class Exp3Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, excel_path):\n",
    "        super().__init__()\n",
    "        excel_file=pd.read_excel(excel_path)\n",
    "        self.file_names=excel_file['file_path'].to_numpy(dtype=str)\n",
    "        self.label=excel_file['label'].to_numpy()\n",
    "        self.ab_label=excel_file['ab_label'].to_numpy()\n",
    "    def __getitem__(self, index):\n",
    "        file=np.load(self.file_names[index])\n",
    "        window=file['data']\n",
    "        window=window.reshape(n_chans,input_time_length,order='F')\n",
    "        #window=np.expand_dims(window,axis=-1)\n",
    "        #print(window.shape)\n",
    "        ab_label=np.array(self.ab_label[index])\n",
    "        #ab_label=np.eye(len(ab_label_dict), dtype='uint8')[ab_label]\n",
    "        return window,ab_label\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=Exp3Dataset(f'{processed_folder}/eval.xlsx')\n",
    "train_set=Exp3Dataset(f'{processed_folder}/train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 600)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5, dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"transformer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 10])\n",
      "transformer\n"
     ]
    }
   ],
   "source": [
    "criterion=torch.nn.NLLLoss\n",
    "n_classes = len(ab_label_dict)\n",
    "if model_name==\"shallow\":\n",
    "    optimizer_lr = 0.001\n",
    "    optimizer_weight_decay = 0\n",
    "    pool_time_length = 16\n",
    "    pool_time_stride = 2\n",
    "    filter_time_length = 16\n",
    "    n_filters_spat = 64\n",
    "    n_filters_time = 32\n",
    "    split_first_layer = True\n",
    "    drop_prob = 0.328794\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = ShallowFBCSPNet(n_chans,\n",
    "                                    n_classes,\n",
    "                                    #n_filters_time=n_start_chans,\n",
    "                                    #n_filters_spat=n_start_chans,\n",
    "                                    n_times=input_time_length,\n",
    "                                    pool_time_length=pool_time_length,\n",
    "                                    pool_time_stride=pool_time_stride,\n",
    "                                    drop_prob=drop_prob,\n",
    "                                    n_filters_time=n_filters_time,\n",
    "                                    n_filters_spat=n_filters_spat,\n",
    "                                    filter_time_length=filter_time_length,\n",
    "                                    split_first_layer=split_first_layer,\n",
    "                                    final_conv_length='auto',)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"deep\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                         n_filters_time=n_start_chans,\n",
    "                         n_filters_spat=n_start_chans,\n",
    "                         n_times=input_time_length,\n",
    "                         n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                         n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                         n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                         final_conv_length='auto',\n",
    "                        stride_before_pool=True)\n",
    "    test=torch.ones(size=(6,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"deep_smac\" or model_name == 'deep_smac_bnorm':\n",
    "    optimizer_lr = 0.0000625\n",
    "    if model_name == 'deep_smac':\n",
    "            do_batch_norm = False\n",
    "    else:\n",
    "        do_batch_norm = True\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 12\n",
    "    filter_length_3 = 16\n",
    "    filter_length_4 = 20\n",
    "    filter_time_length = 8\n",
    "    #final_conv_length = 1\n",
    "    first_nonlin = elu\n",
    "    first_pool_mode = 'mean'\n",
    "    later_nonlin = elu\n",
    "    later_pool_mode = 'mean'\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    pool_time_length = 1\n",
    "    pool_time_stride = 1\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    n_start_chans = n_filters_start\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "            n_filters_time=n_start_chans,\n",
    "            n_filters_spat=n_start_chans,\n",
    "            n_times=input_time_length,\n",
    "            n_filters_2=int(n_start_chans * n_chan_factor),\n",
    "            n_filters_3=int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "            n_filters_4=int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "            final_conv_length='auto',\n",
    "            stride_before_pool=True,\n",
    "            drop_prob=drop_prob,\n",
    "            filter_length_2=filter_length_2,\n",
    "            filter_length_3=filter_length_3,\n",
    "            filter_length_4=filter_length_4,\n",
    "            filter_time_length=filter_time_length,\n",
    "            first_conv_nonlin=first_nonlin,\n",
    "            first_pool_mode=first_pool_mode,\n",
    "            later_conv_nonlin=later_nonlin,\n",
    "            later_pool_mode=later_pool_mode,\n",
    "            pool_time_length=pool_time_length,\n",
    "            pool_time_stride=pool_time_stride,\n",
    "            split_first_layer=split_first_layer\n",
    "            )\n",
    "    test=torch.ones(size=(6,n_chans,input_time_length))\n",
    "    #out=model.forward(test)\n",
    "    #print(out.shape)\n",
    "\n",
    "elif model_name==\"TCN\":\n",
    "    import warnings\n",
    "    #This disables the warning of the dropout2d layers receiving 3d input\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    n_blocks=4\n",
    "    n_filters=32\n",
    "    kernel_size=10\n",
    "    drop_prob = 0.3\n",
    "    min_len = 1\n",
    "    for i in range(n_blocks):\n",
    "        dilation = 2 ** i\n",
    "        min_len += 2 * (kernel_size - 1) * dilation\n",
    "    print(f\"Minimum length :{min_len}\")\n",
    "    x=TCN(n_chans,n_classes,n_blocks,n_filters,kernel_size,drop_prob)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=x.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    #There is no hyperparameter where output of TCN is (Batch_Size,Classes) when input is (Batch_Size,21,6000) so add new layers to meet size\n",
    "    model=nn.Sequential(x,nn.Conv1d(n_classes,n_classes,out_length,bias=True,),nn.LogSoftmax(dim=1),nn.Flatten())\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length,x\n",
    "    \n",
    "elif model_name==\"shallow_deep\":\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 8\n",
    "    filter_length_3 = 12\n",
    "    filter_length_4 = 16\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    #n_start_chans = n_filters_start\n",
    "\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    filter_time_length=4\n",
    "    first_conv_nonlin=relu\n",
    "    first_pool_nonlin=safe_log\n",
    "    later_conv_nonlin=elu\n",
    "    later_pool_nonlin=safe_log\n",
    "    first_pool_mode = \"mean\"\n",
    "    later_pool_mode = \"mean\"\n",
    "    pool_time_length=2\n",
    "    pool_time_stride=2\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                            n_filters_time=n_start_chans,\n",
    "                            n_filters_spat=n_start_chans,\n",
    "                            n_times=input_time_length,\n",
    "                            n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                            n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                            n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                            final_conv_length='auto',\n",
    "                            first_pool_nonlin=first_pool_nonlin,\n",
    "                            first_conv_nonlin=first_conv_nonlin,\n",
    "                            #later_pool_nonlin=later_pool_nonlin,\n",
    "                            #later_conv_nonlin=later_conv_nonlin,\n",
    "                            filter_time_length=filter_time_length,\n",
    "                            pool_time_length=pool_time_length,\n",
    "                            pool_time_stride=pool_time_stride,\n",
    "                            first_pool_mode=first_pool_mode,\n",
    "                            later_pool_mode=later_pool_mode,\n",
    "                            split_first_layer=split_first_layer,\n",
    "                            drop_prob=drop_prob,\n",
    "                            filter_length_2=filter_length_2,\n",
    "                            filter_length_3=filter_length_3,\n",
    "                            filter_length_4=filter_length_4,\n",
    "                            )\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "#    out=model.forward(test)\n",
    "#    print(out.shape)\n",
    "\n",
    "elif model_name==\"attention\":\n",
    "    #For attention model, we can afford to have only a single channel compared to in \n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    model=ATCNet(n_chans,n_classes,input_time_length//sampling_freq,sampling_freq,concat=True)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"transformer\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #criterion=torch.nn.CrossEntropyLoss\n",
    "    n_filters_time=32\n",
    "    att_depth=1\n",
    "    filter_time_length=16\n",
    "    att_heads=8\n",
    "    add_log_softmax=False\n",
    "    criterion=torch.nn.CrossEntropyLoss\n",
    "    model=EEGConformer(n_outputs=n_classes,n_chans=n_chans,n_times=input_time_length,input_window_seconds=input_time_length//sampling_freq,\n",
    "                    sfreq=sampling_freq,final_fc_length=\"auto\",n_filters_time=n_filters_time,att_depth=att_depth,\n",
    "                    filter_time_length=filter_time_length,att_heads=att_heads,add_log_softmax=add_log_softmax)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "del test\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EEGConformer(\n",
       "  (patch_embedding): _PatchEmbedding(\n",
       "    (shallownet): Sequential(\n",
       "      (0): Conv2d(1, 32, kernel_size=(1, 16), stride=(1, 1))\n",
       "      (1): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): ELU(alpha=1.0)\n",
       "      (4): AvgPool2d(kernel_size=(1, 75), stride=(1, 15), padding=0)\n",
       "      (5): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (projection): Sequential(\n",
       "      (0): Conv2d(32, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (1): Rearrange('b d_model 1 seq -> b seq d_model')\n",
       "    )\n",
       "  )\n",
       "  (transformer): _TransformerEncoder(\n",
       "    (0): _TransformerEncoderBlock(\n",
       "      (0): _ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): _MultiHeadAttention(\n",
       "            (keys): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (queries): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (values): Linear(in_features=32, out_features=32, bias=True)\n",
       "            (att_drop): Dropout(p=0.5, inplace=False)\n",
       "            (projection): Linear(in_features=32, out_features=32, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): _ResidualAdd(\n",
       "        (fn): Sequential(\n",
       "          (0): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
       "          (1): _FeedForwardBlock(\n",
       "            (0): Linear(in_features=32, out_features=128, bias=True)\n",
       "            (1): GELU(approximate='none')\n",
       "            (2): Dropout(p=0.5, inplace=False)\n",
       "            (3): Linear(in_features=128, out_features=32, bias=True)\n",
       "          )\n",
       "          (2): Dropout(p=0.5, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc): _FullyConnected(\n",
       "    (fc): Sequential(\n",
       "      (0): Linear(in_features=1120, out_features=256, bias=True)\n",
       "      (1): ELU(alpha=1.0)\n",
       "      (2): Dropout(p=0.5, inplace=False)\n",
       "      (3): Linear(in_features=256, out_features=32, bias=True)\n",
       "      (4): ELU(alpha=1.0)\n",
       "      (5): Dropout(p=0.3, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_layer): _FinalLayer(\n",
       "    (final_layer): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=10, bias=True)\n",
       "      (classification): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.NLLLoss()\n",
    "input=[]\n",
    "input.append(train_set.__getitem__(0)[0])\n",
    "input=np.array(input)\n",
    "input=torch.from_numpy(input).cuda()\n",
    "target=[]\n",
    "target.append(train_set.__getitem__(0)[1])\n",
    "target=np.array(target)\n",
    "target=torch.from_numpy(target).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=loss(model(input),target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_============================================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)                       Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ===========================================================================================================================================================\n",
       "  EEGConformer (EEGConformer)                             [1, 1, 600]               [1, 10]                   --                        --\n",
       "  ├─_PatchEmbedding (patch_embedding): 1-1                [1, 1, 1, 600]            [1, 35, 32]               --                        --\n",
       "  │    └─Sequential (shallownet): 2-1                     [1, 1, 1, 600]            [1, 32, 1, 35]            --                        --\n",
       "  │    │    └─Conv2d (0): 3-1                             [1, 1, 1, 600]            [1, 32, 1, 585]           544                       [1, 16]\n",
       "  │    │    └─Conv2d (1): 3-2                             [1, 32, 1, 585]           [1, 32, 1, 585]           1,056                     [1, 1]\n",
       "  │    │    └─BatchNorm2d (2): 3-3                        [1, 32, 1, 585]           [1, 32, 1, 585]           64                        --\n",
       "  │    │    └─ELU (3): 3-4                                [1, 32, 1, 585]           [1, 32, 1, 585]           --                        --\n",
       "  │    │    └─AvgPool2d (4): 3-5                          [1, 32, 1, 585]           [1, 32, 1, 35]            --                        [1, 75]\n",
       "  │    │    └─Dropout (5): 3-6                            [1, 32, 1, 35]            [1, 32, 1, 35]            --                        --\n",
       "  │    └─Sequential (projection): 2-2                     [1, 32, 1, 35]            [1, 35, 32]               --                        --\n",
       "  │    │    └─Conv2d (0): 3-7                             [1, 32, 1, 35]            [1, 32, 1, 35]            1,056                     [1, 1]\n",
       "  │    │    └─Rearrange (1): 3-8                          [1, 32, 1, 35]            [1, 35, 32]               --                        --\n",
       "  ├─_TransformerEncoder (transformer): 1-2                [1, 35, 32]               [1, 35, 32]               --                        --\n",
       "  │    └─_TransformerEncoderBlock (0): 2-3                [1, 35, 32]               [1, 35, 32]               --                        --\n",
       "  │    │    └─_ResidualAdd (0): 3-9                       [1, 35, 32]               [1, 35, 32]               4,288                     --\n",
       "  │    │    └─_ResidualAdd (1): 3-10                      [1, 35, 32]               [1, 35, 32]               8,416                     --\n",
       "  ├─_FullyConnected (fc): 1-3                             [1, 35, 32]               [1, 32]                   --                        --\n",
       "  │    └─Sequential (fc): 2-4                             [1, 1120]                 [1, 32]                   --                        --\n",
       "  │    │    └─Linear (0): 3-11                            [1, 1120]                 [1, 256]                  286,976                   --\n",
       "  │    │    └─ELU (1): 3-12                               [1, 256]                  [1, 256]                  --                        --\n",
       "  │    │    └─Dropout (2): 3-13                           [1, 256]                  [1, 256]                  --                        --\n",
       "  │    │    └─Linear (3): 3-14                            [1, 256]                  [1, 32]                   8,224                     --\n",
       "  │    │    └─ELU (4): 3-15                               [1, 32]                   [1, 32]                   --                        --\n",
       "  │    │    └─Dropout (5): 3-16                           [1, 32]                   [1, 32]                   --                        --\n",
       "  ├─_FinalLayer (final_layer): 1-4                        [1, 32]                   [1, 10]                   --                        --\n",
       "  │    └─Sequential (final_layer): 2-5                    [1, 32]                   [1, 10]                   --                        --\n",
       "  │    │    └─Linear (0): 3-17                            [1, 32]                   [1, 10]                   330                       --\n",
       "  │    │    └─Identity (classification): 3-18             [1, 10]                   [1, 10]                   --                        --\n",
       "  ===========================================================================================================================================================\n",
       "  Total params: 310,954\n",
       "  Trainable params: 310,954\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (M): 1.28\n",
       "  ===========================================================================================================================================================\n",
       "  Input size (MB): 0.00\n",
       "  Forward/backward pass size (MB): 0.56\n",
       "  Params size (MB): 1.24\n",
       "  Estimated Total Size (MB): 1.81\n",
       "  ===========================================================================================================================================================,\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "path=f'{model_name}II'\n",
    "classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=criterion,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train=DataLoader,\n",
    "        iterator_valid=DataLoader,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        #iterator_train__num_workers=1,\n",
    "        #iterator_valid__num_workers=1,\n",
    "        #iterator_train__persistent_workers=True,\n",
    "        #iterator_valid__persistent_workers=True,\n",
    "        batch_size=64,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\"],#,\"f1\",cp,],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8 8 8]\n"
     ]
    }
   ],
   "source": [
    "test=np.random.rand(3,n_chans,input_time_length)\n",
    "out=classifier.predict(test)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5, dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=============================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ============================================================================================================================================\n",
       "  ShallowFBCSPNet (ShallowFBCSPNet)        [1, 10, 60]               [1, 10]                   --                        --\n",
       "  ├─Ensure4d (ensuredims): 1-1             [1, 10, 60]               [1, 10, 60, 1]            --                        --\n",
       "  ├─Rearrange (dimshuffle): 1-2            [1, 10, 60, 1]            [1, 1, 60, 10]            --                        --\n",
       "  ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 60, 10]            [1, 25, 36, 1]            6,900                     --\n",
       "  ├─BatchNorm2d (bnorm): 1-4               [1, 25, 36, 1]            [1, 25, 36, 1]            50                        --\n",
       "  ├─Expression (conv_nonlin_exp): 1-5      [1, 25, 36, 1]            [1, 25, 36, 1]            --                        --\n",
       "  ├─AvgPool2d (pool): 1-6                  [1, 25, 36, 1]            [1, 25, 3, 1]             --                        [30, 1]\n",
       "  ├─Expression (pool_nonlin_exp): 1-7      [1, 25, 3, 1]             [1, 25, 3, 1]             --                        --\n",
       "  ├─Dropout (drop): 1-8                    [1, 25, 3, 1]             [1, 25, 3, 1]             --                        --\n",
       "  ├─Sequential (final_layer): 1-9          [1, 25, 3, 1]             [1, 10]                   --                        --\n",
       "  │    └─Conv2d (conv_classifier): 2-1     [1, 25, 3, 1]             [1, 10, 1, 1]             760                       [3, 1]\n",
       "  │    └─LogSoftmax (logsoftmax): 2-2      [1, 10, 1, 1]             [1, 10, 1, 1]             --                        --\n",
       "  │    └─Expression (squeeze): 2-3         [1, 10, 1, 1]             [1, 10]                   --                        --\n",
       "  ============================================================================================================================================\n",
       "  Total params: 7,710\n",
       "  Trainable params: 7,710\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (M): 0.00\n",
       "  ============================================================================================================================================\n",
       "  Input size (MB): 0.00\n",
       "  Forward/backward pass size (MB): 0.01\n",
       "  Params size (MB): 0.00\n",
       "  Estimated Total Size (MB): 0.01\n",
       "  ============================================================================================================================================,\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_set,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paramters Loaded\n"
     ]
    }
   ],
   "source": [
    "classifier.load_params(\n",
    "        f_params=f'model/{model_name}best_param.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")\n",
    "pred_labels=classifier.predict(test_set)\n",
    "actual_labels=[label[1] for label in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer\n",
      "Accuracy:0.5886545886545886\n",
      "F1-Score:0.5584426724825471\n"
     ]
    }
   ],
   "source": [
    "#auc=roc_auc_score(actual_labels,classifier.predict_proba(test_set)[:,1],multi_class='ovr')\n",
    "actual_labels=np.array(actual_labels)\n",
    "accuracy=np.mean(pred_labels==actual_labels)\n",
    "f1=f1_score(actual_labels,pred_labels, average='weighted')\n",
    "print(model_name)\n",
    "#Accuracy seems incorrect compared to the one reported by skorch\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "print(f\"F1-Score:{f1}\")\n",
    "#print(f\"roc_auc score:{auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
