{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import skorch\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import elu,relu,leaky_relu\n",
    "from torchvision.transforms import Resize\n",
    "import braindecode \n",
    "from braindecode.models import ShallowFBCSPNet,Deep4Net,TCN,ATCNet,HybridNet,EEGConformer\n",
    "from braindecode.models.modules import Expression\n",
    "from braindecode.models.functions import squeeze_final_output,safe_log\n",
    "from skorch.callbacks import Checkpoint\n",
    "from skorch.helper import predefined_split\n",
    "from config_exp3 import *\n",
    "from dataset import *\n",
    "from sklearn.metrics import roc_auc_score,f1_score\n",
    "from mne import set_log_level\n",
    "from skorch.callbacks import LRScheduler\n",
    "import os\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "ab_label_dict = ['Normal','Sharp Wave','Delta Slow Wave', 'Spike and Wave Discharge', 'Beta Wave', 'Theta Wave', 'Triphasic Wave', 'Low Voltage','Burst Suppression','Unknown']\n",
    "n_chans=2\n",
    "input_time_length=input_time_length//n_chans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders=['Abnormality_Crops','Normal_Crops']\n",
    "file_names=[]\n",
    "labels=[]\n",
    "ab_labels=[]\n",
    "for fold in folders:\n",
    "    folder_path=os.path.join(processed_folder,fold)\n",
    "    files=os.listdir(folder_path)\n",
    "    for file in files:\n",
    "        file_path=os.path.join(folder_path,file)\n",
    "        file_names.append(file_path)\n",
    "        data=np.load(file_path)\n",
    "        labels.append(int(data['label']))\n",
    "        ab_labels.append(int(data['ab_label']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "windows=np.array(list(zip(file_names,labels,ab_labels)))\n",
    "train_data,test_data=train_test_split(windows,test_size=0.2)\n",
    "train_dataframe=pd.DataFrame({'file_path':train_data[:,0],'label':train_data[:,1],'ab_label':train_data[:,2]})\n",
    "train_dataframe.to_excel(f\"{processed_folder}/train.xlsx\",index=False)\n",
    "test_dataframe=pd.DataFrame({'file_path':test_data[:,0],'label':test_data[:,1],'ab_label':test_data[:,2]})\n",
    "test_dataframe.to_excel(f\"{processed_folder}/eval.xlsx\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataframe=pd.read_excel(f'{processed_folder}/train.xlsx')\n",
    "eval_dataframe=pd.read_excel(f'{processed_folder}/eval.xlsx')\n",
    "\n",
    "train_data=[]\n",
    "train_label=[]\n",
    "for i in range(len(train_dataframe)):\n",
    "    file=np.load(train_dataframe['file_path'][i])\n",
    "    train_data.append(file['data'])\n",
    "    train_label.append(train_dataframe['ab_label'][i])\n",
    "train_data=np.array(train_data)\n",
    "train_label=np.array(train_label)\n",
    "\n",
    "\n",
    "eval_data=[]\n",
    "eval_label=[]\n",
    "for i in range(len(eval_dataframe)):\n",
    "    file=np.load(eval_dataframe['file_path'][i])\n",
    "    eval_data.append(file['data'])\n",
    "    eval_label.append(eval_dataframe['ab_label'][i])\n",
    "eval_data=np.array(eval_data)\n",
    "eval_label=np.array(eval_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.reshape(len(train_data),n_chans,input_time_length,order='F')\n",
    "eval_data=eval_data.reshape(len(eval_data),n_chans,input_time_length,order='F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set=skorch.dataset.Dataset(train_data,train_label)\n",
    "test_set=skorch.dataset.Dataset(eval_data,eval_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make it reshape it as well\n",
    "#train_x=train_x.reshape(len(train_x),n_chans,input_time_length,order='F')\n",
    "#This is not giving results, just load all the windows as a single numpy array and train on that.\n",
    "class Exp3Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, excel_path):\n",
    "        super().__init__()\n",
    "        excel_file=pd.read_excel(excel_path)\n",
    "        self.file_names=excel_file['file_path'].to_numpy(dtype=str)\n",
    "        self.label=excel_file['label'].to_numpy()\n",
    "        self.ab_label=excel_file['ab_label'].to_numpy()\n",
    "    def __getitem__(self, index):\n",
    "        file=np.load(self.file_names[index])\n",
    "        window=file['data']\n",
    "        window=window.reshape(n_chans,input_time_length,order='F')\n",
    "        #window=np.expand_dims(window,axis=-1)\n",
    "        #print(window.shape)\n",
    "        ab_label=np.array(self.ab_label[index])\n",
    "        #ab_label=np.eye(len(ab_label_dict), dtype='uint8')[ab_label]\n",
    "        return window,ab_label\n",
    " \n",
    "    def __len__(self):\n",
    "        return len(self.file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=Exp3Dataset(f'{processed_folder}/eval.xlsx')\n",
    "train_set=Exp3Dataset(f'{processed_folder}/train.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5, dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"shallow_deep\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shallow_deep\n"
     ]
    }
   ],
   "source": [
    "criterion=torch.nn.NLLLoss\n",
    "n_classes = len(ab_label_dict)\n",
    "if model_name==\"shallow\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    pool_time_length = 16\n",
    "    pool_time_stride = 2\n",
    "    filter_time_length = 16\n",
    "    n_filters_spat = 64\n",
    "    n_filters_time = 32\n",
    "    split_first_layer = True\n",
    "    drop_prob = 0.328794\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = ShallowFBCSPNet(n_chans,\n",
    "                                    n_classes,\n",
    "                                    #n_filters_time=n_start_chans,\n",
    "                                    #n_filters_spat=n_start_chans,\n",
    "                                    n_times=input_time_length,\n",
    "                                    pool_time_length=pool_time_length,\n",
    "                                    pool_time_stride=pool_time_stride,\n",
    "                                    drop_prob=drop_prob,\n",
    "                                    n_filters_time=n_filters_time,\n",
    "                                    n_filters_spat=n_filters_spat,\n",
    "                                    filter_time_length=filter_time_length,\n",
    "                                    split_first_layer=split_first_layer,\n",
    "                                    final_conv_length='auto',)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"deep\":\n",
    "    optimizer_lr = init_lr\n",
    "    optimizer_weight_decay = 0\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                         n_filters_time=n_start_chans,\n",
    "                         n_filters_spat=n_start_chans,\n",
    "                         n_times=input_time_length,\n",
    "                         n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                         n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                         n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                         final_conv_length='auto',\n",
    "                        stride_before_pool=True)\n",
    "    test=torch.ones(size=(6,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"deep_smac\" or model_name == 'deep_smac_bnorm':\n",
    "    optimizer_lr = 0.0000625\n",
    "    if model_name == 'deep_smac':\n",
    "            do_batch_norm = False\n",
    "    else:\n",
    "        do_batch_norm = True\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 4\n",
    "    filter_length_3 = 8\n",
    "    filter_length_4 = 12\n",
    "    filter_time_length = 8\n",
    "    #final_conv_length = 1\n",
    "    first_nonlin = elu\n",
    "    first_pool_mode = 'mean'\n",
    "    later_nonlin = elu\n",
    "    later_pool_mode = 'mean'\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    pool_time_length = 1\n",
    "    pool_time_stride = 1\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    n_start_chans = n_filters_start\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "            n_filters_time=n_start_chans,\n",
    "            n_filters_spat=n_start_chans,\n",
    "            n_times=input_time_length,\n",
    "            n_filters_2=int(n_start_chans * n_chan_factor),\n",
    "            n_filters_3=int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "            n_filters_4=int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "            final_conv_length='auto',\n",
    "            stride_before_pool=True,\n",
    "            drop_prob=drop_prob,\n",
    "            filter_length_2=filter_length_2,\n",
    "            filter_length_3=filter_length_3,\n",
    "            filter_length_4=filter_length_4,\n",
    "            filter_time_length=filter_time_length,\n",
    "            first_conv_nonlin=first_nonlin,\n",
    "            first_pool_mode=first_pool_mode,\n",
    "            later_conv_nonlin=later_nonlin,\n",
    "            later_pool_mode=later_pool_mode,\n",
    "            pool_time_length=pool_time_length,\n",
    "            pool_time_stride=pool_time_stride,\n",
    "            split_first_layer=split_first_layer\n",
    "            )\n",
    "    test=torch.ones(size=(6,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del do_batch_norm,drop_prob,filter_length_2,filter_length_3,filter_length_4,filter_time_length,first_nonlin,n_chan_factor,n_start_chans,first_pool_mode,later_nonlin,later_pool_mode,n_filters_factor,n_filters_start,pool_time_length,pool_time_stride,split_first_layer\n",
    "#Works properly, fit the hybrid cnn\n",
    "elif model_name==\"hybrid\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    #The final conv length is auto to ensure that output will give two values for single EEG window\n",
    "    model = HybridNet(n_chans, n_classes,n_times=input_time_length,)\n",
    "    test=torch.ones(size=(2,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    model.final_layer=nn.Conv2d(100,n_classes,(out_length,1),bias=True,)\n",
    "    model=nn.Sequential(model,nn.Flatten(),nn.LogSoftmax(dim=1))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length\n",
    "elif model_name==\"TCN\":\n",
    "    import warnings\n",
    "    #This disables the warning of the dropout2d layers receiving 3d input\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    n_blocks=7\n",
    "    n_filters=32\n",
    "    kernel_size=24\n",
    "    drop_prob = 0.3\n",
    "    x=TCN(n_chans,n_classes,n_blocks,n_filters,kernel_size,drop_prob)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=x.forward(test)\n",
    "    out_length=out.shape[2]\n",
    "    #There is no hyperparameter where output of TCN is (Batch_Size,Classes) when input is (Batch_Size,21,6000) so add new layers to meet size\n",
    "    model=nn.Sequential(x,nn.Conv1d(n_classes,n_classes,out_length,bias=True,),nn.LogSoftmax(dim=1),nn.Flatten())\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "    del out_length,x\n",
    "elif model_name==\"shallow_deep\":\n",
    "    drop_prob = 0.244445\n",
    "    filter_length_2 = 8\n",
    "    filter_length_3 = 12\n",
    "    filter_length_4 = 16\n",
    "    n_filters_factor = 1.679066\n",
    "    n_filters_start = 32\n",
    "    split_first_layer = True\n",
    "    n_chan_factor = n_filters_factor\n",
    "    #n_start_chans = n_filters_start\n",
    "\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    filter_time_length=4\n",
    "    first_conv_nonlin=relu\n",
    "    first_pool_nonlin=safe_log\n",
    "    later_conv_nonlin=elu\n",
    "    later_pool_nonlin=safe_log\n",
    "    first_pool_mode = \"mean\"\n",
    "    later_pool_mode = \"mean\"\n",
    "    pool_time_length=2\n",
    "    pool_time_stride=2\n",
    "    model = Deep4Net(n_chans, n_classes,\n",
    "                            n_filters_time=n_start_chans,\n",
    "                            n_filters_spat=n_start_chans,\n",
    "                            n_times=input_time_length,\n",
    "                            n_filters_2 = int(n_start_chans * n_chan_factor),\n",
    "                            n_filters_3 = int(n_start_chans * (n_chan_factor ** 2.0)),\n",
    "                            n_filters_4 = int(n_start_chans * (n_chan_factor ** 3.0)),\n",
    "                            final_conv_length='auto',\n",
    "                            first_pool_nonlin=first_pool_nonlin,\n",
    "                            first_conv_nonlin=first_conv_nonlin,\n",
    "                            #later_pool_nonlin=later_pool_nonlin,\n",
    "                            #later_conv_nonlin=later_conv_nonlin,\n",
    "                            filter_time_length=filter_time_length,\n",
    "                            pool_time_length=pool_time_length,\n",
    "                            pool_time_stride=pool_time_stride,\n",
    "                            first_pool_mode=first_pool_mode,\n",
    "                            later_pool_mode=later_pool_mode,\n",
    "                            split_first_layer=split_first_layer,\n",
    "                            drop_prob=drop_prob,\n",
    "                            filter_length_2=filter_length_2,\n",
    "                            filter_length_3=filter_length_3,\n",
    "                            filter_length_4=filter_length_4,\n",
    "                            )\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "#    out=model.forward(test)\n",
    "#    print(out.shape)\n",
    "\n",
    "elif model_name==\"attention\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    model=ATCNet(n_chans,n_classes,input_time_length//sampling_freq,sampling_freq,concat=True)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "elif model_name==\"transformer\":\n",
    "    optimizer_lr = 0.0000625\n",
    "    optimizer_weight_decay = 0\n",
    "    criterion=torch.nn.CrossEntropyLoss\n",
    "    n_filters_time=20\n",
    "    att_depth=8\n",
    "    filter_time_length=35\n",
    "    att_heads=5\n",
    "    model=EEGConformer(n_outputs=n_classes,n_chans=n_chans,n_times=input_time_length,input_window_seconds=input_time_length//sampling_freq,\n",
    "                       sfreq=sampling_freq,final_fc_length=7860,n_filters_time=n_filters_time,att_depth=att_depth,\n",
    "                       filter_time_length=filter_time_length,att_heads=att_heads)\n",
    "    test=torch.ones(size=(7,n_chans,input_time_length))\n",
    "    out=model.forward(test)\n",
    "    print(out.shape)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "del test\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Deep4Net(\n",
       "  (ensuredims): Ensure4d()\n",
       "  (dimshuffle): Rearrange('batch C T 1 -> batch 1 T C')\n",
       "  (conv_time_spat): CombinedConv(\n",
       "    (conv_time): Conv2d(1, 25, kernel_size=(4, 1), stride=(1, 1))\n",
       "    (conv_spat): Conv2d(25, 25, kernel_size=(1, 2), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (bnorm): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv_nonlin): Expression(expression=relu) \n",
       "  (pool): AvgPool2dWithConv()\n",
       "  (pool_nonlin): Expression(expression=safe_log) \n",
       "  (drop_2): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_2): Conv2d(25, 41, kernel_size=(8, 1), stride=(1, 1), bias=False)\n",
       "  (bnorm_2): BatchNorm2d(41, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_2): Expression(expression=elu) \n",
       "  (pool_2): AvgPool2dWithConv()\n",
       "  (pool_nonlin_2): Expression(expression=identity) \n",
       "  (drop_3): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_3): Conv2d(41, 70, kernel_size=(12, 1), stride=(1, 1), bias=False)\n",
       "  (bnorm_3): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_3): Expression(expression=elu) \n",
       "  (pool_3): AvgPool2dWithConv()\n",
       "  (pool_nonlin_3): Expression(expression=identity) \n",
       "  (drop_4): Dropout(p=0.244445, inplace=False)\n",
       "  (conv_4): Conv2d(70, 118, kernel_size=(16, 1), stride=(1, 1), bias=False)\n",
       "  (bnorm_4): BatchNorm2d(118, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (nonlin_4): Expression(expression=elu) \n",
       "  (pool_4): AvgPool2dWithConv()\n",
       "  (pool_nonlin_4): Expression(expression=identity) \n",
       "  (final_layer): Sequential(\n",
       "    (conv_classifier): Conv2d(118, 10, kernel_size=(7, 1), stride=(1, 1))\n",
       "    (logsoftmax): LogSoftmax(dim=1)\n",
       "    (squeeze): Expression(expression=squeeze_final_output) \n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=nn.NLLLoss()\n",
    "input=[]\n",
    "input.append(train_set.__getitem__(0)[0])\n",
    "input=np.array(input)\n",
    "input=torch.from_numpy(input).cuda()\n",
    "target=[]\n",
    "target.append(train_set.__getitem__(0)[1])\n",
    "target=np.array(target)\n",
    "target=torch.from_numpy(target).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=loss(model(input),target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=============================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ============================================================================================================================================\n",
       "  Deep4Net (Deep4Net)                      [1, 2, 300]               [1, 10]                   --                        --\n",
       "  ├─Ensure4d (ensuredims): 1-1             [1, 2, 300]               [1, 2, 300, 1]            --                        --\n",
       "  ├─Rearrange (dimshuffle): 1-2            [1, 2, 300, 1]            [1, 1, 300, 2]            --                        --\n",
       "  ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 300, 2]            [1, 25, 297, 1]           1,375                     --\n",
       "  ├─BatchNorm2d (bnorm): 1-4               [1, 25, 297, 1]           [1, 25, 297, 1]           50                        --\n",
       "  ├─Expression (conv_nonlin): 1-5          [1, 25, 297, 1]           [1, 25, 297, 1]           --                        --\n",
       "  ├─AvgPool2dWithConv (pool): 1-6          [1, 25, 297, 1]           [1, 25, 148, 1]           --                        [2, 1]\n",
       "  ├─Expression (pool_nonlin): 1-7          [1, 25, 148, 1]           [1, 25, 148, 1]           --                        --\n",
       "  ├─Dropout (drop_2): 1-8                  [1, 25, 148, 1]           [1, 25, 148, 1]           --                        --\n",
       "  ├─Conv2d (conv_2): 1-9                   [1, 25, 148, 1]           [1, 41, 141, 1]           8,200                     [8, 1]\n",
       "  ├─BatchNorm2d (bnorm_2): 1-10            [1, 41, 141, 1]           [1, 41, 141, 1]           82                        --\n",
       "  ├─Expression (nonlin_2): 1-11            [1, 41, 141, 1]           [1, 41, 141, 1]           --                        --\n",
       "  ├─AvgPool2dWithConv (pool_2): 1-12       [1, 41, 141, 1]           [1, 41, 70, 1]            --                        [2, 1]\n",
       "  ├─Expression (pool_nonlin_2): 1-13       [1, 41, 70, 1]            [1, 41, 70, 1]            --                        --\n",
       "  ├─Dropout (drop_3): 1-14                 [1, 41, 70, 1]            [1, 41, 70, 1]            --                        --\n",
       "  ├─Conv2d (conv_3): 1-15                  [1, 41, 70, 1]            [1, 70, 59, 1]            34,440                    [12, 1]\n",
       "  ├─BatchNorm2d (bnorm_3): 1-16            [1, 70, 59, 1]            [1, 70, 59, 1]            140                       --\n",
       "  ├─Expression (nonlin_3): 1-17            [1, 70, 59, 1]            [1, 70, 59, 1]            --                        --\n",
       "  ├─AvgPool2dWithConv (pool_3): 1-18       [1, 70, 59, 1]            [1, 70, 29, 1]            --                        [2, 1]\n",
       "  ├─Expression (pool_nonlin_3): 1-19       [1, 70, 29, 1]            [1, 70, 29, 1]            --                        --\n",
       "  ├─Dropout (drop_4): 1-20                 [1, 70, 29, 1]            [1, 70, 29, 1]            --                        --\n",
       "  ├─Conv2d (conv_4): 1-21                  [1, 70, 29, 1]            [1, 118, 14, 1]           132,160                   [16, 1]\n",
       "  ├─BatchNorm2d (bnorm_4): 1-22            [1, 118, 14, 1]           [1, 118, 14, 1]           236                       --\n",
       "  ├─Expression (nonlin_4): 1-23            [1, 118, 14, 1]           [1, 118, 14, 1]           --                        --\n",
       "  ├─AvgPool2dWithConv (pool_4): 1-24       [1, 118, 14, 1]           [1, 118, 7, 1]            --                        [2, 1]\n",
       "  ├─Expression (pool_nonlin_4): 1-25       [1, 118, 7, 1]            [1, 118, 7, 1]            --                        --\n",
       "  ├─Sequential (final_layer): 1-26         [1, 118, 7, 1]            [1, 10]                   --                        --\n",
       "  │    └─Conv2d (conv_classifier): 2-1     [1, 118, 7, 1]            [1, 10, 1, 1]             8,270                     [7, 1]\n",
       "  │    └─LogSoftmax (logsoftmax): 2-2      [1, 10, 1, 1]             [1, 10, 1, 1]             --                        --\n",
       "  │    └─Expression (squeeze): 2-3         [1, 10, 1, 1]             [1, 10]                   --                        --\n",
       "  ============================================================================================================================================\n",
       "  Total params: 184,953\n",
       "  Trainable params: 184,953\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (M): 5.05\n",
       "  ============================================================================================================================================\n",
       "  Input size (MB): 0.00\n",
       "  Forward/backward pass size (MB): 0.24\n",
       "  Params size (MB): 0.73\n",
       "  Estimated Total Size (MB): 0.98\n",
       "  ============================================================================================================================================,\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monitor = lambda net: any(net.history[-1, ('valid_accuracy_best','valid_f1_best','valid_loss_best')])\n",
    "cp=Checkpoint(monitor='valid_f1_best',dirname='model',f_params=f'{model_name}best_param.pkl',\n",
    "               f_optimizer=f'{model_name}best_opt.pkl', f_history=f'{model_name}best_history.json')\n",
    "path=f'{model_name}II'\n",
    "classifier = braindecode.EEGClassifier(\n",
    "        model,\n",
    "        criterion=criterion,\n",
    "        optimizer=torch.optim.AdamW,\n",
    "        train_split=predefined_split(test_set),\n",
    "        optimizer__lr=optimizer_lr,\n",
    "        #optimizer__weight_decay=optimizer_weight_decay,\n",
    "        iterator_train=DataLoader,\n",
    "        iterator_valid=DataLoader,\n",
    "        iterator_train__shuffle=True,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        #iterator_train__num_workers=1,\n",
    "        #iterator_valid__num_workers=1,\n",
    "        #iterator_train__persistent_workers=True,\n",
    "        #iterator_valid__persistent_workers=True,\n",
    "        batch_size=64,\n",
    "        device=device,\n",
    "        callbacks=[\"accuracy\"],#,\"f1\",cp,],\n",
    "        warm_start=True,\n",
    "        )\n",
    "classifier.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 5 5]\n"
     ]
    }
   ],
   "source": [
    "test=np.random.rand(3,n_chans,input_time_length)\n",
    "out=classifier.predict(test)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 300)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.__getitem__(0)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(5, dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.__getitem__(0)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=============================================================================================================================================\n",
       "  Layer (type (var_name):depth-idx)        Input Shape               Output Shape              Param #                   Kernel Shape\n",
       "  ============================================================================================================================================\n",
       "  ShallowFBCSPNet (ShallowFBCSPNet)        [1, 10, 60]               [1, 10]                   --                        --\n",
       "  ├─Ensure4d (ensuredims): 1-1             [1, 10, 60]               [1, 10, 60, 1]            --                        --\n",
       "  ├─Rearrange (dimshuffle): 1-2            [1, 10, 60, 1]            [1, 1, 60, 10]            --                        --\n",
       "  ├─CombinedConv (conv_time_spat): 1-3     [1, 1, 60, 10]            [1, 25, 36, 1]            6,900                     --\n",
       "  ├─BatchNorm2d (bnorm): 1-4               [1, 25, 36, 1]            [1, 25, 36, 1]            50                        --\n",
       "  ├─Expression (conv_nonlin_exp): 1-5      [1, 25, 36, 1]            [1, 25, 36, 1]            --                        --\n",
       "  ├─AvgPool2d (pool): 1-6                  [1, 25, 36, 1]            [1, 25, 3, 1]             --                        [30, 1]\n",
       "  ├─Expression (pool_nonlin_exp): 1-7      [1, 25, 3, 1]             [1, 25, 3, 1]             --                        --\n",
       "  ├─Dropout (drop): 1-8                    [1, 25, 3, 1]             [1, 25, 3, 1]             --                        --\n",
       "  ├─Sequential (final_layer): 1-9          [1, 25, 3, 1]             [1, 10]                   --                        --\n",
       "  │    └─Conv2d (conv_classifier): 2-1     [1, 25, 3, 1]             [1, 10, 1, 1]             760                       [3, 1]\n",
       "  │    └─LogSoftmax (logsoftmax): 2-2      [1, 10, 1, 1]             [1, 10, 1, 1]             --                        --\n",
       "  │    └─Expression (squeeze): 2-3         [1, 10, 1, 1]             [1, 10]                   --                        --\n",
       "  ============================================================================================================================================\n",
       "  Total params: 7,710\n",
       "  Trainable params: 7,710\n",
       "  Non-trainable params: 0\n",
       "  Total mult-adds (M): 0.00\n",
       "  ============================================================================================================================================\n",
       "  Input size (MB): 0.00\n",
       "  Forward/backward pass size (MB): 0.01\n",
       "  Params size (MB): 0.00\n",
       "  Estimated Total Size (MB): 0.01\n",
       "  ============================================================================================================================================,\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(train_set,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_params(\n",
    "        f_params=f'model/{model_name}best_param.pkl', f_history=f'model/{model_name}best_history.json')\n",
    "print(\"Paramters Loaded\")\n",
    "pred_labels=classifier.predict(test_set)\n",
    "actual_labels=[label[1] for label in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shallow\n",
      "Accuracy:0.5634095634095634\n",
      "F1-Score:0.521509870819439\n"
     ]
    }
   ],
   "source": [
    "#auc=roc_auc_score(actual_labels,classifier.predict_proba(test_set)[:,1],multi_class='ovr')\n",
    "actual_labels=np.array(actual_labels)\n",
    "accuracy=np.mean(pred_labels==actual_labels)\n",
    "f1=f1_score(actual_labels,pred_labels, average='weighted')\n",
    "print(model_name)\n",
    "#Accuracy seems incorrect compared to the one reported by skorch\n",
    "print(f\"Accuracy:{accuracy}\")\n",
    "print(f\"F1-Score:{f1}\")\n",
    "#print(f\"roc_auc score:{auc}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
